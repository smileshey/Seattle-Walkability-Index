{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arcpy\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from arcpy.sa import Raster\n",
    "from scipy.stats import rankdata\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Classes in the Geodatabase:\n",
      "WorldImagery_DetectObjectsUs\n",
      "WorldImagery_DetectObjectsUs1\n",
      "WorldImagery_DetectObjectsUs2\n",
      "limits_JSONToFeatures\n",
      "citylimits_JSONToFeatures\n",
      "citylimits_JSONToFeatur_Clip\n",
      "Park_Boundarie_JSONToFeature\n",
      "Parks_Intersect\n",
      "Parks_Boundary_out_Intersect\n",
      "fishnet_park_Intersect\n",
      "fishnet_park_Inter_Intersect\n",
      "fishnet_park_Inter_Intersect1\n",
      "Walkability\n",
      "fishnet_inter\n",
      "citylimits\n",
      "fishnet_Intersect\n",
      "fishnet_Intersect1\n",
      "Parks_Boundary_out_Intersect1\n",
      "Park_Boundarie_JSONToFeature1\n",
      "Parks_SPR_Merge\n",
      "Parks_Merge_Intersect\n",
      "Parks_Merge_Intersect1\n",
      "fishnet_Intersect2\n",
      "Parks_Merge_Intersect2\n",
      "fishnet_sidewalk_intersect\n",
      "fishnet_sidewalks_Intersect\n",
      "Seattle_Streets_poly\n",
      "trails_Intersect\n",
      "fishnet_ExportFeatures\n",
      "fishnet_dataframe\n",
      "Streets_Intersect\n",
      "Bike_facilities\n",
      "Bike_facilities_BKF_BL\n",
      "Bike_facilities_BKF_CLMB\n",
      "Bike_facilities_BKF_OFFST\n",
      "Bike_facilities_BKF_SHW\n",
      "Bike_Greenways_Intersect\n",
      "Bike_protected_Intersect\n",
      "Bike_buffer_Intersect\n",
      "Streets_Intersect1\n",
      "fishnet_check\n",
      "fishnet_check_label\n",
      "fishnet_check_Clip\n",
      "fishnet__\n",
      "fishnet___label\n",
      "fishnet__Intersect\n",
      "fishnet_clipped_graveyard\n",
      "temp_fishnet_Universities\n",
      "Sidewalks_projected\n",
      "Sidewalks_intersect\n",
      "Universities_intersect\n",
      "temp_fishnet_joined\n",
      "temp_fishnet_initial\n",
      "Intersections\n",
      "fishnet_clipped_updated\n",
      "fishnet_clipped_Clip\n",
      "neighborhoodMap\n",
      "neighborhoodsMap\n",
      "Bike_greenways_proj\n",
      "Bike_protected_proj\n",
      "Bike_buffer_proj\n",
      "Healthy_Streets_proj\n",
      "Parks_proj\n",
      "Sidewalks_proj\n",
      "trails_proj\n",
      "MultiUseTrails_proj\n",
      "Streets_proj\n",
      "Industrial_legacy\n",
      "GolfCourse\n",
      "Cemeteries\n",
      "Hospitals\n",
      "Bike_greenways\n",
      "Bike_protected\n",
      "Bike_buffer\n",
      "Healthy_Streets\n",
      "Parks\n",
      "Universities\n",
      "Sidewalks_legacy\n",
      "Plaza\n",
      "trails_legacy\n",
      "MultiUseTrails\n",
      "Streets_legacy\n",
      "Marked_Crosswalks\n",
      "neighborhoods_legacy\n",
      "GreenLake\n",
      "ParkingLots_clipped\n",
      "TrafficControlDevices_Clip\n",
      "TrafficControlDevices_clipped\n",
      "PointsofInterest\n",
      "Business_Amenities\n",
      "Public_Amenities\n",
      "Public_Amenities_proj\n",
      "Public_Amenities_int\n",
      "walkscore_fishnet_export\n",
      "ParkingLots_proj\n",
      "ParkingLots\n",
      "ParkingLots_Clip\n",
      "Sidewalks\n",
      "Streets\n",
      "combined_polygons\n",
      "combined_polylines\n",
      "Point\n",
      "Industrial\n",
      "trails\n",
      "fishnet_label\n",
      "walkscore_fishnet_with_neighborhood\n",
      "neighborhoods\n",
      "TreeCanopy\n",
      "population_with_neighborhood\n",
      "population_with_neighborhood_new\n",
      "Public_Amenities_intersect\n",
      "population_with_fishnet\n",
      "crime_fishnet_intersect\n",
      "population_proj\n",
      "population_int\n",
      "population\n",
      "Links\n",
      "Business_Amenities_intersect\n",
      "smoothed_points\n",
      "smoothed_fishnet_output\n",
      "Business_Amenities_proj\n",
      "Business_Amenities_int\n",
      "LargeStreets\n",
      "Streets_WideStreets\n",
      "SPD_Crime_Data_intersect\n",
      "Points_2\n",
      "Polylines_2\n",
      "Road_safety_XYTableToPoint\n",
      "SDOT_Collisions_All_Years_XYTableToPoint\n",
      "crash_data\n",
      "TreeCanopy_proj\n",
      "TreeCanopy_int\n",
      "Public_Amenities_clipped\n",
      "SPD_Crime_Data_XYTableToPoint\n",
      "SPD_Crime_Data\n",
      "Polylines_2_1\n",
      "walkscore_fish_point\n",
      "Polylines_2_2\n",
      "base_case_points\n",
      "base_case_poin_FeatureToPoin\n",
      "Polylines_2_3\n",
      "walkscore_fishnet_points_AggregatePoints\n",
      "hex\n",
      "Polygons_2\n",
      "Polygons_2_1\n",
      "GenerateTessellation\n",
      "Polygons_2_2\n",
      "Polygons_2_3\n",
      "Polygons_2_4\n",
      "Polygons_2_5\n",
      "Polygons_2_6\n",
      "Polygons_2_7\n",
      "walkscore_fishnet_points\n",
      "Polygons_2_8\n",
      "Polygons_2_9\n",
      "walkscore_fishnet_clipped\n",
      "walkscore_neighborhoods_join\n",
      "walkscore_neighborhoods\n",
      "fishnet\n",
      "fishnet_Clipped\n",
      "Industrial_int\n",
      "ParkingLots_int\n",
      "GolfCourse_int\n",
      "Cemeteries_int\n",
      "Hospitals_int\n",
      "Bike_greenways_int\n",
      "Bike_protected_int\n",
      "Bike_buffer_int\n",
      "Healthy_Streets_int\n",
      "Parks_int\n",
      "Universities_int\n",
      "Sidewalks_int\n",
      "Plaza_int\n",
      "trails_int\n",
      "MultiUseTrails_int\n",
      "Streets_int\n",
      "population_intersect\n",
      "crashes_intersect\n",
      "SPD_Crime_Data_proj\n",
      "SPD_Crime_Data_int\n",
      "Business_Amenities_clipped\n",
      "walkscore_fishnet\n",
      "SPD_Crime_Data_clipped\n",
      "crashes_clipped\n",
      "crashes\n"
     ]
    }
   ],
   "source": [
    "gdb_path = r\"C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\"\n",
    "arcpy.env.workspace = gdb_path\n",
    "\n",
    "# List all feature classes in the geodatabase\n",
    "feature_classes = arcpy.ListFeatureClasses()\n",
    "print(\"Feature Classes in the Geodatabase:\")\n",
    "for fc in feature_classes:\n",
    "    print(fc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Field Names and Types:\n",
      "Name: OBJECTID, Type: OID, Alias: OBJECTID\n",
      "Name: Shape, Type: Geometry, Alias: Shape\n",
      "Name: FID_walkscore_fishnet, Type: Integer, Alias: FID_walkscore_fishnet\n",
      "Name: GRID_ID, Type: String, Alias: GRID_ID\n",
      "Name: IndexID, Type: Integer, Alias: IndexID\n",
      "Name: total_area, Type: Double, Alias: total_area\n",
      "Name: Max_Speed_Limit, Type: Double, Alias: MAX_effective_SPEEDLIMIT\n",
      "Name: SUM_proportional_population, Type: Double, Alias: SUM_proportional_population\n",
      "Name: Grid_Slope_MEAN, Type: Double, Alias: MEAN\n",
      "Name: Sidewalks_Slope_Mean, Type: Double, Alias: Sidewalks_Slope_Mean\n",
      "Name: Streets_Slope_Mean, Type: Double, Alias: Streets_Slope_Mean\n",
      "Name: MultiUseTrails_Slope_Mean, Type: Double, Alias: MultiUseTrails_Slope_Mean\n",
      "Name: trails_Slope_Mean, Type: Double, Alias: trails_Slope_Mean\n",
      "Name: effective_slope, Type: Double, Alias: effective_slope\n",
      "Name: business_density, Type: Double, Alias: business_density\n",
      "Name: Industrial_SUM_Industrial_effective_area, Type: Double, Alias: Industrial_SUM_Industrial_effective_area\n",
      "Name: ParkingLots_SUM_ParkingLots_effective_area, Type: Double, Alias: ParkingLots_SUM_ParkingLots_effective_area\n",
      "Name: GolfCourse_SUM_GolfCourse_effective_area, Type: Double, Alias: GolfCourse_SUM_GolfCourse_effective_area\n",
      "Name: Cemeteries_SUM_Cemeteries_effective_area, Type: Double, Alias: Cemeteries_SUM_Cemeteries_effective_area\n",
      "Name: Hospitals_SUM_Hospitals_effective_area, Type: Double, Alias: Hospitals_SUM_Hospitals_effective_area\n",
      "Name: Slope_MEAN, Type: Double, Alias: Slope_MEAN\n",
      "Name: Bike_greenways_SUM_Bike_greenways_area, Type: Double, Alias: Bike_greenways_SUM_Bike_greenways_area\n",
      "Name: Bike_protected_SUM_Bike_protected_area, Type: Double, Alias: Bike_protected_SUM_Bike_protected_area\n",
      "Name: Bike_buffer_SUM_Bike_buffer_area, Type: Double, Alias: Bike_buffer_SUM_Bike_buffer_area\n",
      "Name: Healthy_Streets_SUM_Healthy_Streets_area, Type: Double, Alias: Healthy_Streets_SUM_Healthy_Streets_area\n",
      "Name: Parks_SUM_Parks_area, Type: Double, Alias: Parks_SUM_Parks_area\n",
      "Name: Universities_SUM_Universities_area, Type: Double, Alias: Universities_SUM_Universities_area\n",
      "Name: Sidewalks_SUM_Sidewalks_area, Type: Double, Alias: Sidewalks_SUM_Sidewalks_area\n",
      "Name: Plaza_SUM_Plaza_area, Type: Double, Alias: Plaza_SUM_Plaza_area\n",
      "Name: trails_SUM_trails_area, Type: Double, Alias: trails_SUM_trails_area\n",
      "Name: MultiUseTrails_SUM_MultiUseTrails_area, Type: Double, Alias: MultiUseTrails_SUM_MultiUseTrails_area\n",
      "Name: Streets_SUM_Streets_effective_area, Type: Double, Alias: Streets_SUM_Streets_effective_area\n",
      "Name: population_SUM_proportional_population, Type: Double, Alias: population_SUM_proportional_population\n",
      "Name: SPD_Crime_Data_COUNT_Offense_ID, Type: Double, Alias: SPD_Crime_Data_COUNT_Offense_ID\n",
      "Name: FID_neighborhoods, Type: Integer, Alias: FID_neighborhoods\n",
      "Name: city, Type: String, Alias: city\n",
      "Name: nested, Type: String, Alias: nested\n",
      "Name: neighborhood_area, Type: Double, Alias: neighborhood_area\n",
      "Name: is_tourist, Type: Integer, Alias: is_tourist\n",
      "Name: latitude, Type: Double, Alias: latitude\n",
      "Name: longitude, Type: Double, Alias: longitude\n",
      "Name: is_industrial, Type: Integer, Alias: is_industrial\n",
      "Name: Shape_Length, Type: Double, Alias: Shape_Length\n",
      "Name: Shape_Area, Type: Double, Alias: Shape_Area\n",
      "Name: Fragment_Area, Type: Double, Alias: Fragment_Area\n",
      "Name: IsIndustrial, Type: SmallInteger, Alias: IsIndustrial\n",
      "Name: IsParkingLots, Type: SmallInteger, Alias: IsParkingLots\n",
      "Name: IsGolfCourse, Type: SmallInteger, Alias: IsGolfCourse\n",
      "Name: IsCemeteries, Type: SmallInteger, Alias: IsCemeteries\n",
      "Name: IsHospitals, Type: SmallInteger, Alias: IsHospitals\n",
      "Name: crime_density, Type: Double, Alias: crime_density\n",
      "Name: crime_density_normalized, Type: Double, Alias: crime_density_normalized\n",
      "Name: crash_density, Type: Double, Alias: crash_density\n",
      "Name: crash_density_normalized, Type: Double, Alias: crash_density_normalized\n"
     ]
    }
   ],
   "source": [
    "exported_fishnet_layer = r\"C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\walkscore_fishnet\"\n",
    "\n",
    "# Set the workspace to the geodatabase\n",
    "arcpy.env.workspace = gdb_path\n",
    "\n",
    "# List all fields in the exported fishnet layer\n",
    "fields = arcpy.ListFields(exported_fishnet_layer)\n",
    "\n",
    "# Print field names and types\n",
    "print(\"Field Names and Types:\")\n",
    "for field in fields:\n",
    "    print(f\"Name: {field.name}, Type: {field.type}, Alias: {field.aliasName}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_list = []\n",
    "slope_field = \"effective_slope\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fields in walkscore_fishnet:\n",
      "total_area\n",
      "Industrial_SUM_Industrial_effective_area\n",
      "ParkingLots_SUM_ParkingLots_effective_area\n",
      "GolfCourse_SUM_GolfCourse_effective_area\n",
      "Cemeteries_SUM_Cemeteries_effective_area\n",
      "Hospitals_SUM_Hospitals_effective_area\n",
      "Bike_greenways_SUM_Bike_greenways_area\n",
      "Bike_protected_SUM_Bike_protected_area\n",
      "Bike_buffer_SUM_Bike_buffer_area\n",
      "Healthy_Streets_SUM_Healthy_Streets_area\n",
      "Parks_SUM_Parks_area\n",
      "Universities_SUM_Universities_area\n",
      "Sidewalks_SUM_Sidewalks_area\n",
      "Plaza_SUM_Plaza_area\n",
      "trails_SUM_trails_area\n",
      "MultiUseTrails_SUM_MultiUseTrails_area\n",
      "Streets_SUM_Streets_effective_area\n",
      "neighborhood_area\n",
      "Fragment_Area\n"
     ]
    }
   ],
   "source": [
    "for field in arcpy.ListFields(exported_fishnet_layer):\n",
    "    if \"area\" in field.name.lower() and field.name.lower() != \"shape_area\":\n",
    "        field_list.append(field.name)\n",
    "\n",
    "# Print the populated field_list\n",
    "print(\"fields in walkscore_fishnet:\")\n",
    "for field in field_list:\n",
    "    print(field)\n",
    "field_list.append(slope_field)  # Add slope field to the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_field(layer, field_name, norm_field_name, inverse=False, fill_null_value=None):\n",
    "    try:\n",
    "        # Calculate min and max values for the specified field\n",
    "        with arcpy.da.SearchCursor(layer, [field_name]) as cursor:\n",
    "            values = [row[0] for row in cursor if row[0] is not None]\n",
    "            if not values:\n",
    "                print(f\"No values found for {field_name}. Skipping normalization.\")\n",
    "                return\n",
    "            min_value = min(values)\n",
    "            max_value = max(values)\n",
    "        \n",
    "        # Add new field for normalized values if it does not already exist\n",
    "        if not any(f.name == norm_field_name for f in arcpy.ListFields(layer)):\n",
    "            arcpy.management.AddField(layer, norm_field_name, \"DOUBLE\")\n",
    "        \n",
    "        # Normalize the data and store in the new field\n",
    "        with arcpy.da.UpdateCursor(layer, [field_name, norm_field_name]) as cursor:\n",
    "            for row in cursor:\n",
    "                if row[0] is not None:\n",
    "                    normalized_value = (row[0] - min_value) / (max_value - min_value)\n",
    "                    if inverse:\n",
    "                        normalized_value = 1 - normalized_value\n",
    "                    row[1] = normalized_value\n",
    "                else:\n",
    "                    row[1] = fill_null_value if fill_null_value is not None else None\n",
    "                cursor.updateRow(row)\n",
    "        print(f\"Normalization complete for {field_name}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing field {field_name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalization complete for total_area.\n",
      "Normalization complete for Industrial_SUM_Industrial_effective_area.\n",
      "Normalization complete for ParkingLots_SUM_ParkingLots_effective_area.\n",
      "Normalization complete for GolfCourse_SUM_GolfCourse_effective_area.\n",
      "Normalization complete for Cemeteries_SUM_Cemeteries_effective_area.\n",
      "Normalization complete for Hospitals_SUM_Hospitals_effective_area.\n",
      "Normalization complete for Bike_greenways_SUM_Bike_greenways_area.\n",
      "Normalization complete for Bike_protected_SUM_Bike_protected_area.\n",
      "Normalization complete for Bike_buffer_SUM_Bike_buffer_area.\n",
      "Normalization complete for Healthy_Streets_SUM_Healthy_Streets_area.\n",
      "Normalization complete for Parks_SUM_Parks_area.\n",
      "Normalization complete for Universities_SUM_Universities_area.\n",
      "Normalization complete for Sidewalks_SUM_Sidewalks_area.\n",
      "Normalization complete for Plaza_SUM_Plaza_area.\n",
      "Normalization complete for trails_SUM_trails_area.\n",
      "Normalization complete for MultiUseTrails_SUM_MultiUseTrails_area.\n",
      "Normalization complete for Streets_SUM_Streets_effective_area.\n",
      "Normalization complete for neighborhood_area.\n",
      "Normalization complete for Fragment_Area.\n",
      "Normalization complete for effective_slope.\n"
     ]
    }
   ],
   "source": [
    "for field in field_list:\n",
    "    norm_field = \"NORM_\" + field\n",
    "    if field == slope_field:\n",
    "        normalize_field(exported_fishnet_layer, 'effective_slope', 'NORM_effective_slope', inverse=True, fill_null_value=1)\n",
    "    else:\n",
    "        normalize_field(exported_fishnet_layer, field, norm_field)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating Walkscore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "sidewalk_score_field = 'sidewalk_score'\n",
    "park_score_field = 'park_score'\n",
    "trail_score_field = 'trail_score'\n",
    "street_score_field = 'street_score'\n",
    "bike_score_field = 'bike_score'\n",
    "walkscore_field = 'walk_score'\n",
    "unadjusted_walkscore_field = 'unadjusted_walkscore'\n",
    "slope_field = \"effective_slope\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# positive_weights = {\n",
    "#     sidewalk_score_field: 0.70,\n",
    "#     park_score_field: 0.175,\n",
    "#     trail_score_field: 0.1,\n",
    "#     bike_score_field: 0.025\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_weights = {\n",
    "    sidewalk_score_field: 0.5,\n",
    "    park_score_field: 0.40,\n",
    "    trail_score_field: 0.05,\n",
    "    bike_score_field: 0.05\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_weights = {\n",
    "    \"effective_speed_limit_scaler\": 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert abs(sum(positive_weights.values()) - 1.0) < 1e-6, \"The positive weights must sum up to 1.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "for score_field in [walkscore_field,sidewalk_score_field,park_score_field,trail_score_field,street_score_field,bike_score_field,unadjusted_walkscore_field]:\n",
    "    if not any(f.name == score_field for f in arcpy.ListFields(exported_fishnet_layer)):\n",
    "        arcpy.management.AddField(exported_fishnet_layer, score_field, \"DOUBLE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slope Scaler\n",
    "\n",
    "As part of the data, we have a slope element that describes how steep the slope is within each fishnet grid. Instead of handling this element like the others, I'd like to implement a custom function that will allow me to more readily customize the output of the slope scaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "walkscore_fishnet = exported_fishnet_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populated null values in Slope field with 0.\n"
     ]
    }
   ],
   "source": [
    "with arcpy.da.UpdateCursor(walkscore_fishnet, [slope_field]) as cursor:\n",
    "    for row in cursor:\n",
    "        if row[0] is None:\n",
    "            row[0] = 0\n",
    "        cursor.updateRow(row)\n",
    "\n",
    "print(\"Populated null values in Slope field with 0.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 10831 slope values.\n"
     ]
    }
   ],
   "source": [
    "# Field containing the slope values\n",
    "slope_field = \"effective_slope\"\n",
    "\n",
    "# List to store slope values\n",
    "slope_values = []\n",
    "\n",
    "# Extract slope values\n",
    "with arcpy.da.SearchCursor(exported_fishnet_layer, [slope_field]) as cursor:\n",
    "    for row in cursor:\n",
    "        if row[0] is not None:\n",
    "            slope_values.append(row[0])\n",
    "\n",
    "print(f\"Extracted {len(slope_values)} slope values.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descriptive Statistics for Slope Values:\n",
      "count    10831.000000\n",
      "mean         4.095428\n",
      "std          3.315808\n",
      "min          0.000000\n",
      "25%          1.908050\n",
      "50%          3.365004\n",
      "75%          5.496817\n",
      "max         35.683580\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "slope_series = pd.Series(slope_values)\n",
    "slope_stats = slope_series.describe()\n",
    "print(\"Descriptive Statistics for Slope Values:\")\n",
    "print(slope_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_slope_scaler(slope_value):\n",
    "    if slope_value is None or slope_value < 0:\n",
    "        return 1.0\n",
    "    elif slope_value < 2:\n",
    "        return 1.0\n",
    "    elif 2 <= slope_value < 3:\n",
    "        return .9\n",
    "    elif 3 <= slope_value < 4:\n",
    "        return 0.7\n",
    "    elif 4 <= slope_value < 5:\n",
    "        return 0.5\n",
    "    elif 5 <= slope_value < 7:\n",
    "        return 0.3\n",
    "    elif 7 <= slope_value < 10:\n",
    "        return 0.1\n",
    "    elif 10 <= slope_value < 15:\n",
    "        return 0.01\n",
    "    elif 15 <= slope_value < 20:\n",
    "        return 0.00\n",
    "    elif 20 <= slope_value < 25:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streets, ParkingLots, and Industrial Scaler Values\n",
    "\n",
    "In the previous sheet, I created \"effective areas\" for each of Streets, Industrial, and Parkinglots. These effective area represent the area of surface streets in each fishnet grid multiplied by the speed limit on each street. Since Industrial and Parklots don't have explicit speed limits, I've set a universal speed value of 15 and used this to calculate the effective area.\n",
    "\n",
    "Below, I'll create a scaler value based on the sum of all three effective area fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "walkscore_fishnet_layer = exported_fishnet_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure max_speed_limit_field field exists in walkscore_fishnet_layer\n",
    "max_speed_limit_field = \"Max_Speed_Limit\"\n",
    "if not any(f.name == max_speed_limit_field for f in arcpy.ListFields(walkscore_fishnet_layer)):\n",
    "    raise ValueError(f\"The field {max_speed_limit_field} does not exist in the walkscore_fishnet_layer.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_effective_speed_limit_scaler(max_speed_limit_field):\n",
    "    if avg_speed_limit <= 0:\n",
    "        return 1.0\n",
    "    elif avg_speed_limit < 15:\n",
    "        return .95\n",
    "    elif avg_speed_limit < 20:\n",
    "        return 0.9\n",
    "    elif avg_speed_limit < 25:\n",
    "        return 0.85\n",
    "    elif avg_speed_limit < 30:\n",
    "        return 0.75\n",
    "    elif avg_speed_limit < 35:\n",
    "        return 0.6\n",
    "    elif avg_speed_limit < 40:\n",
    "        return 0.4\n",
    "    elif avg_speed_limit < 45:\n",
    "        return 0.2\n",
    "    elif avg_speed_limit < 60:\n",
    "        return 0.01\n",
    "    else:\n",
    "        return 0.00"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Adding Normalized effective area to fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "streets_field = \"Streets_SUM_Streets_effective_area\"\n",
    "industrial_field = \"Industrial_SUM_Industrial_effective_area\"\n",
    "parkinglots_field = \"ParkingLots_SUM_Parkinglots_effective_area\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_values = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 10831 combined values.\n"
     ]
    }
   ],
   "source": [
    "with arcpy.da.SearchCursor(exported_fishnet_layer, [streets_field, industrial_field, parkinglots_field]) as cursor:\n",
    "    for row in cursor:\n",
    "        combined_value = (row[0] if row[0] is not None else 0) + \\\n",
    "                         (row[1] if row[1] is not None else 0) + \\\n",
    "                         (row[2] if row[2] is not None else 0)\n",
    "        combined_values.append(combined_value)\n",
    "\n",
    "print(f\"Extracted {len(combined_values)} combined values.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_series = pd.Series(combined_values)\n",
    "combined_stats = combined_series.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_series = pd.Series(combined_values)\n",
    "min_combined = combined_series.min()\n",
    "max_combined = combined_series.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_combined_value(combined_value):\n",
    "    return (combined_value - min_combined) / (max_combined - min_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_values = combined_series.apply(normalize_combined_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_field = \"NORM_Combined_effective_area\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not any(f.name == normalized_field for f in arcpy.ListFields(exported_fishnet_layer)):\n",
    "    arcpy.management.AddField(exported_fishnet_layer, normalized_field, \"DOUBLE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized values added to the walkscore_fishnet layer.\n"
     ]
    }
   ],
   "source": [
    "with arcpy.da.UpdateCursor(exported_fishnet_layer, [normalized_field]) as cursor:\n",
    "    for i, row in enumerate(cursor):\n",
    "        row[0] = normalized_values[i]\n",
    "        cursor.updateRow(row)\n",
    "\n",
    "print(\"Normalized values added to the walkscore_fishnet layer.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_combined_values = [normalize_combined_value(value) for value in combined_values]\n",
    "normalized_combined_series = pd.Series(normalized_combined_values)\n",
    "normalized_combined_stats = normalized_combined_series.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descriptive Statistics for Combined Values:\n",
      "count    10831.000000\n",
      "mean         0.050821\n",
      "std          0.067004\n",
      "min          0.000000\n",
      "25%          0.021823\n",
      "50%          0.037102\n",
      "75%          0.055141\n",
      "max          1.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"Descriptive Statistics for Combined Values:\")\n",
    "print(normalized_combined_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Business Density Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "business_density = 'business_density'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 10831 business density values.\n"
     ]
    }
   ],
   "source": [
    "# List to store business density values\n",
    "business_values = []\n",
    "\n",
    "# Extract business density values\n",
    "with arcpy.da.SearchCursor(exported_fishnet_layer, [business_density]) as cursor:\n",
    "    for row in cursor:\n",
    "        business_values.append(row[0] if row[0] is not None else 0)  # Append 0 for None values\n",
    "\n",
    "print(f\"Extracted {len(business_values)} business density values.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descriptive Statistics for Business Values:\n",
      "count    10831.000000\n",
      "mean         0.180530\n",
      "std          0.423916\n",
      "min          0.000000\n",
      "25%          0.002866\n",
      "50%          0.044044\n",
      "75%          0.163902\n",
      "max          5.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "business_series = pd.Series(business_values)\n",
    "business_density_stats = business_series.describe()\n",
    "print(\"Descriptive Statistics for Business Values:\")\n",
    "print(business_density_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the distribution above, we can see that the majority of values are 0. This indicates that most fishnet grids don't have a business in them.\n",
    "\n",
    "Because of this distribution, I'd like to reward fishnet grids that contain businesses and set the minimum scaler to 1. Below I'll adjust the business density scaler to reflect this, using 1 as the minimum and 2 as the maximum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_business_density_scaler(business_density_value):\n",
    "    if business_density_value is None or business_density_value <= 0.1:\n",
    "        return 1.0\n",
    "    elif 1.5 <= business_density_value <= 5:\n",
    "        return 2.0 \n",
    "    elif 1.0 <= business_density_value < 1.5:\n",
    "        return 1.5\n",
    "    elif 0.5 <= business_density_value < 1.0:\n",
    "        return 1.25\n",
    "    else:\n",
    "        return 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Adding Normalized Business Density to Fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "business_density_field = \"business_density\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Public Amenity Density Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "public_amenity_density = 'amenity_density'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_public_amenity_density_scaler(public_amenity_density_value):\n",
    "#     if public_amenity_density_value is None or public_amenity_density_value <= 0:\n",
    "#         return 1.0\n",
    "#     elif public_amenity_density_value >= 7:\n",
    "#         return 1.3\n",
    "#     elif public_amenity_density_value >= 5:\n",
    "#         return 1.2\n",
    "#     elif public_amenity_density_value >= 3:\n",
    "#         return 1.0\n",
    "#     else:\n",
    "#         return 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tree Canopy Density Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_canopy_density_scaler(canopy_density_value):\n",
    "#     if canopy_density_value is None or canopy_density_value <= 0:\n",
    "#         return 1.0\n",
    "#     elif canopy_density_value >= 9:\n",
    "#         return 1.2\n",
    "#     elif canopy_density_value >= 8:\n",
    "#         return 1.2\n",
    "#     elif canopy_density_value >= 7:\n",
    "#         return 1.1\n",
    "#     elif canopy_density_value >= 6:\n",
    "#         return 1.1\n",
    "#     elif canopy_density_value >= 5:\n",
    "#         return 1.05\n",
    "#     elif canopy_density_value >= 4:\n",
    "#         return 1.0\n",
    "#     else:\n",
    "#         return 1.0  # Default case, though it shouldn't normally be reached"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Crash Density Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 10831 crash Density values.\n"
     ]
    }
   ],
   "source": [
    "crash_values = []\n",
    "crash_density = 'crash_density_normalized'\n",
    "# Extract crash values\n",
    "with arcpy.da.SearchCursor(exported_fishnet_layer, [crash_density]) as cursor:\n",
    "    for row in cursor:\n",
    "        if row[0] is not None:\n",
    "            crash_values.append(row[0] if row[0] is not None else 0)\n",
    "\n",
    "print(f\"Extracted {len(crash_values)} crash Density values.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descriptive Statistics for crash Density Values:\n",
      "count    10831.000000\n",
      "mean         0.447705\n",
      "std          0.677248\n",
      "min          0.000000\n",
      "25%          0.000108\n",
      "50%          0.154054\n",
      "75%          0.625694\n",
      "max          5.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "crash_series = pd.Series(crash_values)\n",
    "crash_density_stats = crash_series.describe()\n",
    "print(\"Descriptive Statistics for crash Density Values:\")\n",
    "print(crash_density_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_crash_density_scaler(crash_density_value):\n",
    "    if crash_density_value is None or crash_density_value < 0.1:\n",
    "        return 1.0\n",
    "    elif crash_density_value < .5:\n",
    "        return 0.95\n",
    "    elif crash_density_value < 1.5:\n",
    "        return 0.90\n",
    "    elif crash_density_value < 3:\n",
    "        return 0.85\n",
    "    else:\n",
    "        return 0.80"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Crime Density Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_crime_density_scaler(crime_density_value):\n",
    "    if crime_density_value is None or crime_density_value < 0.25:\n",
    "        return 1.0\n",
    "    elif crime_density_value < 0.7:\n",
    "        return 0.95\n",
    "    elif crime_density_value < 1.5:\n",
    "        return 0.90\n",
    "    elif crime_density_value < 3:\n",
    "        return 0.85\n",
    "    else:\n",
    "        return 0.80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Scaler Fields for Troubleshooting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_fields = {\n",
    "    'slope_scaler': 'slope_scaler',\n",
    "    'effective_speed_limit_scaler': 'effective_speed_limit_scaler',\n",
    "    'business_density_scaler': 'business_density_scaler',\n",
    "#     'public_amenity_density_scaler': 'public_amenity_density_scaler',\n",
    "#     'tree_density_scaler':'tree_density_scaler',\n",
    "    'crash_density_scaler':'crash_density_scaler',\n",
    "    'crime_density_scaler':'crime_density_scaler'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "industrial_field = \"NORM_Industrial_SUM_Industrial_effective_area\"\n",
    "golfcourses_field = \"NORM_GolfCourse_SUM_GolfCourse_effective_area\"\n",
    "hospitals_field = \"NORM_Hospitals_SUM_Hospitals_effective_area\"\n",
    "cemeteries_field = \"NORM_Cemeteries_SUM_Cemeteries_effective_area\"\n",
    "parkinglots_field = \"NORM_ParkingLots_SUM_Parkinglots_effective_area\"\n",
    "streets_field = \"NORM_Streets_SUM_Streets_effective_area\"\n",
    "slope_field = \"effective_slope\"\n",
    "walkscore_field = \"walk_score\"\n",
    "walkscore_positive_field = 'walkscore_positive'\n",
    "walkscore_negative_field = 'walkscore_negative'\n",
    "unadjusted_walkscore_field = \"unadjusted_walkscore\"\n",
    "# public_amenity_density_field = 'amenity_density'\n",
    "business_density_field = 'business_density'\n",
    "# tree_density_field = 'tree_density'\n",
    "crash_density_field = 'crash_density_normalized'\n",
    "crime_density_field = 'crime_density_normalized'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_fields = [sidewalk_score_field, park_score_field, trail_score_field, bike_score_field, walkscore_field, unadjusted_walkscore_field]\n",
    "# amenities_fields = [business_density, public_amenity_density,tree_density_field,crash_density_field]\n",
    "amenities_fields = [business_density,crash_density_field,crime_density_field]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = [streets_field, industrial_field, golfcourses_field, hospitals_field, cemeteries_field, parkinglots_field, \n",
    "          slope_field, walkscore_field, max_speed_limit_field] + list(scaler_fields.values()) + list(score_fields) + list(amenities_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NORM_Streets_SUM_Streets_effective_area', 'NORM_Industrial_SUM_Industrial_effective_area', 'NORM_GolfCourse_SUM_GolfCourse_effective_area', 'NORM_Hospitals_SUM_Hospitals_effective_area', 'NORM_Cemeteries_SUM_Cemeteries_effective_area', 'NORM_ParkingLots_SUM_Parkinglots_effective_area', 'effective_slope', 'walk_score', 'Max_Speed_Limit', 'slope_scaler', 'effective_speed_limit_scaler', 'business_density_scaler', 'crash_density_scaler', 'crime_density_scaler', 'sidewalk_score', 'park_score', 'trail_score', 'bike_score', 'walk_score', 'unadjusted_walkscore', 'business_density', 'crash_density_normalized', 'crime_density_normalized']"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add new scaler fields if they don't exist\n",
    "for field_name in scaler_fields.values():\n",
    "    if not any(f.name == field_name for f in arcpy.ListFields(walkscore_fishnet)):\n",
    "        arcpy.management.AddField(walkscore_fishnet, field_name, \"DOUBLE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaler values updated and stored in walkscore_fishnet.\n"
     ]
    }
   ],
   "source": [
    "# Update scalers in walkscore_fishnet\n",
    "with arcpy.da.UpdateCursor(walkscore_fishnet_layer, fields) as cursor:\n",
    "    for row in cursor:\n",
    "        slope_value = row[fields.index(slope_field)]\n",
    "        business_density_value = row[fields.index(business_density_field)]\n",
    "        avg_speed_limit = row[fields.index(\"Max_Speed_Limit\")]\n",
    "        crash_density_value = row[fields.index(crash_density_field)]\n",
    "        crime_density_value = row[fields.index(crime_density_field)]\n",
    "\n",
    "        # Handle the case where avg_speed_limit is None\n",
    "        if avg_speed_limit is None:\n",
    "            avg_speed_limit = 0  # Set a default value, modify as necessary\n",
    "\n",
    "        # Calculate individual scalers\n",
    "        slope_scaler = get_slope_scaler(slope_value)\n",
    "        effective_speed_limit_scaler = get_effective_speed_limit_scaler(avg_speed_limit)\n",
    "        business_density_scaler = get_business_density_scaler(business_density_value)\n",
    "        crash_density_scaler = get_crash_density_scaler(crash_density_value)\n",
    "        crime_density_scaler = get_crime_density_scaler(crime_density_value)\n",
    "\n",
    "        # Update row with calculated scalers\n",
    "        row[fields.index(scaler_fields['slope_scaler'])] = slope_scaler\n",
    "        row[fields.index(scaler_fields['effective_speed_limit_scaler'])] = effective_speed_limit_scaler\n",
    "        row[fields.index(scaler_fields['business_density_scaler'])] = business_density_scaler\n",
    "        row[fields.index(scaler_fields['crash_density_scaler'])] = crash_density_scaler\n",
    "        row[fields.index(scaler_fields['crime_density_scaler'])] = crime_density_scaler\n",
    "\n",
    "        cursor.updateRow(row)\n",
    "\n",
    "print(\"Scaler values updated and stored in walkscore_fishnet.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score Component Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "sidewalk_score_components = ['NORM_Sidewalks_SUM_Sidewalks_area','NORM_MultiUseTrails_SUM_MultiUseTrails_area',\"NORM_Plaza_SUM_Plaza_area\"]\n",
    "park_score_components = ['NORM_Parks_SUM_parks_area', 'NORM_Universities_SUM_Universities_area']\n",
    "trail_score_components = ['NORM_trails_SUM_trails_area']\n",
    "bike_score_components = ['NORM_Bike_greenways_SUM_Bike_greenways_area',\n",
    "                         'NORM_Bike_protected_SUM_Bike_protected_area',\n",
    "                         'NORM_Bike_buffer_SUM_Bike_buffer_area',\n",
    "                         'NORM_Healthy_Streets_SUM_Healthy_Streets_area']\n",
    "street_score_components = ['NORM_Combined_effective_area']\n",
    "slope_component = [slope_field]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "walkscore_fields = (sidewalk_score_components + park_score_components + trail_score_components +\n",
    "                    bike_score_components + street_score_components + slope_component +\n",
    "                    score_fields + list(scaler_fields.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "for field_name in [walkscore_field, unadjusted_walkscore_field, walkscore_negative_field]:\n",
    "    if not any(f.name == field_name for f in arcpy.ListFields(walkscore_fishnet)):\n",
    "        arcpy.management.AddField(walkscore_fishnet, field_name, \"DOUBLE\")\n",
    "        walkscore_fields.append(field_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated and scaled walkscore calculation complete.\n"
     ]
    }
   ],
   "source": [
    "# Calculate walkscore using the new effective area scaler\n",
    "with arcpy.da.UpdateCursor(walkscore_fishnet_layer, walkscore_fields) as cursor:\n",
    "    for row in cursor:\n",
    "        # Calculate individual scores\n",
    "        sidewalk_score = sum(row[walkscore_fields.index(component)] for component in sidewalk_score_components if row[walkscore_fields.index(component)] is not None)\n",
    "        park_score = sum(row[walkscore_fields.index(component)] for component in park_score_components if row[walkscore_fields.index(component)] is not None)\n",
    "        trail_score = sum(row[walkscore_fields.index(component)] for component in trail_score_components if row[walkscore_fields.index(component)] is not None)\n",
    "        bike_score = sum(row[walkscore_fields.index(component)] for component in bike_score_components if row[walkscore_fields.index(component)] is not None)\n",
    "\n",
    "        row[walkscore_fields.index('sidewalk_score')] = sidewalk_score\n",
    "        row[walkscore_fields.index('park_score')] = park_score\n",
    "        row[walkscore_fields.index('trail_score')] = trail_score\n",
    "        row[walkscore_fields.index('bike_score')] = bike_score\n",
    "\n",
    "        slope_scaler = row[walkscore_fields.index(scaler_fields['slope_scaler'])]\n",
    "        effective_speed_limit_scaler = row[walkscore_fields.index(scaler_fields['effective_speed_limit_scaler'])]\n",
    "        business_density_scaler = row[walkscore_fields.index(scaler_fields['business_density_scaler'])]\n",
    "        crash_density_scaler = row[walkscore_fields.index(scaler_fields['crash_density_scaler'])]\n",
    "        crime_density_scaler = row[walkscore_fields.index(scaler_fields['crime_density_scaler'])]\n",
    "\n",
    "        # Calculate positive component of the walkscore (unadjusted walkscore)\n",
    "        walkscore_positive = sum(row[walkscore_fields.index(field)] * positive_weights[field] for field in score_fields if row[walkscore_fields.index(field)] is not None)\n",
    "\n",
    "        # Calculate the negative component of the walkscore\n",
    "        walkscore_negative = row[walkscore_fields.index('NORM_Combined_effective_area')]\n",
    "        row[walkscore_fields.index(walkscore_negative_field)] = walkscore_negative\n",
    "\n",
    "        # Normalize walkscore_positive and walkscore_negative\n",
    "        max_positive_score = 5  # Change from 10 to 5\n",
    "        max_negative_score = 5  # Change from 10 to 5\n",
    "\n",
    "        normalized_positive = walkscore_positive / max_positive_score\n",
    "        normalized_negative = walkscore_negative / max_negative_score\n",
    "\n",
    "        # Assign the unadjusted walkscore to the unadjusted_walkscore field\n",
    "        row[walkscore_fields.index(unadjusted_walkscore_field)] = normalized_positive\n",
    "\n",
    "        # Calculate final walkscore\n",
    "#         walkscore = normalized_positive * slope_scaler * effective_speed_limit_scaler * business_density_scaler * public_amenity_density_scaler * tree_density_scaler * crash_density_scaler\n",
    "        walkscore = normalized_positive * slope_scaler * effective_speed_limit_scaler * business_density_scaler * crash_density_scaler * crime_density_scaler\n",
    "\n",
    "\n",
    "        # Assign the calculated walkscore to the walkscore field explicitly\n",
    "        row[walkscore_fields.index(walkscore_field)] = walkscore\n",
    "\n",
    "        cursor.updateRow(row)\n",
    "\n",
    "print(\"Aggregated and scaled walkscore calculation complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Walkscore normalization using rank transformation and scaling to [0, 100] complete.\n"
     ]
    }
   ],
   "source": [
    "walkscore_fishnet = r\"C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\walkscore_fishnet\"\n",
    "walkscore_field = \"walk_score\"\n",
    "\n",
    "# Check if the field exists in the layer\n",
    "field_names = [field.name for field in arcpy.ListFields(walkscore_fishnet)]\n",
    "if walkscore_field not in field_names:\n",
    "    raise ValueError(f\"Field '{walkscore_field}' does not exist in the layer.\")\n",
    "\n",
    "# Extract walkscore values\n",
    "walkscore_values = []\n",
    "\n",
    "with arcpy.da.SearchCursor(walkscore_fishnet, [walkscore_field]) as cursor:\n",
    "    for row in cursor:\n",
    "        walkscore_values.append(row[0] if row[0] is not None else 0)\n",
    "\n",
    "# Rank transformation\n",
    "ranks = rankdata(walkscore_values, method='dense')\n",
    "\n",
    "# Min-max scaling to [0, 100] after ranking\n",
    "min_rank = min(ranks)\n",
    "max_rank = max(ranks)\n",
    "\n",
    "def min_max_scale(value, min_value, max_value, new_min, new_max):\n",
    "    return new_min + (value - min_value) * (new_max - new_min) / (max_value - min_value)\n",
    "\n",
    "scaled_scores = [min_max_scale(rank, min_rank, max_rank, 0, 100) for rank in ranks]\n",
    "\n",
    "# Add scaled scores back to the shapefile\n",
    "with arcpy.da.UpdateCursor(walkscore_fishnet, [walkscore_field]) as cursor:\n",
    "    for i, row in enumerate(cursor):\n",
    "        row[0] = scaled_scores[i]\n",
    "        cursor.updateRow(row)\n",
    "\n",
    "print(\"Walkscore normalization using rank transformation and scaling to [0, 100] complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregating Walkscore by Neighborhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clipped walkscore fishnet to neighborhoods layer.\n",
      "Performed spatial join between neighborhoods and clipped walkscore fishnet.\n",
      "Added geometry attributes to calculate area for each fishnet grid.\n",
      "Calculated statistics for walkscore and area for each neighborhood.\n",
      "Created the walkscore_neighborhoods layer.\n",
      "Joined the statistics table to the walkscore_neighborhoods layer.\n",
      "Calculated weighted average walkscore for each neighborhood with power-normalized area.\n",
      "Rank-normalized weighted average walkscores to be between 0 and 5.\n",
      "Deleted temporary field: SUM_walk_score\n",
      "Deleted temporary field: SUM_AREA_GEO\n",
      "Temporary fields cleaned up.\n"
     ]
    }
   ],
   "source": [
    "# Set environment settings\n",
    "arcpy.env.workspace = r\"C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\"\n",
    "arcpy.env.overwriteOutput = True  # Allow outputs to be overwritten\n",
    "\n",
    "# Define the input layers\n",
    "walkscore_fishnet = r\"C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\walkscore_fishnet\"\n",
    "neighborhoods = r\"C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\neighborhoods\"\n",
    "clipped_walkscore_fishnet = \"walkscore_fishnet_clipped\"\n",
    "spatial_join_output = \"walkscore_neighborhoods_join\"\n",
    "statistics_output = \"walkscore_statistics\"\n",
    "output_layer = \"walkscore_neighborhoods\"\n",
    "\n",
    "# Step 1: Ensure the spatial reference systems match\n",
    "walkscore_sr = arcpy.Describe(walkscore_fishnet).spatialReference\n",
    "neighborhoods_sr = arcpy.Describe(neighborhoods).spatialReference\n",
    "\n",
    "if walkscore_sr.name != neighborhoods_sr.name:\n",
    "    raise ValueError(\"Spatial references do not match between walkscore_fishnet and neighborhoods.\")\n",
    "\n",
    "# Step 2: Clip the walkscore fishnet to the neighborhoods layer\n",
    "arcpy.analysis.Clip(walkscore_fishnet, neighborhoods, clipped_walkscore_fishnet)\n",
    "print(\"Clipped walkscore fishnet to neighborhoods layer.\")\n",
    "\n",
    "# Step 3: Perform a spatial join to associate each fishnet cell with a neighborhood\n",
    "arcpy.analysis.SpatialJoin(target_features=clipped_walkscore_fishnet,\n",
    "                           join_features=neighborhoods,\n",
    "                           out_feature_class=spatial_join_output,\n",
    "                           join_type=\"KEEP_COMMON\",\n",
    "                           match_option=\"INTERSECT\")\n",
    "print(\"Performed spatial join between neighborhoods and clipped walkscore fishnet.\")\n",
    "\n",
    "# Step 4: Calculate the total walkscore, total area, and weighted walkscores for each neighborhood\n",
    "# Add fields for area calculation in the fishnet\n",
    "arcpy.management.AddGeometryAttributes(spatial_join_output, \"AREA_GEODESIC\", Area_Unit=\"SQUARE_FEET_US\")\n",
    "print(\"Added geometry attributes to calculate area for each fishnet grid.\")\n",
    "\n",
    "# Step 5: Create a new table to calculate weighted sums using the correct area field name\n",
    "arcpy.analysis.Statistics(spatial_join_output, statistics_output,\n",
    "                          [[\"walk_score\", \"SUM\"], [\"AREA_GEO\", \"SUM\"], [\"walk_score\", \"FIRST\"]],\n",
    "                          \"nested\")\n",
    "print(\"Calculated statistics for walkscore and area for each neighborhood.\")\n",
    "\n",
    "# Step 6: Create the walkscore_neighborhoods layer by exporting the neighborhoods layer\n",
    "arcpy.conversion.FeatureClassToFeatureClass(in_features=neighborhoods,\n",
    "                                            out_path=arcpy.env.workspace,\n",
    "                                            out_name=output_layer)\n",
    "print(\"Created the walkscore_neighborhoods layer.\")\n",
    "\n",
    "# Step 7: Add a field for the weighted average walkscore in the walkscore_neighborhoods layer\n",
    "weighted_average_walkscore_field = \"weighted_avg_walk_score\"\n",
    "\n",
    "# Delete the existing field if it exists\n",
    "if any(f.name == weighted_average_walkscore_field for f in arcpy.ListFields(output_layer)):\n",
    "    arcpy.management.DeleteField(output_layer, weighted_average_walkscore_field)\n",
    "\n",
    "arcpy.management.AddField(output_layer, weighted_average_walkscore_field, \"DOUBLE\")\n",
    "\n",
    "# Step 8: Join the statistics table to the walkscore_neighborhoods layer\n",
    "arcpy.management.JoinField(in_data=output_layer,\n",
    "                           in_field=\"nested\",\n",
    "                           join_table=statistics_output,\n",
    "                           join_field=\"nested\",\n",
    "                           fields=[\"SUM_walk_score\", \"SUM_AREA_GEO\"])\n",
    "print(\"Joined the statistics table to the walkscore_neighborhoods layer.\")\n",
    "\n",
    "# Step 9: Calculate the weighted average walkscore for each neighborhood with power-normalized area\n",
    "exponent = 0.85  # Define the exponent for power normalization (less than 1 to reduce the effect of smaller areas)\n",
    "\n",
    "with arcpy.da.UpdateCursor(output_layer, [\"SUM_walk_score\", \"SUM_AREA_GEO\", weighted_average_walkscore_field]) as cursor:\n",
    "    for row in cursor:\n",
    "        if row[0] is not None and row[1] is not None and row[1] > 0:\n",
    "            # Power normalize the area\n",
    "            normalized_area = math.pow(row[1], exponent)\n",
    "\n",
    "            # Calculate the weighted average walkscore using the power-normalized area\n",
    "            row[2] = row[0] / normalized_area  # SUM_walk_score / (SUM_AREA_GEO ^ exponent)\n",
    "        else:\n",
    "            row[2] = None\n",
    "        cursor.updateRow(row)\n",
    "\n",
    "print(\"Calculated weighted average walkscore for each neighborhood with power-normalized area.\")\n",
    "\n",
    "# Step 10: Rank-normalize the weighted average walkscores\n",
    "rank_normalized_walk_score_field = \"rank_normalized_walk_score\"\n",
    "\n",
    "# Delete the existing field if it exists\n",
    "if any(f.name == rank_normalized_walk_score_field for f in arcpy.ListFields(output_layer)):\n",
    "    arcpy.management.DeleteField(output_layer, rank_normalized_walk_score_field)\n",
    "\n",
    "arcpy.management.AddField(output_layer, rank_normalized_walk_score_field, \"DOUBLE\")\n",
    "\n",
    "# Step 10.1: Extract the weighted average walkscores and rank them\n",
    "scores = [(row[0], i) for i, row in enumerate(arcpy.da.SearchCursor(output_layer, [weighted_average_walkscore_field])) if row[0] is not None]\n",
    "\n",
    "# Sort scores and create ranks\n",
    "sorted_scores = sorted(scores, key=lambda x: x[0])\n",
    "ranks = [(score[1], i + 1) for i, score in enumerate(sorted_scores)]\n",
    "\n",
    "# Normalize the ranks to be between 0 and 100\n",
    "min_rank = min(rank[1] for rank in ranks)\n",
    "max_rank = max(rank[1] for rank in ranks)\n",
    "normalized_ranks = [(rank[0], 100 * (rank[1] - min_rank) / (max_rank - min_rank)) for rank in ranks]\n",
    "\n",
    "# Step 10.2: Assign the rank-normalized scores back to the walkscore_neighborhoods layer\n",
    "normalized_rank_dict = dict(normalized_ranks)\n",
    "\n",
    "with arcpy.da.UpdateCursor(output_layer, [weighted_average_walkscore_field, rank_normalized_walk_score_field]) as cursor:\n",
    "    for i, row in enumerate(cursor):\n",
    "        if i in normalized_rank_dict:\n",
    "            row[1] = normalized_rank_dict[i]\n",
    "        else:\n",
    "            row[1] = None\n",
    "        cursor.updateRow(row)\n",
    "print(\"Rank-normalized weighted average walkscores to be between 0 and 5.\")\n",
    "\n",
    "# Step 11: Clean up temporary fields\n",
    "fields_to_delete = [\"SUM_walk_score\", \"SUM_AREA_GEO\"]\n",
    "for field in fields_to_delete:\n",
    "    if any(f.name == field for f in arcpy.ListFields(output_layer)):\n",
    "        arcpy.management.DeleteField(output_layer, field)\n",
    "        print(f\"Deleted temporary field: {field}\")\n",
    "\n",
    "print(\"Temporary fields cleaned up.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Reduction\n",
    "\n",
    "The end goal of this map is to be a user interface that allows users to interact with the data. To improve performance for the end-user, I'll trim down the fields in walkscore_fishnet and walkscore_neighborhoods to reduce the amount of data that needs to be served to each person that interacts with the UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Field Names and Types:\n",
      "Name: OBJECTID, Type: OID, Alias: OBJECTID\n",
      "Name: Shape, Type: Geometry, Alias: Shape\n",
      "Name: FID_walkscore_fishnet, Type: Integer, Alias: FID_walkscore_fishnet\n",
      "Name: GRID_ID, Type: String, Alias: GRID_ID\n",
      "Name: IndexID, Type: Integer, Alias: IndexID\n",
      "Name: total_area, Type: Double, Alias: total_area\n",
      "Name: Max_Speed_Limit, Type: Double, Alias: MAX_effective_SPEEDLIMIT\n",
      "Name: SUM_proportional_population, Type: Double, Alias: SUM_proportional_population\n",
      "Name: Grid_Slope_MEAN, Type: Double, Alias: MEAN\n",
      "Name: Sidewalks_Slope_Mean, Type: Double, Alias: Sidewalks_Slope_Mean\n",
      "Name: Streets_Slope_Mean, Type: Double, Alias: Streets_Slope_Mean\n",
      "Name: MultiUseTrails_Slope_Mean, Type: Double, Alias: MultiUseTrails_Slope_Mean\n",
      "Name: trails_Slope_Mean, Type: Double, Alias: trails_Slope_Mean\n",
      "Name: effective_slope, Type: Double, Alias: effective_slope\n",
      "Name: business_density, Type: Double, Alias: business_density\n",
      "Name: Industrial_SUM_Industrial_effective_area, Type: Double, Alias: Industrial_SUM_Industrial_effective_area\n",
      "Name: ParkingLots_SUM_ParkingLots_effective_area, Type: Double, Alias: ParkingLots_SUM_ParkingLots_effective_area\n",
      "Name: GolfCourse_SUM_GolfCourse_effective_area, Type: Double, Alias: GolfCourse_SUM_GolfCourse_effective_area\n",
      "Name: Cemeteries_SUM_Cemeteries_effective_area, Type: Double, Alias: Cemeteries_SUM_Cemeteries_effective_area\n",
      "Name: Hospitals_SUM_Hospitals_effective_area, Type: Double, Alias: Hospitals_SUM_Hospitals_effective_area\n",
      "Name: Slope_MEAN, Type: Double, Alias: Slope_MEAN\n",
      "Name: Bike_greenways_SUM_Bike_greenways_area, Type: Double, Alias: Bike_greenways_SUM_Bike_greenways_area\n",
      "Name: Bike_protected_SUM_Bike_protected_area, Type: Double, Alias: Bike_protected_SUM_Bike_protected_area\n",
      "Name: Bike_buffer_SUM_Bike_buffer_area, Type: Double, Alias: Bike_buffer_SUM_Bike_buffer_area\n",
      "Name: Healthy_Streets_SUM_Healthy_Streets_area, Type: Double, Alias: Healthy_Streets_SUM_Healthy_Streets_area\n",
      "Name: Parks_SUM_Parks_area, Type: Double, Alias: Parks_SUM_Parks_area\n",
      "Name: Universities_SUM_Universities_area, Type: Double, Alias: Universities_SUM_Universities_area\n",
      "Name: Sidewalks_SUM_Sidewalks_area, Type: Double, Alias: Sidewalks_SUM_Sidewalks_area\n",
      "Name: Plaza_SUM_Plaza_area, Type: Double, Alias: Plaza_SUM_Plaza_area\n",
      "Name: trails_SUM_trails_area, Type: Double, Alias: trails_SUM_trails_area\n",
      "Name: MultiUseTrails_SUM_MultiUseTrails_area, Type: Double, Alias: MultiUseTrails_SUM_MultiUseTrails_area\n",
      "Name: Streets_SUM_Streets_effective_area, Type: Double, Alias: Streets_SUM_Streets_effective_area\n",
      "Name: population_SUM_proportional_population, Type: Double, Alias: population_SUM_proportional_population\n",
      "Name: SPD_Crime_Data_COUNT_Offense_ID, Type: Double, Alias: SPD_Crime_Data_COUNT_Offense_ID\n",
      "Name: FID_neighborhoods, Type: Integer, Alias: FID_neighborhoods\n",
      "Name: city, Type: String, Alias: city\n",
      "Name: nested, Type: String, Alias: nested\n",
      "Name: neighborhood_area, Type: Double, Alias: neighborhood_area\n",
      "Name: is_tourist, Type: Integer, Alias: is_tourist\n",
      "Name: latitude, Type: Double, Alias: latitude\n",
      "Name: longitude, Type: Double, Alias: longitude\n",
      "Name: is_industrial, Type: Integer, Alias: is_industrial\n",
      "Name: Shape_Length, Type: Double, Alias: Shape_Length\n",
      "Name: Shape_Area, Type: Double, Alias: Shape_Area\n",
      "Name: Fragment_Area, Type: Double, Alias: Fragment_Area\n",
      "Name: IsIndustrial, Type: SmallInteger, Alias: IsIndustrial\n",
      "Name: IsParkingLots, Type: SmallInteger, Alias: IsParkingLots\n",
      "Name: IsGolfCourse, Type: SmallInteger, Alias: IsGolfCourse\n",
      "Name: IsCemeteries, Type: SmallInteger, Alias: IsCemeteries\n",
      "Name: IsHospitals, Type: SmallInteger, Alias: IsHospitals\n",
      "Name: crime_density, Type: Double, Alias: crime_density\n",
      "Name: crime_density_normalized, Type: Double, Alias: crime_density_normalized\n",
      "Name: crash_density, Type: Double, Alias: crash_density\n",
      "Name: crash_density_normalized, Type: Double, Alias: crash_density_normalized\n",
      "Name: NORM_total_area, Type: Double, Alias: NORM_total_area\n",
      "Name: NORM_Industrial_SUM_Industrial_effective_area, Type: Double, Alias: NORM_Industrial_SUM_Industrial_effective_area\n",
      "Name: NORM_ParkingLots_SUM_ParkingLots_effective_area, Type: Double, Alias: NORM_ParkingLots_SUM_ParkingLots_effective_area\n",
      "Name: NORM_GolfCourse_SUM_GolfCourse_effective_area, Type: Double, Alias: NORM_GolfCourse_SUM_GolfCourse_effective_area\n",
      "Name: NORM_Cemeteries_SUM_Cemeteries_effective_area, Type: Double, Alias: NORM_Cemeteries_SUM_Cemeteries_effective_area\n",
      "Name: NORM_Hospitals_SUM_Hospitals_effective_area, Type: Double, Alias: NORM_Hospitals_SUM_Hospitals_effective_area\n",
      "Name: NORM_Bike_greenways_SUM_Bike_greenways_area, Type: Double, Alias: NORM_Bike_greenways_SUM_Bike_greenways_area\n",
      "Name: NORM_Bike_protected_SUM_Bike_protected_area, Type: Double, Alias: NORM_Bike_protected_SUM_Bike_protected_area\n",
      "Name: NORM_Bike_buffer_SUM_Bike_buffer_area, Type: Double, Alias: NORM_Bike_buffer_SUM_Bike_buffer_area\n",
      "Name: NORM_Healthy_Streets_SUM_Healthy_Streets_area, Type: Double, Alias: NORM_Healthy_Streets_SUM_Healthy_Streets_area\n",
      "Name: NORM_Parks_SUM_Parks_area, Type: Double, Alias: NORM_Parks_SUM_Parks_area\n",
      "Name: NORM_Universities_SUM_Universities_area, Type: Double, Alias: NORM_Universities_SUM_Universities_area\n",
      "Name: NORM_Sidewalks_SUM_Sidewalks_area, Type: Double, Alias: NORM_Sidewalks_SUM_Sidewalks_area\n",
      "Name: NORM_Plaza_SUM_Plaza_area, Type: Double, Alias: NORM_Plaza_SUM_Plaza_area\n",
      "Name: NORM_trails_SUM_trails_area, Type: Double, Alias: NORM_trails_SUM_trails_area\n",
      "Name: NORM_MultiUseTrails_SUM_MultiUseTrails_area, Type: Double, Alias: NORM_MultiUseTrails_SUM_MultiUseTrails_area\n",
      "Name: NORM_Streets_SUM_Streets_effective_area, Type: Double, Alias: NORM_Streets_SUM_Streets_effective_area\n",
      "Name: NORM_neighborhood_area, Type: Double, Alias: NORM_neighborhood_area\n",
      "Name: NORM_Fragment_Area, Type: Double, Alias: NORM_Fragment_Area\n",
      "Name: NORM_effective_slope, Type: Double, Alias: NORM_effective_slope\n",
      "Name: walk_score, Type: Double, Alias: walk_score\n",
      "Name: sidewalk_score, Type: Double, Alias: sidewalk_score\n",
      "Name: park_score, Type: Double, Alias: park_score\n",
      "Name: trail_score, Type: Double, Alias: trail_score\n",
      "Name: street_score, Type: Double, Alias: street_score\n",
      "Name: bike_score, Type: Double, Alias: bike_score\n",
      "Name: unadjusted_walkscore, Type: Double, Alias: unadjusted_walkscore\n",
      "Name: NORM_Combined_effective_area, Type: Double, Alias: NORM_Combined_effective_area\n",
      "Name: slope_scaler, Type: Double, Alias: slope_scaler\n",
      "Name: effective_speed_limit_scaler, Type: Double, Alias: effective_speed_limit_scaler\n",
      "Name: business_density_scaler, Type: Double, Alias: business_density_scaler\n",
      "Name: crash_density_scaler, Type: Double, Alias: crash_density_scaler\n",
      "Name: crime_density_scaler, Type: Double, Alias: crime_density_scaler\n",
      "Name: walkscore_negative, Type: Double, Alias: walkscore_negative\n"
     ]
    }
   ],
   "source": [
    "check_fields = arcpy.ListFields(exported_fishnet_layer)\n",
    "\n",
    "print(\"Field Names and Types:\")\n",
    "for field in check_fields:\n",
    "    print(f\"Name: {field.name}, Type: {field.type}, Alias: {field.aliasName}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted fields: ['FID_walkscore_fishnet', 'GRID_ID', 'total_area', 'SUM_proportional_population', 'Grid_Slope_MEAN', 'Sidewalks_Slope_Mean', 'Streets_Slope_Mean', 'MultiUseTrails_Slope_Mean', 'trails_Slope_Mean', 'Industrial_SUM_Industrial_effective_area', 'ParkingLots_SUM_ParkingLots_effective_area', 'GolfCourse_SUM_GolfCourse_effective_area', 'Cemeteries_SUM_Cemeteries_effective_area', 'Hospitals_SUM_Hospitals_effective_area', 'Slope_MEAN', 'Bike_greenways_SUM_Bike_greenways_area', 'Bike_protected_SUM_Bike_protected_area', 'Bike_buffer_SUM_Bike_buffer_area', 'Healthy_Streets_SUM_Healthy_Streets_area', 'Parks_SUM_Parks_area', 'Universities_SUM_Universities_area', 'Sidewalks_SUM_Sidewalks_area', 'Plaza_SUM_Plaza_area', 'trails_SUM_trails_area', 'MultiUseTrails_SUM_MultiUseTrails_area', 'Streets_SUM_Streets_effective_area', 'population_SUM_proportional_population', 'SPD_Crime_Data_COUNT_Offense_ID', 'FID_neighborhoods', 'city', 'neighborhood_area', 'is_tourist', 'is_industrial', 'Fragment_Area', 'IsIndustrial', 'IsParkingLots', 'IsGolfCourse', 'IsCemeteries', 'IsHospitals', 'crime_density', 'crash_density', 'NORM_Industrial_SUM_Industrial_effective_area', 'NORM_ParkingLots_SUM_ParkingLots_effective_area', 'NORM_GolfCourse_SUM_GolfCourse_effective_area', 'NORM_Cemeteries_SUM_Cemeteries_effective_area', 'NORM_Hospitals_SUM_Hospitals_effective_area', 'NORM_Bike_greenways_SUM_Bike_greenways_area', 'NORM_Bike_protected_SUM_Bike_protected_area', 'NORM_Bike_buffer_SUM_Bike_buffer_area', 'NORM_Healthy_Streets_SUM_Healthy_Streets_area', 'NORM_Parks_SUM_Parks_area', 'NORM_Universities_SUM_Universities_area', 'NORM_Sidewalks_SUM_Sidewalks_area', 'NORM_Plaza_SUM_Plaza_area', 'NORM_trails_SUM_trails_area', 'NORM_MultiUseTrails_SUM_MultiUseTrails_area', 'NORM_Streets_SUM_Streets_effective_area', 'NORM_neighborhood_area', 'NORM_Fragment_Area', 'NORM_effective_slope', 'sidewalk_score', 'park_score', 'trail_score', 'street_score', 'bike_score', 'NORM_Combined_effective_area', 'walkscore_negative']\n",
      "Field pruning complete.\n"
     ]
    }
   ],
   "source": [
    "fields_to_keep = [\n",
    "    \"OBJECTID\", \"Shape\",\"NORM_total_area\", \"IndexID\", \"Shape_Length\", \"Shape_Area\", \"walk_score\",\"effective_slope\",\n",
    "    \"unadjusted_walkscore\", \"slope_scaler\", \"effective_speed_limit_scaler\",\"business_density_scaler\",\"crash_density_scaler\",\n",
    "    \"crime_density_scaler\",\"Max_Speed_Limit\",\"business_density\",'crash_density_normalized','crime_density_normalized','nested','latitude','longitude'\n",
    "]\n",
    "walkscore_points_layer = r\"C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\walkscore_fishnet\"\n",
    "# List all fields in the walkscore_fishnet_layer\n",
    "all_fields = arcpy.ListFields(walkscore_points_layer)\n",
    "\n",
    "# Create a list of fields to delete\n",
    "fields_to_delete = [field.name for field in all_fields if field.name not in fields_to_keep]\n",
    "\n",
    "# Delete the unwanted fields\n",
    "if fields_to_delete:\n",
    "    arcpy.management.DeleteField(walkscore_points_layer, fields_to_delete)\n",
    "    print(f\"Deleted fields: {fields_to_delete}\")\n",
    "else:\n",
    "    print(\"No fields to delete.\")\n",
    "\n",
    "print(\"Field pruning complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Field Names and Types:\n",
      "Name: OBJECTID, Type: OID, Alias: OBJECTID\n",
      "Name: Shape, Type: Geometry, Alias: Shape\n",
      "Name: IndexID, Type: Integer, Alias: IndexID\n",
      "Name: Max_Speed_Limit, Type: Double, Alias: MAX_effective_SPEEDLIMIT\n",
      "Name: effective_slope, Type: Double, Alias: effective_slope\n",
      "Name: business_density, Type: Double, Alias: business_density\n",
      "Name: nested, Type: String, Alias: nested\n",
      "Name: latitude, Type: Double, Alias: latitude\n",
      "Name: longitude, Type: Double, Alias: longitude\n",
      "Name: Shape_Length, Type: Double, Alias: Shape_Length\n",
      "Name: Shape_Area, Type: Double, Alias: Shape_Area\n",
      "Name: crime_density_normalized, Type: Double, Alias: crime_density_normalized\n",
      "Name: crash_density_normalized, Type: Double, Alias: crash_density_normalized\n",
      "Name: NORM_total_area, Type: Double, Alias: NORM_total_area\n",
      "Name: walk_score, Type: Double, Alias: walk_score\n",
      "Name: unadjusted_walkscore, Type: Double, Alias: unadjusted_walkscore\n",
      "Name: slope_scaler, Type: Double, Alias: slope_scaler\n",
      "Name: effective_speed_limit_scaler, Type: Double, Alias: effective_speed_limit_scaler\n",
      "Name: business_density_scaler, Type: Double, Alias: business_density_scaler\n",
      "Name: crash_density_scaler, Type: Double, Alias: crash_density_scaler\n",
      "Name: crime_density_scaler, Type: Double, Alias: crime_density_scaler\n"
     ]
    }
   ],
   "source": [
    "final_fields = arcpy.ListFields(walkscore_points_layer)\n",
    "\n",
    "print(\"Field Names and Types:\")\n",
    "for field in final_fields:\n",
    "    print(f\"Name: {field.name}, Type: {field.type}, Alias: {field.aliasName}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted fields: ['is_tourist', 'is_industrial', 'weighted_avg_walk_score']\n",
      "Field pruning complete.\n"
     ]
    }
   ],
   "source": [
    "neighborhood_fields_to_keep = [\n",
    "    \"OBJECTID\", \"Shape\", \"city\", \"nested\", \"neighborhood_area\", \"rank_normalized_walk_score\",\"Shape_Length\", \"Shape_Area\",'latitude','longitude'\n",
    "]\n",
    "neighborhood_walkscore_layer = \"walkscore_neighborhoods\"\n",
    "\n",
    "# List all fields in the walkscore_fishnet_layer\n",
    "all_fields = arcpy.ListFields(neighborhood_walkscore_layer)\n",
    "\n",
    "# Create a list of fields to delete\n",
    "fields_to_delete = [\n",
    "    field.name for field in all_fields \n",
    "    if field.name not in neighborhood_fields_to_keep and not field.name.startswith(\"SUM_COUNT\")\n",
    "]\n",
    "# Delete the unwanted fields\n",
    "if fields_to_delete:\n",
    "    arcpy.management.DeleteField(neighborhood_walkscore_layer, fields_to_delete)\n",
    "    print(f\"Deleted fields: {fields_to_delete}\")\n",
    "else:\n",
    "    print(\"No fields to delete.\")\n",
    "\n",
    "print(\"Field pruning complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "file_extension": ".py",
   "name": "python",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
