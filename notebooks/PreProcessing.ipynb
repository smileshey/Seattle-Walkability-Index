{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PreProcessing\n",
    "\n",
    "Now that we have the base layers organized into a base_layers group in the contents frame, we'll use python to iterate through each layer, preparing them to be used in the calculation of the walkscore in the next notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arcpy\n",
    "import arcpy.mp\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0: Formatting Fishnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "fishnet_layer = r\"C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\fishnet_clipped\"\n",
    "base_layers_group = r\"C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\"\n",
    "output_gdb = r\"C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\"\n",
    "geodatabase_path = r\"C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\"\n",
    "fishnet_clipped = r\"C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\fishnet_clipped\"\n",
    "\n",
    "arcpy.env.workspace = r\"C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\"\n",
    "fishnet_area_field = \"total_area\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = r\"C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\output\"\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Field Name: OBJECTID, Field Type: OID\n",
      "Field Name: Shape, Field Type: Geometry\n",
      "Field Name: Shape_Length, Field Type: Double\n",
      "Field Name: Shape_Area, Field Type: Double\n",
      "Field Name: IndexID, Field Type: Integer\n"
     ]
    }
   ],
   "source": [
    "fields = arcpy.ListFields(fishnet_layer)\n",
    "for field in fields:\n",
    "    print(f\"Field Name: {field.name}, Field Type: {field.type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IndexID field created and populated in fishnet_clipped.\n"
     ]
    }
   ],
   "source": [
    "# Add the IndexID field if it doesn't exist\n",
    "index_field = \"IndexID\"\n",
    "if not any(f.name == index_field for f in arcpy.ListFields(fishnet_clipped)):\n",
    "    arcpy.management.AddField(fishnet_clipped, index_field, \"LONG\")\n",
    "\n",
    "# Populate the IndexID field with unique values\n",
    "with arcpy.da.UpdateCursor(fishnet_clipped, [index_field]) as cursor:\n",
    "    for i, row in enumerate(cursor):\n",
    "        row[0] = i + 1\n",
    "        cursor.updateRow(row)\n",
    "\n",
    "print(\"IndexID field created and populated in fishnet_clipped.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mandatory Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_layers = [\n",
    "#     \"TreeCanopy\",\n",
    "#     \"Public_Amenities\",\n",
    "    \"Business_Amenities\",\n",
    "    \"Industrial\",\n",
    "    \"ParkingLots\",\n",
    "    \"GolfCourse\",\n",
    "    \"Cemeteries\",\n",
    "    \"Hospitals\",\n",
    "    \"Slope\",\n",
    "    \"Bike_greenways\",\n",
    "    \"Bike_protected\",\n",
    "    \"Bike_buffer\",\n",
    "    \"Healthy_Streets\",\n",
    "    \"Parks\",\n",
    "    \"Universities\",\n",
    "    \"Sidewalks\",\n",
    "    \"Plaza\",\n",
    "    \"trails\",\n",
    "    \"MultiUseTrails\",\n",
    "    \"Streets\",\n",
    "    \"population\",\n",
    "    \"SPD_Crime_Data\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for layer: C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\Business_Amenities\n",
      "Checking for layer: C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\Industrial\n",
      "Checking for layer: C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\ParkingLots\n",
      "Checking for layer: C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\GolfCourse\n",
      "Checking for layer: C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\Cemeteries\n",
      "Checking for layer: C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\Hospitals\n",
      "Checking for layer: C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\Slope\n",
      "Checking for layer: C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\Bike_greenways\n",
      "Checking for layer: C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\Bike_protected\n",
      "Checking for layer: C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\Bike_buffer\n",
      "Checking for layer: C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\Healthy_Streets\n",
      "Checking for layer: C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\Parks\n",
      "Checking for layer: C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\Universities\n",
      "Checking for layer: C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\Sidewalks\n",
      "Checking for layer: C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\Plaza\n",
      "Checking for layer: C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\trails\n",
      "Checking for layer: C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\MultiUseTrails\n",
      "Checking for layer: C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\Streets\n",
      "Checking for layer: C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\population\n",
      "Checking for layer: C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\SPD_Crime_Data\n"
     ]
    }
   ],
   "source": [
    "for layer_name in base_layers:\n",
    "    input_layer = f\"{base_layers_group}\\\\{layer_name}\"\n",
    "    print(f\"Checking for layer: {input_layer}\")  # Add this line\n",
    "    if not arcpy.Exists(input_layer):\n",
    "        print(f\"Layer {input_layer} does not exist. Skipping.\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_average_slope(fishnet_layer, slope_raster, features_layer, output_table):\n",
    "    arcpy.CheckOutExtension(\"Spatial\")\n",
    "    extracted_slope = arcpy.sa.ExtractByMask(slope_raster, features_layer)\n",
    "    temp_extracted_slope = f\"{output_gdb}\\\\temp_extracted_slope\"\n",
    "    extracted_slope.save(temp_extracted_slope)\n",
    "    arcpy.sa.ZonalStatisticsAsTable(fishnet_layer, \"IndexID\", temp_extracted_slope, output_table, \"NODATA\", \"MEAN\")\n",
    "    arcpy.management.Delete(temp_extracted_slope)\n",
    "\n",
    "def integrate_slope_and_area(intersect_output, slope_output_table, area_field, effective_slope_field):\n",
    "    arcpy.management.AddField(intersect_output, effective_slope_field, \"DOUBLE\")\n",
    "    slope_df = arcpy.da.TableToNumPyArray(slope_output_table, [\"IndexID\", \"MEAN\"])\n",
    "    slope_df = pd.DataFrame(slope_df)\n",
    "    with arcpy.da.UpdateCursor(intersect_output, [\"IndexID\", area_field, effective_slope_field]) as cursor:\n",
    "        for row in cursor:\n",
    "            mean_slope = slope_df.loc[slope_df[\"IndexID\"] == row[0], \"MEAN\"]\n",
    "            row[2] = (mean_slope.values[0] * row[1]) if not mean_slope.empty and row[1] is not None else 0\n",
    "            cursor.updateRow(row)\n",
    "\n",
    "def calculate_polygon_area(layer, area_field):\n",
    "    if not any(f.name.lower() == area_field.lower() for f in arcpy.ListFields(layer)):\n",
    "        arcpy.management.AddField(layer, area_field, \"DOUBLE\")\n",
    "    arcpy.management.CalculateGeometryAttributes(layer, [[area_field, \"AREA_GEODESIC\"]], area_unit=\"SQUARE_FEET_US\")\n",
    "\n",
    "def calculate_polyline_area_with_recalculated_length(layer, area_field, width_field):\n",
    "    recalculated_length_field = f\"{area_field}_len\"\n",
    "    if not any(f.name.lower() == recalculated_length_field.lower() for f in arcpy.ListFields(layer)):\n",
    "        arcpy.management.AddField(layer, recalculated_length_field, \"DOUBLE\")\n",
    "    arcpy.management.CalculateGeometryAttributes(layer, [[recalculated_length_field, \"LENGTH_GEODESIC\"]], length_unit=\"FEET_US\")\n",
    "    if not any(f.name.lower() == area_field.lower() for f in arcpy.ListFields(layer)):\n",
    "        arcpy.management.AddField(layer, area_field, \"DOUBLE\")\n",
    "    with arcpy.da.UpdateCursor(layer, [width_field, recalculated_length_field, area_field]) as cursor:\n",
    "        for row in cursor:\n",
    "            row[2] = row[0] * row[1] if row[0] is not None and row[1] is not None else 0\n",
    "            cursor.updateRow(row)\n",
    "\n",
    "def calculate_effective_area(layer, effective_area_field, length_field=\"Shape_Length\", width_field=\"street_width\", speed_limit_field=\"SPEEDLIMIT\", at_grade_field=\"at_grade_scalar\"):\n",
    "    recalculated_length_field = f\"{effective_area_field}_len\"\n",
    "    if not any(f.name.lower() == recalculated_length_field.lower() for f in arcpy.ListFields(layer)):\n",
    "        arcpy.management.AddField(layer, recalculated_length_field, \"DOUBLE\")\n",
    "    arcpy.management.CalculateGeometryAttributes(layer, [[recalculated_length_field, \"LENGTH_GEODESIC\"]], length_unit=\"FEET_US\")\n",
    "\n",
    "    if not any(f.name == effective_area_field for f in arcpy.ListFields(layer)):\n",
    "        arcpy.management.AddField(layer, effective_area_field, \"DOUBLE\")\n",
    "\n",
    "    with arcpy.da.UpdateCursor(layer, [recalculated_length_field, width_field, speed_limit_field, at_grade_field, effective_area_field]) as cursor:\n",
    "        for row in cursor:\n",
    "            if row[0] is not None and row[1] is not None and row[2] is not None and row[3] is not None:\n",
    "                row[4] = row[0] * row[1] * row[2] * row[3]\n",
    "            else:\n",
    "                row[4] = None\n",
    "            cursor.updateRow(row)\n",
    "    print(f\"Calculated effective area for {layer} and stored in {effective_area_field}.\")\n",
    "    \n",
    "def create_layer(input_layer, fclass_list, output_layer_name):\n",
    "    # Create a query to filter the input layer based on fclass values\n",
    "    fclass_query = f\"\"\"fclass IN ({','.join([f\"'{fc}'\" for fc in fclass_list])})\"\"\"\n",
    "    \n",
    "    # Create the output layer\n",
    "    arcpy.management.MakeFeatureLayer(input_layer, \"temp_layer\", fclass_query)\n",
    "    output_layer = f\"{workspace}\\\\{output_layer_name}\"\n",
    "    \n",
    "    # Check if the output layer already exists and delete it if it does\n",
    "    if arcpy.Exists(output_layer):\n",
    "        arcpy.management.Delete(output_layer)\n",
    "    \n",
    "    # Save the filtered features to a new feature class\n",
    "    arcpy.management.CopyFeatures(\"temp_layer\", output_layer)\n",
    "    print(f\"Created {output_layer_name} layer with {len(fclass_list)} fclass values.\")\n",
    "    \n",
    "def calculate_counts(input_layer, intersect_output, fishnet_layer, summary_output, id_field):\n",
    "    # Intersect the input layer (amenity or crime) with the fishnet\n",
    "    arcpy.analysis.Intersect([input_layer, fishnet_layer], intersect_output)\n",
    "\n",
    "    # Add IndexID to intersect output if it doesn't exist\n",
    "    if not any(f.name == \"IndexID\" for f in arcpy.ListFields(intersect_output)):\n",
    "        arcpy.management.AddField(intersect_output, \"IndexID\", \"LONG\")\n",
    "        # Populate the IndexID field in intersect output\n",
    "        with arcpy.da.UpdateCursor(intersect_output, [\"IndexID\"]) as cursor:\n",
    "            for i, row in enumerate(cursor):\n",
    "                row[0] = i + 1  # Assign sequential IDs for IndexID\n",
    "                cursor.updateRow(row)\n",
    "\n",
    "    # Calculate the count of points within each fishnet cell\n",
    "    arcpy.analysis.Statistics(intersect_output, summary_output, [[id_field, \"COUNT\"]], \"IndexID\")\n",
    "\n",
    "    # Join the summary table back to the fishnet layer\n",
    "    count_field = f\"COUNT_{id_field}\"\n",
    "    arcpy.management.JoinField(fishnet_layer, \"IndexID\", summary_output, \"IndexID\", [count_field])\n",
    "\n",
    "    # Update null values in the joined count field to 0\n",
    "    with arcpy.da.UpdateCursor(fishnet_layer, [count_field]) as cursor:\n",
    "        for row in cursor:\n",
    "            if row[0] is None:\n",
    "                row[0] = 0  # Set null counts to 0\n",
    "            cursor.updateRow(row)\n",
    "\n",
    "    print(f\"Joined {count_field} to {fishnet_layer} and created summary table {summary_output}.\")\n",
    "    \n",
    "def calculate_crime_density(input_layer, intersect_output, fishnet_layer, summary_output, id_field):\n",
    "    # Intersect the input crime data layer with the fishnet\n",
    "    arcpy.analysis.Intersect([input_layer, fishnet_layer], intersect_output)\n",
    "\n",
    "    # Add IndexID to intersect output if it doesn't exist\n",
    "    if not any(f.name == \"IndexID\" for f in arcpy.ListFields(intersect_output)):\n",
    "        arcpy.management.AddField(intersect_output, \"IndexID\", \"LONG\")\n",
    "        # Populate the IndexID field in intersect output\n",
    "        with arcpy.da.UpdateCursor(intersect_output, [\"IndexID\"]) as cursor:\n",
    "            for i, row in enumerate(cursor):\n",
    "                row[0] = i + 1\n",
    "                cursor.updateRow(row)\n",
    "\n",
    "    # Calculate the count of crimes within each fishnet grid cell\n",
    "    arcpy.analysis.Statistics(intersect_output, summary_output, [[id_field, \"COUNT\"]], \"IndexID\")\n",
    "\n",
    "    # Join the summary table back to the fishnet layer\n",
    "    count_field = f\"COUNT_{id_field}\"\n",
    "    arcpy.management.JoinField(fishnet_layer, \"IndexID\", summary_output, \"IndexID\", [count_field])\n",
    "\n",
    "    # Update null values in the joined count field to 0\n",
    "    with arcpy.da.UpdateCursor(fishnet_layer, [count_field]) as cursor:\n",
    "        for row in cursor:\n",
    "            if row[0] is None:\n",
    "                row[0] = 0  # Set null counts to 0\n",
    "            cursor.updateRow(row)\n",
    "\n",
    "    print(f\"Joined {count_field} to {fishnet_layer} and created summary table {summary_output}.\")\n",
    "\n",
    "    \n",
    "def calculate_max_speed_limit(intersect_layer, fishnet_layer, output_table, speed_limit_field):\n",
    "    # Calculate the max speed limit for each intersected grid cell\n",
    "    arcpy.analysis.Statistics(intersect_layer, output_table, [[speed_limit_field, \"MAX\"]], \"IndexID\")\n",
    "    # Join the result back to the fishnet layer\n",
    "    arcpy.management.JoinField(fishnet_layer, \"IndexID\", output_table, \"IndexID\", [\"MAX_\" + speed_limit_field])\n",
    "    # Rename the field to Max_Speed_Limit\n",
    "    arcpy.management.AlterField(fishnet_layer, \"MAX_\" + speed_limit_field, \"Max_Speed_Limit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating the Effective Slope\n",
    "\n",
    "Since the slope data is provided in a raster, I'll need to segment this data to use only the slope data pertinent to the layers in my dataset. In the below function, we'll take the raster data and mask it with the layers provided in comb_feats, a list of features that we'll use to calculate the average slope.\n",
    "\n",
    "Using these combined features we can determine the exact slope of the sidewalk in a fishnet grid, rather than use the average slope over a grid as a proxy for the slope of the infrastructure a person will actually be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted existing C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\walkscore_fishnet.\n",
      "Copied fishnet_clipped to walkscore_fishnet.\n",
      "Index field populated with unique values.\n",
      "Calculated total area for each fishnet grid cell.\n"
     ]
    }
   ],
   "source": [
    "arcpy.env.overwriteOutput = True  # Allow outputs to be overwritten\n",
    "\n",
    "# Get the spatial reference of the fishnet layer\n",
    "fishnet_sr = arcpy.Describe(fishnet_layer).spatialReference\n",
    "\n",
    "# Define the walkscore_fishnet_layer\n",
    "walkscore_fishnet_layer = f\"{output_gdb}\\\\walkscore_fishnet\"\n",
    "\n",
    "# Check if the walkscore_fishnet_layer exists and delete it if it does\n",
    "if arcpy.Exists(walkscore_fishnet_layer):\n",
    "    arcpy.management.Delete(walkscore_fishnet_layer)\n",
    "    print(f\"Deleted existing {walkscore_fishnet_layer}.\")\n",
    "\n",
    "# Create a copy of the fishnet layer to work on\n",
    "arcpy.management.CopyFeatures(fishnet_layer, walkscore_fishnet_layer)\n",
    "print(\"Copied fishnet_clipped to walkscore_fishnet.\")\n",
    "\n",
    "# Add a new field for indexing and populate it with unique values\n",
    "index_field = \"IndexID\"\n",
    "if not any(f.name == index_field for f in arcpy.ListFields(walkscore_fishnet_layer)):\n",
    "    arcpy.management.AddField(walkscore_fishnet_layer, index_field, \"LONG\")\n",
    "\n",
    "# Populate the new index field with unique values\n",
    "with arcpy.da.UpdateCursor(walkscore_fishnet_layer, [index_field]) as cursor:\n",
    "    for i, row in enumerate(cursor):\n",
    "        row[0] = i + 1\n",
    "        cursor.updateRow(row)\n",
    "\n",
    "print(\"Index field populated with unique values.\")\n",
    "\n",
    "# Add a new field for total area if it doesn't exist\n",
    "total_area_field = \"total_area\"\n",
    "if not any(f.name == total_area_field for f in arcpy.ListFields(walkscore_fishnet_layer)):\n",
    "    arcpy.management.AddField(walkscore_fishnet_layer, total_area_field, \"DOUBLE\")\n",
    "\n",
    "# Calculate the total area for each fishnet grid cell\n",
    "arcpy.management.CalculateGeometryAttributes(walkscore_fishnet_layer, [[total_area_field, \"AREA_GEODESIC\"]])\n",
    "print(\"Calculated total area for each fishnet grid cell.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Process Each Layer and Calculate Allocations for each Fishnet Grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Effective Area Scalers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# norm_fields = [sidewalk_score_field, park_score_field, trail_score_field, street_score_field, bike_score_field]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalers = {\n",
    "    \"parkinglots\": 25,\n",
    "    \"industrial\": 25,\n",
    "    \"golfcourse\": 15,\n",
    "    \"hospitals\": 15,\n",
    "    \"cemeteries\": 15\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing Public Amenity Data & Separating Out Amenity Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique fclass values: 108\n",
      "Unique fclass values:\n",
      "sports_shop\n",
      "recycling_clothes\n",
      "bicycle_rental\n",
      "clinic\n",
      "library\n",
      "school\n",
      "wayside_shrine\n",
      "chemist\n",
      "bakery\n",
      "vending_any\n",
      "memorial\n",
      "jeweller\n",
      "stationery\n",
      "department_store\n",
      "computer_shop\n",
      "dentist\n",
      "recycling_glass\n",
      "video_shop\n",
      "garden_centre\n",
      "butcher\n",
      "greengrocer\n",
      "picnic_site\n",
      "water_well\n",
      "beverages\n",
      "optician\n",
      "kindergarten\n",
      "bar\n",
      "car_rental\n",
      "sports_centre\n",
      "shoe_shop\n",
      "vending_parking\n",
      "camera_surveillance\n",
      "guesthouse\n",
      "swimming_pool\n",
      "wastewater_plant\n",
      "ruins\n",
      "college\n",
      "telephone\n",
      "fountain\n",
      "car_dealership\n",
      "pitch\n",
      "courthouse\n",
      "motel\n",
      "tower\n",
      "hotel\n",
      "market_place\n",
      "post_office\n",
      "car_sharing\n",
      "biergarten\n",
      "general\n",
      "furniture_shop\n",
      "mobile_phone_shop\n",
      "fast_food\n",
      "convenience\n",
      "toy_shop\n",
      "artwork\n",
      "dog_park\n",
      "veterinary\n",
      "arts_centre\n",
      "doctors\n",
      "post_box\n",
      "recycling_paper\n",
      "bookshop\n",
      "pharmacy\n",
      "shelter\n",
      "travel_agent\n",
      "hospital\n",
      "food_court\n",
      "newsagent\n",
      "comms_tower\n",
      "museum\n",
      "university\n",
      "drinking_water\n",
      "cafe\n",
      "pub\n",
      "doityourself\n",
      "nightclub\n",
      "vending_machine\n",
      "laundry\n",
      "bench\n",
      "embassy\n",
      "outdoor_shop\n",
      "bank\n",
      "theatre\n",
      "hostel\n",
      "atm\n",
      "waste_basket\n",
      "bicycle_shop\n",
      "mall\n",
      "community_centre\n",
      "attraction\n",
      "clothes\n",
      "recycling\n",
      "restaurant\n",
      "hairdresser\n",
      "monument\n",
      "car_wash\n",
      "toilet\n",
      "supermarket\n",
      "beauty_shop\n",
      "town_hall\n",
      "cinema\n",
      "florist\n",
      "viewpoint\n",
      "tourist_info\n",
      "fire_station\n",
      "gift_shop\n",
      "playground\n"
     ]
    }
   ],
   "source": [
    "workspace = r\"C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\"\n",
    "layer = r\"C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\PointsofInterest\"\n",
    "\n",
    "# Use a set to collect unique fclass values\n",
    "fclass_set = set()\n",
    "\n",
    "# Use a SearchCursor to iterate through the fclass field\n",
    "with arcpy.da.SearchCursor(layer, [\"fclass\"]) as cursor:\n",
    "    for row in cursor:\n",
    "        fclass_set.add(row[0])\n",
    "\n",
    "# Print the unique fclass values and their count\n",
    "unique_fclass_count = len(fclass_set)\n",
    "print(f\"Number of unique fclass values: {unique_fclass_count}\")\n",
    "print(\"Unique fclass values:\")\n",
    "for fclass in fclass_set:\n",
    "    print(fclass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "business_amenities = [\n",
    "    'supermarket', 'convenience', 'greengrocer', 'butcher', 'department_store', 'mall', \n",
    "    'gift_shop', 'shoe_shop', 'clothes', 'bookshop', 'stationery', 'furniture_shop', 'jeweller', \n",
    "    'computer_shop', 'mobile_phone_shop', 'outdoor_shop', 'general', 'florist', 'toy_shop',\n",
    "    'beauty_shop', 'laundry', 'bank', 'atm', 'cafe', 'restaurant', \n",
    "    'pub', 'bar', 'fast_food', 'bakery', 'food_court', 'beverages', 'nightclub', 'car_sharing',\n",
    "    'car_wash', 'video_shop', 'vending_any', 'theatre', 'museum', 'attraction', 'cinema',\n",
    "    'market_place', 'mobile_phone_shop', 'bookshop', 'laundry', 'mobile_phone_shop',\n",
    "    'garden_centre','doityourself','hairdresser','bicycle_shop','biergarten','sports_shop'\n",
    "]\n",
    "public_amenities = [\n",
    "    'bench', 'drinking_water', 'waste_basket', 'library', 'post_box','post_office', 'recycling', \n",
    "    'recycling_glass', 'recycling_paper', 'vending_machine', 'artwork', 'tourist_info',\n",
    "    'viewpoint', 'monument', 'picnic_site', 'memorial', 'fountain', 'shelter', 'public_building',\n",
    "    'arts_centre','courthouse','community_centre'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main Processing Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing layer: Business_Amenities\n",
      "Joined COUNT_osm_business_id to C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\walkscore_fishnet and created summary table C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\Business_Amenities_sum.\n",
      "Processing layer: Industrial\n",
      "Processing layer: ParkingLots\n",
      "Processing layer: GolfCourse\n",
      "Processing layer: Cemeteries\n",
      "Processing layer: Hospitals\n",
      "Processing layer: Slope\n",
      "Layer Slope does not have a shapeType attribute. Skipping.\n",
      "Processing layer: Bike_greenways\n",
      "Processing layer: Bike_protected\n",
      "Processing layer: Bike_buffer\n",
      "Processing layer: Healthy_Streets\n",
      "Processing layer: Parks\n",
      "Processing layer: Universities\n",
      "Processing layer: Sidewalks\n",
      "Processing layer: Plaza\n",
      "Processing layer: trails\n",
      "Processing layer: MultiUseTrails\n",
      "Processing layer: Streets\n",
      "Calculated effective area for C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\Streets_int and stored in Streets_effective_area.\n",
      "Processing layer: population\n",
      "Processing layer: SPD_Crime_Data\n",
      "Joined COUNT_Offense_ID to C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\walkscore_fishnet and created summary table C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\SPD_Crime_Data_sum.\n",
      "Main processing complete for all layers.\n"
     ]
    }
   ],
   "source": [
    "# Main processing loop for preprocessing layers\n",
    "created_layers = []\n",
    "\n",
    "for layer_name in base_layers:\n",
    "    print(f\"Processing layer: {layer_name}\")\n",
    "\n",
    "    input_layer = f\"{base_layers_group}\\\\{layer_name}\"\n",
    "    \n",
    "    if not arcpy.Exists(input_layer):\n",
    "        print(f\"Layer {input_layer} does not exist. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    desc = arcpy.Describe(input_layer)\n",
    "    if hasattr(desc, \"shapeType\"):\n",
    "        geometry_type = desc.shapeType\n",
    "    else:\n",
    "        print(f\"Layer {layer_name} does not have a shapeType attribute. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    # Handle SPD_crime_data layer\n",
    "    if layer_name.lower() == \"spd_crime_data\":\n",
    "        intersect_output = f\"{workspace}\\\\{layer_name}_intersect\"\n",
    "        summary_output = f\"{workspace}\\\\{layer_name}_sum\"\n",
    "        id_field = \"Offense_ID\"  # Assuming this is the field representing unique crime incidents\n",
    "        calculate_counts(input_layer, intersect_output, walkscore_fishnet_layer, summary_output, id_field)\n",
    "        created_layers.append(summary_output)\n",
    "        continue\n",
    "\n",
    "    # Handle Point layers (e.g., _Amenities)\n",
    "    if geometry_type == \"Point\" and layer_name.endswith(\"_Amenities\"):\n",
    "        intersect_output = f\"{workspace}\\\\{layer_name}_intersect\"\n",
    "        summary_output = f\"{workspace}\\\\{layer_name}_sum\"\n",
    "        id_field = \"osm_business_id\" if \"Business\" in layer_name else \"osm_public_id\" if \"Public\" in layer_name else \"osm_id\"\n",
    "        calculate_counts(input_layer, intersect_output, walkscore_fishnet_layer, summary_output, id_field)\n",
    "        created_layers.append(summary_output)\n",
    "        continue\n",
    "\n",
    "    # Handle Polygons and Polylines as before\n",
    "    input_layer_sr = desc.spatialReference\n",
    "    fishnet_sr = arcpy.Describe(walkscore_fishnet_layer).spatialReference\n",
    "    projected_layer = f\"{workspace}\\\\{layer_name}_proj\"\n",
    "    \n",
    "    if input_layer_sr.name != fishnet_sr.name:\n",
    "        if arcpy.Exists(projected_layer):\n",
    "            arcpy.management.Delete(projected_layer)\n",
    "        arcpy.management.Project(input_layer, projected_layer, fishnet_sr)\n",
    "    else:\n",
    "        projected_layer = input_layer\n",
    "    \n",
    "    intersect_output = f\"{workspace}\\\\{layer_name}_int\"\n",
    "    \n",
    "    if arcpy.Exists(intersect_output):\n",
    "        arcpy.management.Delete(intersect_output)\n",
    "    \n",
    "    arcpy.analysis.Intersect([walkscore_fishnet_layer, projected_layer], intersect_output)\n",
    "    \n",
    "    if layer_name.lower() == \"streets\":\n",
    "        effective_area_field = f\"{layer_name}_effective_area\"\n",
    "        calculate_effective_area(intersect_output, effective_area_field)\n",
    "        area_field = effective_area_field\n",
    "        \n",
    "        # Run the calculate_max_speed_limit function here for the Streets layer\n",
    "        output_table = f\"{output_gdb}\\\\max_speed_limit_Streets_int\"\n",
    "        calculate_max_speed_limit(intersect_output, walkscore_fishnet_layer, output_table, \"effective_SPEEDLIMIT\")\n",
    "        \n",
    "    elif layer_name.lower() in scalers.keys():\n",
    "        area_field = f\"{layer_name}_area\"\n",
    "        area_field = area_field.replace(\"-\", \"_\").replace(\" \", \"_\")\n",
    "        if geometry_type == \"Polygon\":\n",
    "            calculate_polygon_area(intersect_output, area_field)\n",
    "        effective_area_field = f\"{layer_name}_effective_area\"\n",
    "        if not any(f.name == effective_area_field for f in arcpy.ListFields(intersect_output)):\n",
    "            arcpy.management.AddField(intersect_output, effective_area_field, \"DOUBLE\")\n",
    "        scaler = scalers[layer_name.lower()]\n",
    "        with arcpy.da.UpdateCursor(intersect_output, [area_field, effective_area_field]) as cursor:\n",
    "            for row in cursor:\n",
    "                if row[0] is not None:\n",
    "                    row[1] = row[0] * scaler\n",
    "                else:\n",
    "                    row[1] = None\n",
    "                cursor.updateRow(row)\n",
    "        area_field = effective_area_field\n",
    "    else:\n",
    "        area_field = f\"{layer_name}_area\"\n",
    "        area_field = area_field.replace(\"-\", \"_\").replace(\" \", \"_\")\n",
    "        if geometry_type == \"Polygon\":\n",
    "            calculate_polygon_area(intersect_output, area_field)\n",
    "        elif geometry_type == \"Polyline\":\n",
    "            width_field = None\n",
    "            for field in arcpy.ListFields(intersect_output):\n",
    "                if field.name.lower().endswith(\"width\"):\n",
    "                    width_field = field.name\n",
    "            if width_field:\n",
    "                calculate_polyline_area_with_recalculated_length(intersect_output, area_field, width_field)\n",
    "            else:\n",
    "                print(f\"Width field not found for {layer_name}, skipping area calculation.\")\n",
    "\n",
    "    if not any(f.name.lower() == area_field.lower() for f in arcpy.ListFields(intersect_output)):\n",
    "        print(f\"Area field {area_field} was not created for {layer_name}, skipping summary statistics.\")\n",
    "        continue\n",
    "\n",
    "    summary_output = f\"{workspace}\\\\{layer_name}_sum\"\n",
    "    if arcpy.Exists(summary_output):\n",
    "        arcpy.management.Delete(summary_output)\n",
    "\n",
    "    if not any(f.name == index_field for f in arcpy.ListFields(intersect_output)):\n",
    "        arcpy.management.AddField(intersect_output, index_field, \"LONG\")\n",
    "        with arcpy.da.UpdateCursor(intersect_output, [index_field]) as cursor:\n",
    "            for i, row in enumerate(cursor):\n",
    "                row[0] = i + 1\n",
    "                cursor.updateRow(row)\n",
    "\n",
    "    arcpy.analysis.Statistics(intersect_output, summary_output, [[area_field, \"SUM\"]], index_field)\n",
    "    created_layers.append(summary_output)\n",
    "\n",
    "print(\"Main processing complete for all layers.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Slope Processing Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing slope for layer: Sidewalks\n",
      "Calculating slope for Sidewalks\n",
      "Processing slope for layer: Streets\n",
      "Calculating slope for Streets\n",
      "Processing slope for layer: MultiUseTrails\n",
      "Calculating slope for MultiUseTrails\n",
      "Processing slope for layer: trails\n",
      "Calculating slope for trails\n",
      "Slope processing complete for all layers.\n",
      "Sample data from all_data for debugging:\n",
      "IndexID: 1, Data: {'Sidewalks_Slope_Mean': 2.6028627527171175, 'Streets_Slope_Mean': 2.4719635209729587, 'MultiUseTrails_Slope_Mean': None, 'trails_Slope_Mean': None, 'Grid_Slope_MEAN': 3.0777811209360757}\n",
      "IndexID: 2, Data: {'Sidewalks_Slope_Mean': 1.8433810224135716, 'Streets_Slope_Mean': 0.9660569752256075, 'MultiUseTrails_Slope_Mean': None, 'trails_Slope_Mean': None, 'Grid_Slope_MEAN': 2.2087172894842113}\n",
      "IndexID: 3, Data: {'Sidewalks_Slope_Mean': None, 'Streets_Slope_Mean': 3.095643554415022, 'MultiUseTrails_Slope_Mean': None, 'trails_Slope_Mean': 2.7575330917651844, 'Grid_Slope_MEAN': 2.269062724378373}\n",
      "IndexID: 4, Data: {'Sidewalks_Slope_Mean': 4.099437493544359, 'Streets_Slope_Mean': None, 'MultiUseTrails_Slope_Mean': None, 'trails_Slope_Mean': None, 'Grid_Slope_MEAN': 4.542804545164107}\n",
      "IndexID: 5, Data: {'Sidewalks_Slope_Mean': None, 'Streets_Slope_Mean': None, 'MultiUseTrails_Slope_Mean': None, 'trails_Slope_Mean': None, 'Grid_Slope_MEAN': 32.985248031038225}\n",
      "Combined slope mean calculated and updated.\n"
     ]
    }
   ],
   "source": [
    "slope_layers = ['Sidewalks', 'Streets', 'MultiUseTrails', 'trails']\n",
    "\n",
    "# Calculate slope for entire grid\n",
    "grid_slope_output_table = f\"{output_gdb}\\\\grid_slope\"\n",
    "calculate_average_slope(walkscore_fishnet_layer, \"Slope\", walkscore_fishnet_layer, grid_slope_output_table)\n",
    "\n",
    "# Check if \"Grid_Slope_MEAN\" field already exists, and join or alter as necessary\n",
    "if not any(f.name == \"Grid_Slope_MEAN\" for f in arcpy.ListFields(walkscore_fishnet_layer)):\n",
    "    arcpy.management.JoinField(walkscore_fishnet_layer, \"IndexID\", grid_slope_output_table, \"IndexID\", \"MEAN\")\n",
    "    arcpy.management.AlterField(walkscore_fishnet_layer, \"MEAN\", \"Grid_Slope_MEAN\")\n",
    "else:\n",
    "    print(\"Grid_Slope_MEAN field already exists. Skipping join and alter operations.\")\n",
    "\n",
    "# Process slope for specific polyline layers\n",
    "for layer_name in slope_layers:\n",
    "    print(f\"Processing slope for layer: {layer_name}\")\n",
    "\n",
    "    input_layer = f\"{base_layers_group}\\\\{layer_name}\"\n",
    "    \n",
    "    if not arcpy.Exists(input_layer):\n",
    "        print(f\"Layer {input_layer} does not exist. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    desc = arcpy.Describe(input_layer)\n",
    "    if hasattr(desc, \"shapeType\"):\n",
    "        geometry_type = desc.shapeType\n",
    "        if geometry_type != \"Polyline\":\n",
    "            print(f\"Layer {layer_name} is not a polyline. Skipping slope calculation.\")\n",
    "            continue\n",
    "    else:\n",
    "        print(f\"Layer {layer_name} does not have a shapeType attribute. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    intersect_output = f\"{workspace}\\\\{layer_name}_int\"\n",
    "    slope_output_table = f\"{workspace}\\\\{layer_name}_slope\"\n",
    "\n",
    "    if arcpy.Exists(intersect_output):\n",
    "        print(f\"Calculating slope for {layer_name}\")\n",
    "        calculate_average_slope(intersect_output, \"Slope\", intersect_output, slope_output_table)\n",
    "\n",
    "        effective_slope_field = f\"{layer_name}_Slope_Mean\"\n",
    "\n",
    "        # Ensure the effective_slope_field exists\n",
    "        if not any(f.name == effective_slope_field for f in arcpy.ListFields(walkscore_fishnet_layer)):\n",
    "            arcpy.management.AddField(walkscore_fishnet_layer, effective_slope_field, \"DOUBLE\")\n",
    "\n",
    "        slope_df = arcpy.da.TableToNumPyArray(slope_output_table, [\"IndexID\", \"MEAN\"])\n",
    "        slope_dict = {row[\"IndexID\"]: row[\"MEAN\"] for row in slope_df}\n",
    "\n",
    "        with arcpy.da.UpdateCursor(walkscore_fishnet_layer, [\"IndexID\", effective_slope_field]) as cursor:\n",
    "            for row in cursor:\n",
    "                row[1] = slope_dict.get(row[0], None)\n",
    "                cursor.updateRow(row)\n",
    "    else:\n",
    "        print(f\"Intersect output for {layer_name} does not exist. Skipping.\")\n",
    "\n",
    "print(\"Slope processing complete for all layers.\")\n",
    "\n",
    "# Ensure that the combined slope mean is calculated correctly\n",
    "slope_fields = [f\"{layer}_Slope_Mean\" for layer in slope_layers if any(f.name == f\"{layer}_Slope_Mean\" for f in arcpy.ListFields(walkscore_fishnet_layer))]\n",
    "\n",
    "# Collect all necessary data in one go\n",
    "all_data = {\n",
    "    row[0]: {\n",
    "        field: row[idx + 1] for idx, field in enumerate(slope_fields + [\"Grid_Slope_MEAN\"])\n",
    "    } for row in arcpy.da.SearchCursor(walkscore_fishnet_layer, [\"IndexID\"] + slope_fields + [\"Grid_Slope_MEAN\"])\n",
    "}\n",
    "\n",
    "# Debug: Print a few entries to check data integrity\n",
    "print(\"Sample data from all_data for debugging:\")\n",
    "for idx, (key, value) in enumerate(all_data.items()):\n",
    "    if idx < 5:  # Print first 5 entries\n",
    "        print(f\"IndexID: {key}, Data: {value}\")\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# Update the effective_slope field based on the collected data\n",
    "if not any(f.name == \"effective_slope\" for f in arcpy.ListFields(walkscore_fishnet_layer)):\n",
    "    arcpy.management.AddField(walkscore_fishnet_layer, \"effective_slope\", \"DOUBLE\")\n",
    "\n",
    "with arcpy.da.UpdateCursor(walkscore_fishnet_layer, [\"IndexID\", \"effective_slope\"] + slope_fields + [\"Grid_Slope_MEAN\"]) as cursor:\n",
    "    for row in cursor:\n",
    "        index_id = row[0]\n",
    "        sidewalks_slope = all_data[index_id].get(\"Sidewalks_Slope_Mean\")\n",
    "        multiuse_trails_slope = all_data[index_id].get(\"MultiUseTrails_Slope_Mean\")\n",
    "        streets_slope = all_data[index_id].get(\"Streets_Slope_Mean\")\n",
    "        grid_slope_mean = all_data[index_id][\"Grid_Slope_MEAN\"]\n",
    "\n",
    "        if sidewalks_slope is not None:\n",
    "            slope_mean = sidewalks_slope\n",
    "        elif multiuse_trails_slope is not None:\n",
    "            slope_mean = multiuse_trails_slope\n",
    "        elif streets_slope is not None:\n",
    "            slope_mean = streets_slope\n",
    "        else:\n",
    "            slope_mean = grid_slope_mean\n",
    "\n",
    "        row[1] = slope_mean\n",
    "        cursor.updateRow(row)\n",
    "\n",
    "print(\"Combined slope mean calculated and updated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating an Index Field for walkscore_fishnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Populate the new index field with unique values\n",
    "with arcpy.da.UpdateCursor(walkscore_fishnet_layer, [index_field]) as cursor:\n",
    "    for i, row in enumerate(cursor):\n",
    "        row[0] = i + 1\n",
    "        cursor.updateRow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3:  Join Summary Statistic Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deleted existing summary table\n",
      "Created merged_sums table.\n"
     ]
    }
   ],
   "source": [
    "merged_summary = f\"{output_gdb}\\\\merged_sums\"\n",
    "\n",
    "if arcpy.Exists(merged_summary):\n",
    "    arcpy.management.Delete(merged_summary)\n",
    "    print('deleted existing summary table')\n",
    "    \n",
    "arcpy.management.CreateTable(output_gdb, \"merged_sums\")\n",
    "print(\"Created merged_sums table.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated merged summary table created successfully.\n"
     ]
    }
   ],
   "source": [
    "# Add IndexID field to the merged summary table if it doesn't exist\n",
    "if not any(f.name == index_field for f in arcpy.ListFields(merged_summary)):\n",
    "    arcpy.management.AddField(merged_summary, index_field, \"LONG\")\n",
    "\n",
    "# Create a dictionary to store the aggregated sums\n",
    "aggregated_sums = defaultdict(lambda: defaultdict(float))\n",
    "\n",
    "# Iterate through each summary table and aggregate values by IndexID\n",
    "for layer_name in base_layers:\n",
    "    summary_output = f\"{output_gdb}\\\\{layer_name}_sum\"\n",
    "    \n",
    "    # Verify if summary_output exists\n",
    "    if not arcpy.Exists(summary_output):\n",
    "        print(f\"Summary table {summary_output} does not exist. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    fields = arcpy.ListFields(summary_output)\n",
    "    field_names = [f.name for f in fields if f.name != index_field]\n",
    "    \n",
    "    # Aggregate the summary fields into the dictionary\n",
    "    with arcpy.da.SearchCursor(summary_output, [index_field] + field_names) as cursor:\n",
    "        for row in cursor:\n",
    "            idx = row[0]\n",
    "            for i, field_name in enumerate(field_names):\n",
    "                value = row[i+1] if row[i+1] is not None else 0\n",
    "                aggregated_sums[idx][f\"{layer_name}_{field_name}\"] += value\n",
    "\n",
    "# Add aggregated fields to the merged summary table\n",
    "for layer_name in base_layers:\n",
    "    summary_output = f\"{output_gdb}\\\\{layer_name}_sum\"\n",
    "    \n",
    "    # Verify if summary_output exists\n",
    "    if not arcpy.Exists(summary_output):\n",
    "        print(f\"Summary table {summary_output} does not exist. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    fields = arcpy.ListFields(summary_output)\n",
    "    for field in fields:\n",
    "        if field.name != index_field:\n",
    "            field_name = f\"{layer_name}_{field.name}\"\n",
    "            if not any(f.name == field_name for f in arcpy.ListFields(merged_summary)):\n",
    "                arcpy.management.AddField(merged_summary, field_name, \"DOUBLE\")\n",
    "\n",
    "# Insert the aggregated sums into the merged summary table\n",
    "field_names_to_insert = [index_field] + [f\"{layer_name}_{field.name}\" for layer_name in base_layers for field in arcpy.ListFields(f\"{output_gdb}\\\\{layer_name}_sum\") if field.name != index_field]\n",
    "with arcpy.da.InsertCursor(merged_summary, field_names_to_insert) as cursor:\n",
    "    for idx, fields in aggregated_sums.items():\n",
    "        row = [idx] + [fields.get(field_name, 0) for field_name in field_names_to_insert if field_name != index_field]\n",
    "        cursor.insertRow(row)\n",
    "\n",
    "print(\"Aggregated merged summary table created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fields in merged summary table: ['OBJECTID', 'IndexID', 'Business_Amenities_OBJECTID', 'Business_Amenities_FREQUENCY', 'Business_Amenities_COUNT_osm_business_id', 'Industrial_OBJECTID', 'Industrial_FREQUENCY', 'Industrial_SUM_Industrial_effective_area', 'ParkingLots_OBJECTID', 'ParkingLots_FREQUENCY', 'ParkingLots_SUM_ParkingLots_effective_area', 'GolfCourse_OBJECTID', 'GolfCourse_FREQUENCY', 'GolfCourse_SUM_GolfCourse_effective_area', 'Cemeteries_OBJECTID', 'Cemeteries_FREQUENCY', 'Cemeteries_SUM_Cemeteries_effective_area', 'Hospitals_OBJECTID', 'Hospitals_FREQUENCY', 'Hospitals_SUM_Hospitals_effective_area', 'Slope_OBJECTID', 'Slope_COUNT', 'Slope_AREA', 'Slope_MEAN', 'Bike_greenways_OBJECTID', 'Bike_greenways_FREQUENCY', 'Bike_greenways_SUM_Bike_greenways_area', 'Bike_protected_OBJECTID', 'Bike_protected_FREQUENCY', 'Bike_protected_SUM_Bike_protected_area', 'Bike_buffer_OBJECTID', 'Bike_buffer_FREQUENCY', 'Bike_buffer_SUM_Bike_buffer_area', 'Healthy_Streets_OBJECTID', 'Healthy_Streets_FREQUENCY', 'Healthy_Streets_SUM_Healthy_Streets_area', 'Parks_OBJECTID', 'Parks_FREQUENCY', 'Parks_SUM_Parks_area', 'Universities_OBJECTID', 'Universities_FREQUENCY', 'Universities_SUM_Universities_area', 'Sidewalks_OBJECTID', 'Sidewalks_FREQUENCY', 'Sidewalks_SUM_Sidewalks_area', 'Plaza_OBJECTID', 'Plaza_FREQUENCY', 'Plaza_SUM_Plaza_area', 'trails_OBJECTID', 'trails_FREQUENCY', 'trails_SUM_trails_area', 'MultiUseTrails_OBJECTID', 'MultiUseTrails_FREQUENCY', 'MultiUseTrails_SUM_MultiUseTrails_area', 'Streets_OBJECTID', 'Streets_FREQUENCY', 'Streets_SUM_Streets_effective_area', 'population_OBJECTID', 'population_FREQUENCY', 'population_SUM_population_area', 'SPD_Crime_Data_OBJECTID', 'SPD_Crime_Data_FREQUENCY', 'SPD_Crime_Data_COUNT_Offense_ID']\n",
      "IndexID already exists in C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\merged_sums.\n",
      "Verified IndexID in merged_sums.\n"
     ]
    }
   ],
   "source": [
    "# Path to the merged summary table\n",
    "merged_summary = r\"C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\merged_sums\"\n",
    "index_field = \"IndexID\"\n",
    "\n",
    "# Check if merged summary table exists\n",
    "if not arcpy.Exists(merged_summary):\n",
    "    raise ValueError(f\"{merged_summary} does not exist.\")\n",
    "\n",
    "# List all fields in the merged summary table for debugging\n",
    "fields = arcpy.ListFields(merged_summary)\n",
    "field_names = [field.name for field in fields]\n",
    "print(\"Fields in merged summary table:\", field_names)\n",
    "\n",
    "# Ensure IndexID exists in merged_sums\n",
    "if not any(f.name == index_field for f in fields):\n",
    "    print(f\"Adding {index_field} to {merged_summary}.\")\n",
    "    arcpy.management.AddField(merged_summary, index_field, \"LONG\")\n",
    "else:\n",
    "    print(f\"{index_field} already exists in {merged_summary}.\")\n",
    "\n",
    "print(\"Verified IndexID in merged_sums.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Join the Summary Statistics to the Fishnet Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    arcpy.management.JoinField(walkscore_fishnet_layer, \"IndexID\", merged_summary, \"IndexID\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during join: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding a neighborhood field to Walkscore Fishnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intersected walkscore fishnet with neighborhoods to create fragments.\n",
      "Using neighborhood field: nested\n",
      "Calculated area for each fragment.\n",
      "Overwritten C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\walkscore_fishnet with the intersected fragments, retaining neighborhood names.\n",
      "Fields in C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\walkscore_fishnet after processing:\n",
      "Name: OBJECTID, Type: OID\n",
      "Name: Shape, Type: Geometry\n",
      "Name: FID_walkscore_fishnet, Type: Integer\n",
      "Name: IndexID, Type: Integer\n",
      "Name: total_area, Type: Double\n",
      "Name: COUNT_osm_business_id, Type: Integer\n",
      "Name: Max_Speed_Limit, Type: Double\n",
      "Name: COUNT_Offense_ID, Type: Integer\n",
      "Name: Grid_Slope_MEAN, Type: Double\n",
      "Name: Sidewalks_Slope_Mean, Type: Double\n",
      "Name: Streets_Slope_Mean, Type: Double\n",
      "Name: MultiUseTrails_Slope_Mean, Type: Double\n",
      "Name: trails_Slope_Mean, Type: Double\n",
      "Name: effective_slope, Type: Double\n",
      "Name: IndexID_1, Type: Integer\n",
      "Name: Business_Amenities_OBJECTID, Type: Double\n",
      "Name: Business_Amenities_FREQUENCY, Type: Double\n",
      "Name: Business_Amenities_COUNT_osm_business_id, Type: Double\n",
      "Name: Industrial_OBJECTID, Type: Double\n",
      "Name: Industrial_FREQUENCY, Type: Double\n",
      "Name: Industrial_SUM_Industrial_effective_area, Type: Double\n",
      "Name: ParkingLots_OBJECTID, Type: Double\n",
      "Name: ParkingLots_FREQUENCY, Type: Double\n",
      "Name: ParkingLots_SUM_ParkingLots_effective_area, Type: Double\n",
      "Name: GolfCourse_OBJECTID, Type: Double\n",
      "Name: GolfCourse_FREQUENCY, Type: Double\n",
      "Name: GolfCourse_SUM_GolfCourse_effective_area, Type: Double\n",
      "Name: Cemeteries_OBJECTID, Type: Double\n",
      "Name: Cemeteries_FREQUENCY, Type: Double\n",
      "Name: Cemeteries_SUM_Cemeteries_effective_area, Type: Double\n",
      "Name: Hospitals_OBJECTID, Type: Double\n",
      "Name: Hospitals_FREQUENCY, Type: Double\n",
      "Name: Hospitals_SUM_Hospitals_effective_area, Type: Double\n",
      "Name: Slope_OBJECTID, Type: Double\n",
      "Name: Slope_COUNT, Type: Double\n",
      "Name: Slope_AREA, Type: Double\n",
      "Name: Slope_MEAN, Type: Double\n",
      "Name: Bike_greenways_OBJECTID, Type: Double\n",
      "Name: Bike_greenways_FREQUENCY, Type: Double\n",
      "Name: Bike_greenways_SUM_Bike_greenways_area, Type: Double\n",
      "Name: Bike_protected_OBJECTID, Type: Double\n",
      "Name: Bike_protected_FREQUENCY, Type: Double\n",
      "Name: Bike_protected_SUM_Bike_protected_area, Type: Double\n",
      "Name: Bike_buffer_OBJECTID, Type: Double\n",
      "Name: Bike_buffer_FREQUENCY, Type: Double\n",
      "Name: Bike_buffer_SUM_Bike_buffer_area, Type: Double\n",
      "Name: Healthy_Streets_OBJECTID, Type: Double\n",
      "Name: Healthy_Streets_FREQUENCY, Type: Double\n",
      "Name: Healthy_Streets_SUM_Healthy_Streets_area, Type: Double\n",
      "Name: Parks_OBJECTID, Type: Double\n",
      "Name: Parks_FREQUENCY, Type: Double\n",
      "Name: Parks_SUM_Parks_area, Type: Double\n",
      "Name: Universities_OBJECTID, Type: Double\n",
      "Name: Universities_FREQUENCY, Type: Double\n",
      "Name: Universities_SUM_Universities_area, Type: Double\n",
      "Name: Sidewalks_OBJECTID, Type: Double\n",
      "Name: Sidewalks_FREQUENCY, Type: Double\n",
      "Name: Sidewalks_SUM_Sidewalks_area, Type: Double\n",
      "Name: Plaza_OBJECTID, Type: Double\n",
      "Name: Plaza_FREQUENCY, Type: Double\n",
      "Name: Plaza_SUM_Plaza_area, Type: Double\n",
      "Name: trails_OBJECTID, Type: Double\n",
      "Name: trails_FREQUENCY, Type: Double\n",
      "Name: trails_SUM_trails_area, Type: Double\n",
      "Name: MultiUseTrails_OBJECTID, Type: Double\n",
      "Name: MultiUseTrails_FREQUENCY, Type: Double\n",
      "Name: MultiUseTrails_SUM_MultiUseTrails_area, Type: Double\n",
      "Name: Streets_OBJECTID, Type: Double\n",
      "Name: Streets_FREQUENCY, Type: Double\n",
      "Name: Streets_SUM_Streets_effective_area, Type: Double\n",
      "Name: population_OBJECTID, Type: Double\n",
      "Name: population_FREQUENCY, Type: Double\n",
      "Name: population_SUM_population_area, Type: Double\n",
      "Name: SPD_Crime_Data_OBJECTID, Type: Double\n",
      "Name: SPD_Crime_Data_FREQUENCY, Type: Double\n",
      "Name: SPD_Crime_Data_COUNT_Offense_ID, Type: Double\n",
      "Name: FID_neighborhoods, Type: Integer\n",
      "Name: city, Type: String\n",
      "Name: nested, Type: String\n",
      "Name: neighborhood_area, Type: Double\n",
      "Name: is_tourist, Type: Integer\n",
      "Name: business_density, Type: Double\n",
      "Name: SUM_COUNT_osm_business_id, Type: Double\n",
      "Name: SUM_COUNT_osm_business_id_1, Type: Double\n",
      "Name: SUM_COUNT_osm_business_id_12, Type: Double\n",
      "Name: unadjusted_walkscore_sum, Type: Double\n",
      "Name: Shape_Length, Type: Double\n",
      "Name: Shape_Area, Type: Double\n",
      "Name: Fragment_Area, Type: Double\n",
      "Fishnet fragments assigned to neighborhoods and saved back to walkscore_fishnet.\n"
     ]
    }
   ],
   "source": [
    "# Set environment settings\n",
    "arcpy.env.overwriteOutput = True  # Allow outputs to be overwritten\n",
    "\n",
    "# Define the input layers\n",
    "walkscore_fishnet = r\"C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\walkscore_fishnet\"\n",
    "neighborhoods = r\"C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\neighborhoods\"\n",
    "fishnet_neighborhoods_intersect = \"fishnet_neighborhoods_intersect\"\n",
    "neighborhood_field = \"nested\"  # Field name to be used from neighborhoods layer\n",
    "\n",
    "# Step 1: Ensure the spatial reference systems match\n",
    "walkscore_sr = arcpy.Describe(walkscore_fishnet).spatialReference\n",
    "neighborhoods_sr = arcpy.Describe(neighborhoods).spatialReference\n",
    "\n",
    "if walkscore_sr.name != neighborhoods_sr.name:\n",
    "    raise ValueError(\"Spatial references do not match between walkscore_fishnet and neighborhoods.\")\n",
    "\n",
    "# Step 2: Intersect fishnet with neighborhoods to split grids at boundaries\n",
    "arcpy.analysis.Intersect([walkscore_fishnet, neighborhoods], fishnet_neighborhoods_intersect)\n",
    "print(\"Intersected walkscore fishnet with neighborhoods to create fragments.\")\n",
    "\n",
    "# Step 3: Verify that the 'nested' field is present in the intersected layer\n",
    "fields = arcpy.ListFields(fishnet_neighborhoods_intersect)\n",
    "field_names = [field.name for field in fields]\n",
    "\n",
    "if neighborhood_field not in field_names:\n",
    "    raise ValueError(f\"Field '{neighborhood_field}' not found in the intersected layer.\")\n",
    "\n",
    "print(f\"Using neighborhood field: {neighborhood_field}\")\n",
    "\n",
    "# Optional: Calculate the area of each fragment for further analysis\n",
    "arcpy.management.AddField(fishnet_neighborhoods_intersect, \"Fragment_Area\", \"DOUBLE\")\n",
    "arcpy.management.CalculateGeometryAttributes(fishnet_neighborhoods_intersect, [[\"Fragment_Area\", \"AREA_GEODESIC\"]])\n",
    "print(\"Calculated area for each fragment.\")\n",
    "\n",
    "# Step 4: Use the intersected result directly as the new walkscore_fishnet\n",
    "# Rename the intersected layer to replace the original walkscore_fishnet\n",
    "arcpy.management.Delete(walkscore_fishnet)  # Delete the original fishnet to allow overwriting\n",
    "arcpy.management.Rename(fishnet_neighborhoods_intersect, walkscore_fishnet)\n",
    "print(f\"Overwritten {walkscore_fishnet} with the intersected fragments, retaining neighborhood names.\")\n",
    "\n",
    "# Verify the output\n",
    "print(f\"Fields in {walkscore_fishnet} after processing:\")\n",
    "fields = arcpy.ListFields(walkscore_fishnet)\n",
    "for field in fields:\n",
    "    print(f\"Name: {field.name}, Type: {field.type}\")\n",
    "\n",
    "print(\"Fishnet fragments assigned to neighborhoods and saved back to walkscore_fishnet.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaling MAX Speed Limit for Different Uses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "speed_limit_scalers = {\n",
    "    \"Industrial\": 2.0,\n",
    "    \"ParkingLots\": 1.5,\n",
    "    \"GolfCourse\": 1.2,\n",
    "    \"Cemeteries\": 1.1,\n",
    "    \"Hospitals\": 2.0,\n",
    "    \"MultiUseTrails\": 0.9,\n",
    "    \"Parks\": 0.75\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add binary fields to walkscore_fishnet_layer for each area type\n",
    "for area_type in speed_limit_scalers.keys():\n",
    "    binary_field = f\"Is{area_type.replace(' ', '')}\"\n",
    "    if not any(f.name == binary_field for f in arcpy.ListFields(walkscore_fishnet_layer)):\n",
    "        arcpy.management.AddField(walkscore_fishnet_layer, binary_field, \"SHORT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary fields updated based on effective area presence.\n"
     ]
    }
   ],
   "source": [
    "# Populate binary fields based on effective area presence\n",
    "for area_type in speed_limit_scalers.keys():\n",
    "    effective_area_field = f\"{area_type}_SUM_{area_type.replace(' ', '')}_effective_area\"\n",
    "    binary_field = f\"Is{area_type.replace(' ', '')}\"\n",
    "    \n",
    "    if any(f.name == effective_area_field for f in arcpy.ListFields(walkscore_fishnet_layer)):\n",
    "        with arcpy.da.UpdateCursor(walkscore_fishnet_layer, [effective_area_field, binary_field]) as cursor:\n",
    "            for row in cursor:\n",
    "                effective_area = row[0] if row[0] is not None else 0\n",
    "                row[1] = 1 if effective_area > 0 else 0\n",
    "                cursor.updateRow(row)\n",
    "\n",
    "print(\"Binary fields updated based on effective area presence.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max_Speed_Limit adjusted based on area type scalers with a ceiling on scaling.\n"
     ]
    }
   ],
   "source": [
    "# Set the maximum allowable scaler value (Industrial scaler)\n",
    "max_scaler_value = speed_limit_scalers[\"Hospitals\"]\n",
    "\n",
    "# Adjust Max_Speed_Limit using the binary fields and scalers with a cap on the scaling\n",
    "with arcpy.da.UpdateCursor(walkscore_fishnet_layer, [\"Max_Speed_Limit\"] + [f\"Is{area_type.replace(' ', '')}\" for area_type in speed_limit_scalers.keys()]) as cursor:\n",
    "    for row in cursor:\n",
    "        max_speed_limit = row[0]\n",
    "        applied_scaler = 1.0\n",
    "        \n",
    "        # Apply scalers based on binary fields\n",
    "        if max_speed_limit is not None:\n",
    "            for i, area_type in enumerate(speed_limit_scalers.keys(), start=1):\n",
    "                if row[i] == 1:  # If binary field is 1, apply the scaler\n",
    "                    applied_scaler *= speed_limit_scalers[area_type]\n",
    "                    \n",
    "                    # Ensure applied_scaler does not exceed the max_scaler_value\n",
    "                    if applied_scaler > max_scaler_value:\n",
    "                        applied_scaler = max_scaler_value\n",
    "                        break  # No need to continue if we've hit the max scaler\n",
    "\n",
    "            # Apply the final scaler to the max speed limit\n",
    "            max_speed_limit *= applied_scaler\n",
    "            row[0] = max_speed_limit\n",
    "            cursor.updateRow(row)\n",
    "\n",
    "print(\"Max_Speed_Limit adjusted based on area type scalers with a ceiling on scaling.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Finalizing Walkscore Fishnet\n",
    "\n",
    "Finally, we'll take the fishnet (walkscore_fishnet) and trim the fields down to only the mandatory fields (and permanent fields). This will include the calculation of the amenity density, which allows me to remove the count fields before passing the fishnet to the next notebook for walkscore calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "walkscore_fishnet = f\"{output_gdb}\\\\walkscore_fishnet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "arcpy.env.overwriteOutput = True  # Allow outputs to be overwritten\n",
    "\n",
    "# Define the input layers\n",
    "neighborhoods_layer = r\"C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\neighborhoods\"\n",
    "fishnet_layer = r\"C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\walkscore_fishnet\"\n",
    "tree_canopy_layer = r\"C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\TreeCanopy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_normalize(value, values):\n",
    "    sorted_values = sorted(values)\n",
    "    rank = sorted_values.index(value) + 1\n",
    "    return rank / len(values) * 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tree Canopy Cover By Fishnet Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 1: Summarize the Canopy Area by fishnet grid\n",
    "# canopy_summary_table = \"fishnet_canopy_density_summary\"\n",
    "# arcpy.analysis.Statistics(fishnet_layer, canopy_summary_table, \n",
    "#                           [[\"TreeCanopy_SUM_TreeCanopy_area\", \"SUM\"]], \n",
    "#                           [\"IndexID\"])  # Assuming 'IndexID' uniquely identifies each fishnet cell\n",
    "\n",
    "# print(\"Summarized CanopyArea by fishnet grid.\")\n",
    "\n",
    "# # Step 2: Add tree_density field to fishnet layer\n",
    "# tree_density_field = \"tree_density\"\n",
    "# if not any(f.name == tree_density_field for f in arcpy.ListFields(fishnet_layer)):\n",
    "#     arcpy.management.AddField(fishnet_layer, tree_density_field, \"DOUBLE\")\n",
    "\n",
    "# # Step 3: Join the canopy summary table back to the fishnet layer\n",
    "# arcpy.management.JoinField(fishnet_layer, \"IndexID\", canopy_summary_table, \"IndexID\", \n",
    "#                            [\"SUM_TreeCanopy_SUM_TreeCanopy_area\"])\n",
    "\n",
    "# # Step 4: Calculate tree density as a factor of the fishnet area\n",
    "# tree_density_values = []\n",
    "# fishnet_area_field = \"fragment_area\"\n",
    "\n",
    "# # Ensure that the fishnet area field is calculated\n",
    "# if not any(f.name == fishnet_area_field for f in arcpy.ListFields(fishnet_layer)):\n",
    "#     arcpy.management.AddField(fishnet_layer, fishnet_area_field, \"DOUBLE\")\n",
    "#     arcpy.management.CalculateGeometryAttributes(fishnet_layer, [[fishnet_area_field, \"AREA_GEODESIC\"]])\n",
    "\n",
    "# exponent = 0.5  # Adjust the exponent as needed for normalization\n",
    "\n",
    "# with arcpy.da.UpdateCursor(fishnet_layer, [\"SUM_TreeCanopy_SUM_TreeCanopy_area\", fishnet_area_field, tree_density_field]) as cursor:\n",
    "#     for row in cursor:\n",
    "#         # Calculate tree density with power normalization\n",
    "#         if row[0] is not None and row[1] is not None and row[1] > 0:\n",
    "#             row[2] = row[0] / math.pow(row[1], exponent)  # SUM_CanopyArea / fishnet_area^exponent\n",
    "#             tree_density_values.append(row[2])\n",
    "#         else:\n",
    "#             row[2] = None\n",
    "        \n",
    "#         cursor.updateRow(row)\n",
    "\n",
    "# print(\"Calculated power-normalized tree density for each fishnet grid.\")\n",
    "\n",
    "# # Step 5: Apply rank normalization to the tree_density field\n",
    "# with arcpy.da.UpdateCursor(fishnet_layer, [tree_density_field]) as cursor:\n",
    "#     for row in cursor:\n",
    "#         if row[0] is not None:\n",
    "#             row[0] = rank_normalize(row[0], tree_density_values)\n",
    "        \n",
    "#         cursor.updateRow(row)\n",
    "\n",
    "# print(\"Rank-normalized tree density for each fishnet grid.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Amenity Density Calculation by Fishnet Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 1: Summarize public amenity counts by fishnet grid\n",
    "# amenity_summary_table = \"fishnet_amenity_density_summary\"\n",
    "# arcpy.analysis.Statistics(fishnet_layer, amenity_summary_table, \n",
    "#                           [[\"COUNT_osm_public_id\", \"SUM\"]], \n",
    "#                           [\"IndexID\"])  # Assuming 'IndexID' uniquely identifies each fishnet cell\n",
    "\n",
    "# print(\"Summarized public amenity counts by fishnet grid.\")\n",
    "\n",
    "# # Step 2: Add amenity_density field to fishnet layer\n",
    "# amenity_density_field = \"amenity_density\"\n",
    "# if not any(f.name == amenity_density_field for f in arcpy.ListFields(fishnet_layer)):\n",
    "#     arcpy.management.AddField(fishnet_layer, amenity_density_field, \"DOUBLE\")\n",
    "\n",
    "# # Step 3: Join the amenity summary table back to the fishnet layer\n",
    "# arcpy.management.JoinField(fishnet_layer, \"IndexID\", amenity_summary_table, \"IndexID\", \n",
    "#                            [\"SUM_COUNT_osm_public_id\"])\n",
    "\n",
    "# # Step 4: Calculate amenity density as a factor of the fishnet area\n",
    "# amenity_density_values = []\n",
    "\n",
    "# with arcpy.da.UpdateCursor(fishnet_layer, [\"SUM_COUNT_osm_public_id\", fishnet_area_field, amenity_density_field]) as cursor:\n",
    "#     for row in cursor:\n",
    "#         # Calculate amenity density with power normalization\n",
    "#         if row[0] is not None and row[1] is not None and row[1] > 0:\n",
    "#             row[2] = row[0] / math.pow(row[1], exponent)  # SUM_COUNT_osm_public_id / fishnet_area^exponent\n",
    "#             amenity_density_values.append(row[2])\n",
    "#         else:\n",
    "#             row[2] = None\n",
    "        \n",
    "#         cursor.updateRow(row)\n",
    "\n",
    "# print(\"Calculated power-normalized amenity density for each fishnet grid.\")\n",
    "\n",
    "# # Step 5: Apply rank normalization to the amenity_density field\n",
    "# with arcpy.da.UpdateCursor(fishnet_layer, [amenity_density_field]) as cursor:\n",
    "#     for row in cursor:\n",
    "#         if row[0] is not None:\n",
    "#             row[0] = rank_normalize(row[0], amenity_density_values)\n",
    "        \n",
    "#         cursor.updateRow(row)\n",
    "\n",
    "# print(\"Rank-normalized amenity density for each fishnet grid.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Business Density Calculation by Neighborhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarized business counts by neighborhood.\n",
      "Calculated power-normalized business density for each neighborhood.\n",
      "Rank-normalized business density for each neighborhood.\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Summarize business counts by neighborhood\n",
    "summary_table = \"neighborhood_business_density_summary\"\n",
    "arcpy.analysis.Statistics(fishnet_layer, summary_table, \n",
    "                          [[\"COUNT_osm_business_id\", \"SUM\"]], \n",
    "                          [\"nested\"])  # Assuming 'nested' is the field indicating neighborhood\n",
    "\n",
    "print(\"Summarized business counts by neighborhood.\")\n",
    "\n",
    "# Step 2: Calculate the area for each neighborhood if not already present\n",
    "area_field = \"neighborhood_area\"\n",
    "if not any(f.name == area_field for f in arcpy.ListFields(neighborhoods_layer)):\n",
    "    arcpy.management.AddField(neighborhoods_layer, area_field, \"DOUBLE\")\n",
    "    arcpy.management.CalculateGeometryAttributes(neighborhoods_layer, [[area_field, \"AREA_GEODESIC\"]])\n",
    "    print(\"Calculated area for each neighborhood.\")\n",
    "\n",
    "# Step 3: Add business_density field to neighborhoods layer\n",
    "business_density_field = \"business_density\"\n",
    "if not any(f.name == business_density_field for f in arcpy.ListFields(neighborhoods_layer)):\n",
    "    arcpy.management.AddField(neighborhoods_layer, business_density_field, \"DOUBLE\")\n",
    "\n",
    "# Step 4: Join the business summary table back to the neighborhoods layer\n",
    "arcpy.management.JoinField(neighborhoods_layer, \"nested\", summary_table, \"nested\", \n",
    "                           [\"SUM_COUNT_osm_business_id\"])\n",
    "\n",
    "# Step 5: Calculate business density and update the neighborhoods layer\n",
    "business_density_values = []\n",
    "exponent = 0.7  # Adjust the exponent as needed for normalization\n",
    "\n",
    "with arcpy.da.UpdateCursor(neighborhoods_layer, [\"SUM_COUNT_osm_business_id\", area_field, business_density_field]) as cursor:\n",
    "    for row in cursor:\n",
    "        # Calculate business density with power normalization\n",
    "        if row[0] is not None and row[1] is not None and row[1] > 0:\n",
    "            row[2] = row[0] / math.pow(row[1], exponent)  # SUM_COUNT_osm_business_id / neighborhood_area^exponent\n",
    "            business_density_values.append(row[2])\n",
    "        else:\n",
    "            row[2] = None\n",
    "        \n",
    "        cursor.updateRow(row)\n",
    "\n",
    "print(\"Calculated power-normalized business density for each neighborhood.\")\n",
    "\n",
    "# Step 6: Apply rank normalization to the business_density field\n",
    "with arcpy.da.UpdateCursor(neighborhoods_layer, [business_density_field]) as cursor:\n",
    "    for row in cursor:\n",
    "        if row[0] is not None:\n",
    "            row[0] = rank_normalize(row[0], business_density_values)\n",
    "        \n",
    "        cursor.updateRow(row)\n",
    "\n",
    "print(\"Rank-normalized business density for each neighborhood.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Merging Neighborhood Business Density into Walkscore Fishnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fields_to_join = [\"business_density\", \"amenity_density\"]\n",
    "\n",
    "# arcpy.management.JoinField(walkscore_fishnet, \"nested\", neighborhoods_layer, \"nested\", fields_to_join)\n",
    "# print(f\"Joined fields {fields_to_join} from neighborhoods to walkscore_fishnet based on the 'nested' field.\")\n",
    "\n",
    "# print(\"Business and public amenity densities successfully merged into walkscore_fishnet.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joined fields ['business_density'] from neighborhoods to walkscore_fishnet based on the 'nested' field.\n",
      "Business and public amenity densities successfully merged into walkscore_fishnet.\n"
     ]
    }
   ],
   "source": [
    "fields_to_join = [\"business_density\"]\n",
    "\n",
    "arcpy.management.JoinField(walkscore_fishnet, \"nested\", neighborhoods_layer, \"nested\", fields_to_join)\n",
    "print(f\"Joined fields {fields_to_join} from neighborhoods to walkscore_fishnet based on the 'nested' field.\")\n",
    "\n",
    "print(\"Business and public amenity densities successfully merged into walkscore_fishnet.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging Tree Canopy Density  into Walkscore Fishnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fields_to_join = [\"tree_density\"]\n",
    "\n",
    "# arcpy.management.JoinField(walkscore_fishnet, \"nested\", neighborhoods_layer, \"nested\", fields_to_join)\n",
    "# print(f\"Joined fields {fields_to_join} from neighborhoods to walkscore_fishnet based on the 'nested' field.\")\n",
    "\n",
    "# print(\"Business and public amenity densities successfully merged into walkscore_fishnet.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging Coordinates into Walkscore Fishnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joined fields ['latitude', 'longitude'] from neighborhoods to walkscore_neighborhoods based on the 'nested' field.\n",
      "Coordinates successfully merged into walkscore_neighborhoods.\n"
     ]
    }
   ],
   "source": [
    "walkscore_neighborhoods = f\"{output_gdb}\\\\walkscore_neighborhoods\"\n",
    "fields_to_join = [\"latitude\", \"longitude\"]\n",
    "\n",
    "arcpy.management.JoinField(walkscore_neighborhoods, \"nested\", neighborhoods_layer, \"nested\", fields_to_join)\n",
    "print(f\"Joined fields {fields_to_join} from neighborhoods to walkscore_neighborhoods based on the 'nested' field.\")\n",
    "\n",
    "print(\"Coordinates successfully merged into walkscore_neighborhoods.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculation of Crime Density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated and updated scaled population for each fishnet grid.\n",
      "Calculated population density for each fishnet grid.\n",
      "Merged fields ['population_density'] from population_with_fishnet to walkscore_fishnet based on the 'IndexID' field.\n",
      "Joined COUNT_Offense_ID to C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\walkscore_fishnet and created summary table C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\crime_summary_fishnet.\n",
      "Calculated crime density for each fishnet grid.\n",
      "Rank-normalized crime density for each fishnet grid.\n",
      "Merged fields ['population_density', 'crime_density'] from population_with_fishnet to walkscore_fishnet based on the 'IndexID' field.\n",
      "Population density and crime density successfully merged into walkscore_fishnet.\n"
     ]
    }
   ],
   "source": [
    "# Define the output path for the spatial join of population with fishnet\n",
    "population_with_fishnet = r\"C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\population_with_fishnet\"\n",
    "population_layer = r\"C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\population\"\n",
    "\n",
    "# Perform Spatial Join between population and fishnet grid to associate population data with each fishnet grid\n",
    "if not arcpy.Exists(population_with_fishnet):\n",
    "    arcpy.analysis.SpatialJoin(\n",
    "        target_features=population_layer,\n",
    "        join_features=walkscore_fishnet_layer,\n",
    "        out_feature_class=population_with_fishnet,\n",
    "        join_type=\"KEEP_COMMON\",\n",
    "        match_option=\"INTERSECT\"\n",
    "    )\n",
    "    print(\"Spatial join between population and fishnet completed.\")\n",
    "\n",
    "# Add scaled population field to fishnet\n",
    "scaled_population_field = \"scaled_POP2023\"\n",
    "if not any(f.name == scaled_population_field for f in arcpy.ListFields(population_with_fishnet)):\n",
    "    arcpy.management.AddField(population_with_fishnet, scaled_population_field, \"DOUBLE\")\n",
    "\n",
    "# Scale population based on the 'is_tourist' field in the fishnet layer\n",
    "tourist_scaler = {0: 1.0, 1: 1.25}\n",
    "with arcpy.da.UpdateCursor(population_with_fishnet, [\"POP2023\", \"is_tourist\", scaled_population_field]) as cursor:\n",
    "    for row in cursor:\n",
    "        row[2] = row[0] * tourist_scaler.get(row[1], 1.0) if row[0] is not None else None\n",
    "        cursor.updateRow(row)\n",
    "print(\"Calculated and updated scaled population for each fishnet grid.\")\n",
    "\n",
    "# Calculate Population Density for Fishnet\n",
    "population_density_field = \"population_density\"\n",
    "if not any(f.name == population_density_field for f in arcpy.ListFields(population_with_fishnet)):\n",
    "    arcpy.management.AddField(population_with_fishnet, population_density_field, \"DOUBLE\")\n",
    "\n",
    "with arcpy.da.UpdateCursor(population_with_fishnet, [scaled_population_field, \"Shape_Area\", population_density_field]) as cursor:\n",
    "    for row in cursor:\n",
    "        if row[0] is not None and row[1] is not None and row[1] > 0:\n",
    "            row[2] = row[0] / row[1]  # population_density = scaled_population / fishnet_area\n",
    "        else:\n",
    "            row[2] = None\n",
    "        cursor.updateRow(row)\n",
    "print(\"Calculated population density for each fishnet grid.\")\n",
    "\n",
    "# Merge Population Density into Walkscore Fishnet before calculating crime density\n",
    "fields_to_merge = [population_density_field]\n",
    "arcpy.management.JoinField(walkscore_fishnet_layer, \"IndexID\", population_with_fishnet, \"IndexID\", fields_to_merge)\n",
    "print(f\"Merged fields {fields_to_merge} from population_with_fishnet to walkscore_fishnet based on the 'IndexID' field.\")\n",
    "\n",
    "# Step 1: Calculate crime density using the fishnet grid area and population density\n",
    "# Call the function for crime density calculation\n",
    "crime_layer = f\"{base_layers_group}\\\\spd_crime_data\"\n",
    "intersect_output = f\"{workspace}\\\\crime_fishnet_intersect\"\n",
    "crime_summary_output = f\"{workspace}\\\\crime_summary_fishnet\"\n",
    "calculate_counts(crime_layer, intersect_output, walkscore_fishnet_layer, crime_summary_output, \"Offense_ID\")  # Aggregates crime data by each fishnet cell\n",
    "\n",
    "# Add crime density field to the fishnet layer\n",
    "crime_density_field = \"crime_density\"\n",
    "if not any(f.name == crime_density_field for f in arcpy.ListFields(walkscore_fishnet_layer)):\n",
    "    arcpy.management.AddField(walkscore_fishnet_layer, crime_density_field, \"DOUBLE\")\n",
    "\n",
    "# Calculate crime density using fishnet population density\n",
    "with arcpy.da.UpdateCursor(walkscore_fishnet_layer, [\"COUNT_Offense_ID\", population_density_field, crime_density_field]) as cursor:\n",
    "    for row in cursor:\n",
    "        if row[0] is not None and row[1] is not None and row[1] > 0:\n",
    "            row[2] = row[0] / row[1]  # crime_density = crime_count / population_density\n",
    "        else:\n",
    "            row[2] = None\n",
    "        cursor.updateRow(row)\n",
    "print(\"Calculated crime density for each fishnet grid.\")\n",
    "\n",
    "# Rank Normalize Crime Density in Walkscore Fishnet Layer\n",
    "# Collect crime_density values for normalization\n",
    "crime_density_values = [row[0] for row in arcpy.da.SearchCursor(walkscore_fishnet_layer, [crime_density_field]) if row[0] is not None]\n",
    "\n",
    "# Ensure there are no None values in crime_density_values\n",
    "crime_density_values = [value for value in crime_density_values if value is not None]\n",
    "\n",
    "# Rank normalize crime density\n",
    "with arcpy.da.UpdateCursor(walkscore_fishnet_layer, [crime_density_field]) as cursor:\n",
    "    for row in cursor:\n",
    "        if row[0] is not None:\n",
    "            row[0] = rank_normalize(row[0], crime_density_values)\n",
    "        cursor.updateRow(row)\n",
    "print(\"Rank-normalized crime density for each fishnet grid.\")\n",
    "\n",
    "# Merge Population Density and Crime Density into Walkscore Fishnet\n",
    "fields_to_merge = [population_density_field, crime_density_field]\n",
    "arcpy.management.JoinField(walkscore_fishnet_layer, \"IndexID\", population_with_fishnet, \"IndexID\", fields_to_merge)\n",
    "print(f\"Merged fields {fields_to_merge} from population_with_fishnet to walkscore_fishnet based on the 'IndexID' field.\")\n",
    "\n",
    "print(\"Population density and crime density successfully merged into walkscore_fishnet.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated median crime density by neighborhood.\n",
      "Added MEDIAN_crime_density field to walkscore_neighborhoods layer.\n",
      "Joined median crime density to neighborhoods layer.\n",
      "Updated MEDIAN_crime_density field in neighborhoods layer with median values.\n",
      "Added MEDIAN_crime_density field to walkscore_fishnet layer.\n",
      "Joined median crime density to fishnet layer from neighborhoods layer.\n",
      "Updated MEDIAN_crime_density field in fishnet layer with neighborhood median values.\n"
     ]
    }
   ],
   "source": [
    "# Define the output path for the neighborhood crime density summary\n",
    "crime_density_summary_table = r\"C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\neighborhood_crime_density_summary\"\n",
    "neighborhoods_layer = r\"C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\walkscore_neighborhoods\"\n",
    "\n",
    "# Step 1: Summarize the crime density from fishnet grids by neighborhood\n",
    "# Calculate the median crime density for each neighborhood\n",
    "if not arcpy.Exists(crime_density_summary_table):\n",
    "    arcpy.analysis.Statistics(\n",
    "        in_table=walkscore_fishnet_layer,\n",
    "        out_table=crime_density_summary_table,\n",
    "        statistics_fields=[[\"crime_density\", \"MEDIAN\"]],\n",
    "        case_field=\"nested\"  # Assuming 'nested' is the field indicating neighborhood association in fishnet\n",
    "    )\n",
    "    print(\"Calculated median crime density by neighborhood.\")\n",
    "\n",
    "# Step 2: Add the median crime density field to the neighborhood layer\n",
    "median_crime_density_field = \"MEDIAN_crime_density\"\n",
    "if not any(f.name == median_crime_density_field for f in arcpy.ListFields(neighborhoods_layer)):\n",
    "    arcpy.management.AddField(neighborhoods_layer, median_crime_density_field, \"DOUBLE\")\n",
    "    print(f\"Added {median_crime_density_field} field to walkscore_neighborhoods layer.\")\n",
    "\n",
    "# Step 3: Join the median crime density values from the summary table to the neighborhoods layer\n",
    "arcpy.management.JoinField(\n",
    "    in_data=neighborhoods_layer,\n",
    "    in_field=\"nested\",\n",
    "    join_table=crime_density_summary_table,\n",
    "    join_field=\"nested\",\n",
    "    fields=[\"MEDIAN_crime_density\"]\n",
    ")\n",
    "print(\"Joined median crime density to neighborhoods layer.\")\n",
    "\n",
    "# Step 4: Update the crime_density field in the neighborhood layer with the median value\n",
    "with arcpy.da.UpdateCursor(neighborhoods_layer, [\"MEDIAN_crime_density\", median_crime_density_field]) as cursor:\n",
    "    for row in cursor:\n",
    "        row[1] = row[0]  # Assign the median value to the crime_density field\n",
    "        cursor.updateRow(row)\n",
    "print(f\"Updated {median_crime_density_field} field in neighborhoods layer with median values.\")\n",
    "\n",
    "# Step 5: Add the median crime density field to the fishnet layer\n",
    "median_crime_density_fishnet_field = \"MEDIAN_crime_density\"\n",
    "if not any(f.name == median_crime_density_fishnet_field for f in arcpy.ListFields(walkscore_fishnet_layer)):\n",
    "    arcpy.management.AddField(walkscore_fishnet_layer, median_crime_density_fishnet_field, \"DOUBLE\")\n",
    "    print(f\"Added {median_crime_density_fishnet_field} field to walkscore_fishnet layer.\")\n",
    "\n",
    "# Step 6: Join the median crime density values from the neighborhood layer to the fishnet layer\n",
    "arcpy.management.JoinField(\n",
    "    in_data=walkscore_fishnet_layer,\n",
    "    in_field=\"nested\",\n",
    "    join_table=neighborhoods_layer,\n",
    "    join_field=\"nested\",\n",
    "    fields=[median_crime_density_field]\n",
    ")\n",
    "print(f\"Joined median crime density to fishnet layer from neighborhoods layer.\")\n",
    "\n",
    "# Step 7: Update the fishnet median crime density field\n",
    "with arcpy.da.UpdateCursor(walkscore_fishnet_layer, [median_crime_density_field, median_crime_density_fishnet_field]) as cursor:\n",
    "    for row in cursor:\n",
    "        row[1] = row[0]  # Assign the median value from neighborhood to fishnet\n",
    "        cursor.updateRow(row)\n",
    "print(f\"Updated {median_crime_density_fishnet_field} field in fishnet layer with neighborhood median values.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted temporary neighborhood crime density summary table.\n"
     ]
    }
   ],
   "source": [
    "if arcpy.Exists(crime_density_summary_table):\n",
    "    arcpy.management.Delete(crime_density_summary_table)\n",
    "    print(\"Deleted temporary neighborhood crime density summary table.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing Unnecessary Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fields in C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\walkscore_fishnet after dropping specified fields:\n",
      "Name: OBJECTID, Type: OID\n",
      "Name: Shape, Type: Geometry\n",
      "Name: FID_walkscore_fishnet, Type: Integer\n",
      "Name: IndexID, Type: Integer\n",
      "Name: total_area, Type: Double\n",
      "Name: Max_Speed_Limit, Type: Double\n",
      "Name: COUNT_Offense_ID, Type: Integer\n",
      "Name: Grid_Slope_MEAN, Type: Double\n",
      "Name: Sidewalks_Slope_Mean, Type: Double\n",
      "Name: Streets_Slope_Mean, Type: Double\n",
      "Name: MultiUseTrails_Slope_Mean, Type: Double\n",
      "Name: trails_Slope_Mean, Type: Double\n",
      "Name: effective_slope, Type: Double\n",
      "Name: Industrial_SUM_Industrial_effective_area, Type: Double\n",
      "Name: ParkingLots_SUM_ParkingLots_effective_area, Type: Double\n",
      "Name: GolfCourse_SUM_GolfCourse_effective_area, Type: Double\n",
      "Name: Cemeteries_SUM_Cemeteries_effective_area, Type: Double\n",
      "Name: Hospitals_SUM_Hospitals_effective_area, Type: Double\n",
      "Name: Slope_MEAN, Type: Double\n",
      "Name: Bike_greenways_SUM_Bike_greenways_area, Type: Double\n",
      "Name: Bike_protected_SUM_Bike_protected_area, Type: Double\n",
      "Name: Bike_buffer_SUM_Bike_buffer_area, Type: Double\n",
      "Name: Healthy_Streets_SUM_Healthy_Streets_area, Type: Double\n",
      "Name: Parks_SUM_Parks_area, Type: Double\n",
      "Name: Universities_SUM_Universities_area, Type: Double\n",
      "Name: Sidewalks_SUM_Sidewalks_area, Type: Double\n",
      "Name: Plaza_SUM_Plaza_area, Type: Double\n",
      "Name: trails_SUM_trails_area, Type: Double\n",
      "Name: MultiUseTrails_SUM_MultiUseTrails_area, Type: Double\n",
      "Name: Streets_SUM_Streets_effective_area, Type: Double\n",
      "Name: population_SUM_population_area, Type: Double\n",
      "Name: SPD_Crime_Data_COUNT_Offense_ID, Type: Double\n",
      "Name: FID_neighborhoods, Type: Integer\n",
      "Name: city, Type: String\n",
      "Name: nested, Type: String\n",
      "Name: neighborhood_area, Type: Double\n",
      "Name: is_tourist, Type: Integer\n",
      "Name: business_density, Type: Double\n",
      "Name: SUM_COUNT_osm_business_id_1, Type: Double\n",
      "Name: SUM_COUNT_osm_business_id_12, Type: Double\n",
      "Name: unadjusted_walkscore_sum, Type: Double\n",
      "Name: Shape_Length, Type: Double\n",
      "Name: Shape_Area, Type: Double\n",
      "Name: Fragment_Area, Type: Double\n",
      "Name: IsIndustrial, Type: SmallInteger\n",
      "Name: IsParkingLots, Type: SmallInteger\n",
      "Name: IsGolfCourse, Type: SmallInteger\n",
      "Name: IsCemeteries, Type: SmallInteger\n",
      "Name: IsHospitals, Type: SmallInteger\n",
      "Name: IsMultiUseTrails, Type: SmallInteger\n",
      "Name: IsParks, Type: SmallInteger\n",
      "Name: business_density_1, Type: Double\n",
      "Name: population_density, Type: Double\n",
      "Name: COUNT_Offense_ID_1, Type: Integer\n",
      "Name: crime_density, Type: Double\n",
      "Name: population_density_1, Type: Double\n",
      "Name: MEDIAN_crime_density, Type: Double\n",
      "Name: MEDIAN_crime_density_1, Type: Double\n",
      "Fields dropped successfully.\n"
     ]
    }
   ],
   "source": [
    "# Drop specified fields\n",
    "fields_to_drop = []\n",
    "for field in arcpy.ListFields(walkscore_fishnet):\n",
    "    if field.name.endswith(\"FREQUENCY\") or field.name.endswith(\"_OBJECTID\") or field.name.endswith(\"Slope_AREA\") or field.name.endswith(\"Slope_COUNT\") or field.name.endswith(\"IndexID_1\") or field.name.endswith(\"_id\"):\n",
    "        fields_to_drop.append(field.name)\n",
    "\n",
    "if fields_to_drop:\n",
    "    arcpy.management.DeleteField(walkscore_fishnet, fields_to_drop)\n",
    "\n",
    "# Verify fields in walkscore_fishnet after dropping specified fields\n",
    "walkscore_fishnet = f\"{output_gdb}\\\\walkscore_fishnet\"\n",
    "print(f\"\\nFields in {walkscore_fishnet} after dropping specified fields:\")\n",
    "fields = arcpy.ListFields(walkscore_fishnet)\n",
    "for field in fields:\n",
    "    print(f\"Name: {field.name}, Type: {field.type}\")\n",
    "\n",
    "print(\"Fields dropped successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Cleaning Contents Pane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_created_layers(layers_list):\n",
    "    for layer in layers_list:\n",
    "        if arcpy.Exists(layer):\n",
    "            arcpy.management.Delete(layer)\n",
    "            print(f\"Deleted layer: {layer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_layers = [\n",
    "    \"Industrial\",\n",
    "    \"ParkingLots\",\n",
    "    \"GolfCourse\",\n",
    "    \"Cemeteries\",\n",
    "    \"Hospitals\",\n",
    "#     \"Slope\",\n",
    "    \"Bike_greenways\",\n",
    "    \"Bike_protected\",\n",
    "    \"Bike_buffer\",\n",
    "    \"Healthy_Streets\",\n",
    "    \"Parks\",\n",
    "    \"Universities\",\n",
    "    \"Sidewalks\",\n",
    "    \"Plaza\",\n",
    "    \"trails\",\n",
    "    \"MultiUseTrails\",\n",
    "    \"Streets\",\n",
    "    \"fishnet_clipped\",\n",
    "    \"Marked_Crosswalks\",\n",
    "    \"fishnet_clipped\",\n",
    "    \"neighborhoods\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing layer: Industrial\n",
      "C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\Industrial is already in the target spatial reference.\n",
      "Processing layer: ParkingLots\n",
      "C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\ParkingLots is already in the target spatial reference.\n",
      "Processing layer: GolfCourse\n",
      "C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\GolfCourse is already in the target spatial reference.\n",
      "Processing layer: Cemeteries\n",
      "C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\Cemeteries is already in the target spatial reference.\n",
      "Processing layer: Hospitals\n",
      "C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\Hospitals is already in the target spatial reference.\n",
      "Processing layer: Bike_greenways\n",
      "C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\Bike_greenways is already in the target spatial reference.\n",
      "Processing layer: Bike_protected\n",
      "C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\Bike_protected is already in the target spatial reference.\n",
      "Processing layer: Bike_buffer\n",
      "C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\Bike_buffer is already in the target spatial reference.\n",
      "Processing layer: Healthy_Streets\n",
      "C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\Healthy_Streets is already in the target spatial reference.\n",
      "Processing layer: Parks\n",
      "C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\Parks is already in the target spatial reference.\n",
      "Processing layer: Universities\n",
      "C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\Universities is already in the target spatial reference.\n",
      "Processing layer: Sidewalks\n",
      "C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\Sidewalks is already in the target spatial reference.\n",
      "Processing layer: Plaza\n",
      "C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\Plaza is already in the target spatial reference.\n",
      "Processing layer: trails\n",
      "C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\trails is already in the target spatial reference.\n",
      "Processing layer: MultiUseTrails\n",
      "C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\MultiUseTrails is already in the target spatial reference.\n",
      "Processing layer: Streets\n",
      "C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\Streets is already in the target spatial reference.\n",
      "Processing layer: fishnet_clipped\n",
      "C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\fishnet_clipped is already in the target spatial reference.\n",
      "Processing layer: Marked_Crosswalks\n",
      "C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\Marked_Crosswalks is already in the target spatial reference.\n",
      "Processing layer: fishnet_clipped\n",
      "C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\fishnet_clipped is already in the target spatial reference.\n",
      "Processing layer: neighborhoods\n",
      "C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\neighborhoods is already in the target spatial reference.\n",
      "All layers have been projected to the target spatial reference.\n"
     ]
    }
   ],
   "source": [
    "base_layers_group = r\"C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\"\n",
    "target_spatial_reference = arcpy.SpatialReference(3857)  # WGS 1984 Web Mercator (auxiliary sphere)\n",
    "\n",
    "def project_layer(input_layer, target_sr):\n",
    "    input_layer_sr = arcpy.Describe(input_layer).spatialReference\n",
    "    \n",
    "    if input_layer_sr.name != target_sr.name:\n",
    "        temp_projected_layer = os.path.join(arcpy.env.scratchGDB, f\"{os.path.basename(input_layer)}_proj\")\n",
    "        arcpy.management.Project(input_layer, temp_projected_layer, target_sr)\n",
    "        print(f\"Projected {input_layer} to {temp_projected_layer}.\")\n",
    "        \n",
    "        # Overwrite the original layer with the projected version\n",
    "        arcpy.management.Delete(input_layer)\n",
    "        arcpy.management.CopyFeatures(temp_projected_layer, input_layer)\n",
    "        arcpy.management.Delete(temp_projected_layer)\n",
    "        print(f\"Replaced original {input_layer} with projected version.\")\n",
    "    else:\n",
    "        print(f\"{input_layer} is already in the target spatial reference.\")\n",
    "\n",
    "# Process each base layer\n",
    "for layer_name in base_layers:\n",
    "    print(f\"Processing layer: {layer_name}\")  # Debugging statement\n",
    "\n",
    "    # Access the layer\n",
    "    input_layer = f\"{base_layers_group}\\\\{layer_name}\"\n",
    "    \n",
    "    # Verify if the input_layer exists\n",
    "    if not arcpy.Exists(input_layer):\n",
    "        print(f\"Layer {input_layer} does not exist. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    # Project the input layer to the target spatial reference\n",
    "    project_layer(input_layer, target_spatial_reference)\n",
    "\n",
    "print(\"All layers have been projected to the target spatial reference.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "file_extension": ".py",
   "name": "python",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
