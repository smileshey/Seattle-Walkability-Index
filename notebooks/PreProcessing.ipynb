{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PreProcessing\n",
    "\n",
    "Now that we have the base layers organized into a base_layers group in the contents frame, we'll use python to iterate through each layer, preparing them to be used in the calculation of the walkscore in the next notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arcpy\n",
    "import arcpy.mp\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import os\n",
    "import numpy as np\n",
    "from arcpy.sa import Int\n",
    "\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0: Formatting Fishnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fishnet_layer = r\"C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\fishnet_clipped\"\n",
    "base_layers_group = r\"C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\"\n",
    "output_gdb = r\"C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\"\n",
    "geodatabase_path = r\"C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\"\n",
    "fishnet_clipped = r\"C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\fishnet_clipped\"\n",
    "\n",
    "arcpy.env.workspace = r\"C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\"\n",
    "fishnet_area_field = \"total_area\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = r\"C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\output\"\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Field Name: OBJECTID, Field Type: OID\n",
      "Field Name: Shape, Field Type: Geometry\n",
      "Field Name: Shape_Length, Field Type: Double\n",
      "Field Name: Shape_Area, Field Type: Double\n",
      "Field Name: IndexID, Field Type: Integer\n"
     ]
    }
   ],
   "source": [
    "fields = arcpy.ListFields(fishnet_layer)\n",
    "for field in fields:\n",
    "    print(f\"Field Name: {field.name}, Field Type: {field.type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IndexID field created and populated in fishnet_clipped.\n"
     ]
    }
   ],
   "source": [
    "# Add the IndexID field if it doesn't exist\n",
    "index_field = \"IndexID\"\n",
    "if not any(f.name == index_field for f in arcpy.ListFields(fishnet_clipped)):\n",
    "    arcpy.management.AddField(fishnet_clipped, index_field, \"LONG\")\n",
    "\n",
    "# Populate the IndexID field with unique values\n",
    "with arcpy.da.UpdateCursor(fishnet_clipped, [index_field]) as cursor:\n",
    "    for i, row in enumerate(cursor):\n",
    "        row[0] = i + 1\n",
    "        cursor.updateRow(row)\n",
    "\n",
    "print(\"IndexID field created and populated in fishnet_clipped.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mandatory Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_layers = [\n",
    "#     \"TreeCanopy\",\n",
    "#     \"Public_Amenities\",\n",
    "    \"Business_Amenities\",\n",
    "    \"Industrial\",\n",
    "    \"ParkingLots\",\n",
    "    \"GolfCourse\",\n",
    "    \"Cemeteries\",\n",
    "    \"Hospitals\",\n",
    "    \"Slope\",\n",
    "    \"Bike_greenways\",\n",
    "    \"Bike_protected\",\n",
    "    \"Bike_buffer\",\n",
    "    \"Healthy_Streets\",\n",
    "    \"Parks\",\n",
    "    \"Universities\",\n",
    "    \"Sidewalks\",\n",
    "    \"Plaza\",\n",
    "    \"trails\",\n",
    "    \"MultiUseTrails\",\n",
    "    \"Streets\",\n",
    "    \"population\",\n",
    "    \"crashes\",\n",
    "    \"SPD_Crime_Data\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for layer: C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\Business_Amenities\n",
      "Checking for layer: C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\Industrial\n",
      "Checking for layer: C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\ParkingLots\n",
      "Checking for layer: C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\GolfCourse\n",
      "Checking for layer: C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\Cemeteries\n",
      "Checking for layer: C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\Hospitals\n",
      "Checking for layer: C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\Slope\n",
      "Checking for layer: C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\Bike_greenways\n",
      "Checking for layer: C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\Bike_protected\n",
      "Checking for layer: C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\Bike_buffer\n",
      "Checking for layer: C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\Healthy_Streets\n",
      "Checking for layer: C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\Parks\n",
      "Checking for layer: C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\Universities\n",
      "Checking for layer: C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\Sidewalks\n",
      "Checking for layer: C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\Plaza\n",
      "Checking for layer: C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\trails\n",
      "Checking for layer: C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\MultiUseTrails\n",
      "Checking for layer: C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\Streets\n",
      "Checking for layer: C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\population\n",
      "Checking for layer: C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\crashes\n",
      "Checking for layer: C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\SPD_Crime_Data\n"
     ]
    }
   ],
   "source": [
    "for layer_name in base_layers:\n",
    "    input_layer = f\"{base_layers_group}\\\\{layer_name}\"\n",
    "    print(f\"Checking for layer: {input_layer}\")  # Add this line\n",
    "    if not arcpy.Exists(input_layer):\n",
    "        print(f\"Layer {input_layer} does not exist. Skipping.\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_average_slope(fishnet_layer, slope_raster, features_layer, output_table):\n",
    "    arcpy.CheckOutExtension(\"Spatial\")\n",
    "    extracted_slope = arcpy.sa.ExtractByMask(slope_raster, features_layer)\n",
    "    temp_extracted_slope = f\"{output_gdb}\\\\temp_extracted_slope\"\n",
    "    extracted_slope.save(temp_extracted_slope)\n",
    "    arcpy.sa.ZonalStatisticsAsTable(fishnet_layer, \"IndexID\", temp_extracted_slope, output_table, \"NODATA\", \"MEAN\")\n",
    "    arcpy.management.Delete(temp_extracted_slope)\n",
    "\n",
    "def integrate_slope_and_area(intersect_output, slope_output_table, area_field, effective_slope_field):\n",
    "    arcpy.management.AddField(intersect_output, effective_slope_field, \"DOUBLE\")\n",
    "    slope_df = arcpy.da.TableToNumPyArray(slope_output_table, [\"IndexID\", \"MEAN\"])\n",
    "    slope_df = pd.DataFrame(slope_df)\n",
    "    with arcpy.da.UpdateCursor(intersect_output, [\"IndexID\", area_field, effective_slope_field]) as cursor:\n",
    "        for row in cursor:\n",
    "            mean_slope = slope_df.loc[slope_df[\"IndexID\"] == row[0], \"MEAN\"]\n",
    "            row[2] = (mean_slope.values[0] * row[1]) if not mean_slope.empty and row[1] is not None else 0\n",
    "            cursor.updateRow(row)\n",
    "\n",
    "def calculate_polygon_area(layer, area_field):\n",
    "    if not any(f.name.lower() == area_field.lower() for f in arcpy.ListFields(layer)):\n",
    "        arcpy.management.AddField(layer, area_field, \"DOUBLE\")\n",
    "    arcpy.management.CalculateGeometryAttributes(layer, [[area_field, \"AREA_GEODESIC\"]], area_unit=\"SQUARE_FEET_US\")\n",
    "\n",
    "def calculate_polyline_area_with_recalculated_length(layer, area_field, width_field):\n",
    "    recalculated_length_field = f\"{area_field}_len\"\n",
    "    if not any(f.name.lower() == recalculated_length_field.lower() for f in arcpy.ListFields(layer)):\n",
    "        arcpy.management.AddField(layer, recalculated_length_field, \"DOUBLE\")\n",
    "    arcpy.management.CalculateGeometryAttributes(layer, [[recalculated_length_field, \"LENGTH_GEODESIC\"]], length_unit=\"FEET_US\")\n",
    "    if not any(f.name.lower() == area_field.lower() for f in arcpy.ListFields(layer)):\n",
    "        arcpy.management.AddField(layer, area_field, \"DOUBLE\")\n",
    "    with arcpy.da.UpdateCursor(layer, [width_field, recalculated_length_field, area_field]) as cursor:\n",
    "        for row in cursor:\n",
    "            row[2] = row[0] * row[1] if row[0] is not None and row[1] is not None else 0\n",
    "            cursor.updateRow(row)\n",
    "\n",
    "def calculate_effective_area(layer, effective_area_field, length_field=\"Shape_Length\", width_field=\"street_width\", speed_limit_field=\"SPEEDLIMIT\", at_grade_field=\"at_grade_scalar\"):\n",
    "    recalculated_length_field = f\"{effective_area_field}_len\"\n",
    "    if not any(f.name.lower() == recalculated_length_field.lower() for f in arcpy.ListFields(layer)):\n",
    "        arcpy.management.AddField(layer, recalculated_length_field, \"DOUBLE\")\n",
    "    arcpy.management.CalculateGeometryAttributes(layer, [[recalculated_length_field, \"LENGTH_GEODESIC\"]], length_unit=\"FEET_US\")\n",
    "\n",
    "    if not any(f.name == effective_area_field for f in arcpy.ListFields(layer)):\n",
    "        arcpy.management.AddField(layer, effective_area_field, \"DOUBLE\")\n",
    "\n",
    "    with arcpy.da.UpdateCursor(layer, [recalculated_length_field, width_field, speed_limit_field, at_grade_field, effective_area_field]) as cursor:\n",
    "        for row in cursor:\n",
    "            if row[0] is not None and row[1] is not None and row[2] is not None and row[3] is not None:\n",
    "                row[4] = row[0] * row[1] * row[2] * row[3]\n",
    "            else:\n",
    "                row[4] = None\n",
    "            cursor.updateRow(row)\n",
    "    print(f\"Calculated effective area for {layer} and stored in {effective_area_field}.\")\n",
    "    \n",
    "def create_layer(input_layer, fclass_list, output_layer_name):\n",
    "    # Create a query to filter the input layer based on fclass values\n",
    "    fclass_query = f\"\"\"fclass IN ({','.join([f\"'{fc}'\" for fc in fclass_list])})\"\"\"\n",
    "    \n",
    "    # Create the output layer\n",
    "    arcpy.management.MakeFeatureLayer(input_layer, \"temp_layer\", fclass_query)\n",
    "    output_layer = f\"{workspace}\\\\{output_layer_name}\"\n",
    "    \n",
    "    # Check if the output layer already exists and delete it if it does\n",
    "    if arcpy.Exists(output_layer):\n",
    "        arcpy.management.Delete(output_layer)\n",
    "    \n",
    "    # Save the filtered features to a new feature class\n",
    "    arcpy.management.CopyFeatures(\"temp_layer\", output_layer)\n",
    "    print(f\"Created {output_layer_name} layer with {len(fclass_list)} fclass values.\")\n",
    "    \n",
    "def calculate_counts(input_layer, intersect_output, fishnet_layer, summary_output, id_field):\n",
    "    # Intersect the input crime data layer with the fishnet\n",
    "    arcpy.analysis.Intersect([input_layer, fishnet_layer], intersect_output)\n",
    "\n",
    "    # Add IndexID to intersect output if it doesn't exist\n",
    "    if not any(f.name == \"IndexID\" for f in arcpy.ListFields(intersect_output)):\n",
    "        arcpy.management.AddField(intersect_output, \"IndexID\", \"LONG\")\n",
    "        # Populate the IndexID field in intersect output\n",
    "        with arcpy.da.UpdateCursor(intersect_output, [\"IndexID\"]) as cursor:\n",
    "            for i, row in enumerate(cursor):\n",
    "                row[0] = i + 1\n",
    "                cursor.updateRow(row)\n",
    "\n",
    "    # Calculate the count of crimes within each fishnet grid cell\n",
    "    arcpy.analysis.Statistics(intersect_output, summary_output, [[id_field, \"COUNT\"]], \"IndexID\")\n",
    "\n",
    "    # Join the summary table back to the fishnet layer\n",
    "    count_field = f\"COUNT_{id_field}\"\n",
    "    arcpy.management.JoinField(fishnet_layer, \"IndexID\", summary_output, \"IndexID\", [count_field])\n",
    "\n",
    "    # Update null values in the joined count field to 0\n",
    "    with arcpy.da.UpdateCursor(fishnet_layer, [count_field]) as cursor:\n",
    "        for row in cursor:\n",
    "            if row[0] is None:\n",
    "                row[0] = 0  # Set null counts to 0\n",
    "            cursor.updateRow(row)\n",
    "\n",
    "    print(f\"Joined {count_field} to {fishnet_layer} and created summary table {summary_output}.\")\n",
    "    \n",
    "def calculate_crime_density(input_layer, intersect_output, fishnet_layer, summary_output, id_field):\n",
    "    # Intersect the input crime data layer with the fishnet\n",
    "    arcpy.analysis.Intersect([input_layer, fishnet_layer], intersect_output)\n",
    "\n",
    "    # Add IndexID to intersect output if it doesn't exist\n",
    "    if not any(f.name == \"IndexID\" for f in arcpy.ListFields(intersect_output)):\n",
    "        arcpy.management.AddField(intersect_output, \"IndexID\", \"LONG\")\n",
    "        # Populate the IndexID field in intersect output\n",
    "        with arcpy.da.UpdateCursor(intersect_output, [\"IndexID\"]) as cursor:\n",
    "            for i, row in enumerate(cursor):\n",
    "                row[0] = i + 1\n",
    "                cursor.updateRow(row)\n",
    "\n",
    "    # Calculate the count of crimes within each fishnet grid cell\n",
    "    arcpy.analysis.Statistics(intersect_output, summary_output, [[id_field, \"COUNT\"]], \"IndexID\")\n",
    "\n",
    "    # Join the summary table back to the fishnet layer\n",
    "    count_field = f\"COUNT_{id_field}\"\n",
    "    arcpy.management.JoinField(fishnet_layer, \"IndexID\", summary_output, \"IndexID\", [count_field])\n",
    "\n",
    "    # Update null values in the joined count field to 0\n",
    "    with arcpy.da.UpdateCursor(fishnet_layer, [count_field]) as cursor:\n",
    "        for row in cursor:\n",
    "            if row[0] is None:\n",
    "                row[0] = 0  # Set null counts to 0\n",
    "            cursor.updateRow(row)\n",
    "\n",
    "    print(f\"Joined {count_field} to {fishnet_layer} and created summary table {summary_output}.\")\n",
    "    \n",
    "def calculate_population_density(input_layer, intersect_output, fishnet_layer, summary_output, id_field, density_field):\n",
    "    # Intersect the input population layer with the fishnet\n",
    "    arcpy.analysis.Intersect([input_layer, fishnet_layer], intersect_output)\n",
    "\n",
    "    # Add IndexID to intersect output if it doesn't exist\n",
    "    if not any(f.name == \"IndexID\" for f in arcpy.ListFields(intersect_output)):\n",
    "        arcpy.management.AddField(intersect_output, \"IndexID\", \"LONG\")\n",
    "        # Populate the IndexID field in intersect output\n",
    "        with arcpy.da.UpdateCursor(intersect_output, [\"IndexID\"]) as cursor:\n",
    "            for i, row in enumerate(cursor):\n",
    "                row[0] = i + 1\n",
    "                cursor.updateRow(row)\n",
    "\n",
    "    # Calculate the area of each intersected polygon (geodesic area in ft²)\n",
    "    area_field = \"intersect_area\"\n",
    "    if not any(f.name == area_field for f in arcpy.ListFields(intersect_output)):\n",
    "        arcpy.management.AddField(intersect_output, area_field, \"DOUBLE\")\n",
    "\n",
    "        try:\n",
    "            # Try calculating geometry attributes with geodesic area\n",
    "            arcpy.management.CalculateGeometryAttributes(intersect_output, [[area_field, \"AREA_GEODESIC\"]], area_unit=\"SQUARE_FEET_US\")\n",
    "        except arcpy.ExecuteError:\n",
    "            # Fallback: Use Add Geometry Attributes tool\n",
    "            arcpy.management.AddGeometryAttributes(intersect_output, \"AREA_GEODESIC\", Area_Unit=\"SQUARE_FEET_US\")\n",
    "            arcpy.management.CalculateField(intersect_output, area_field, \"!POLY_AREA!\", \"PYTHON3\")\n",
    "\n",
    "    # Calculate the proportional population for each intersected area\n",
    "    proportional_population_field = \"proportional_population\"\n",
    "    if not any(f.name == proportional_population_field for f in arcpy.ListFields(intersect_output)):\n",
    "        arcpy.management.AddField(intersect_output, proportional_population_field, \"DOUBLE\")\n",
    "\n",
    "    with arcpy.da.UpdateCursor(intersect_output, [area_field, density_field, proportional_population_field]) as cursor:\n",
    "        for row in cursor:\n",
    "            if row[0] is not None and row[1] is not None:\n",
    "                row[2] = row[0] * row[1]  # proportional_population = intersect_area * population_density\n",
    "            else:\n",
    "                row[2] = 0\n",
    "            cursor.updateRow(row)\n",
    "\n",
    "    # Summarize the proportional population by fishnet grid (using IndexID)\n",
    "    arcpy.analysis.Statistics(intersect_output, summary_output, [[proportional_population_field, \"SUM\"]], \"IndexID\")\n",
    "\n",
    "    # Determine the correct name of the output field from summary statistics\n",
    "    summary_field_name = f\"SUM_{proportional_population_field}\"\n",
    "\n",
    "    # Join the summary table back to the fishnet layer\n",
    "    arcpy.management.JoinField(fishnet_layer, \"IndexID\", summary_output, \"IndexID\", [summary_field_name])\n",
    "\n",
    "    # Update null values in the joined population field to 0\n",
    "    with arcpy.da.UpdateCursor(fishnet_layer, [summary_field_name]) as cursor:\n",
    "        for row in cursor:\n",
    "            if row[0] is None:\n",
    "                row[0] = 0  # Set null population to 0\n",
    "            cursor.updateRow(row)\n",
    "\n",
    "    print(f\"Joined {summary_field_name} to {fishnet_layer} and created summary table {summary_output}.\")\n",
    "\n",
    "\n",
    "def calculate_max_speed_limit(intersect_layer, fishnet_layer, output_table, speed_limit_field):\n",
    "    # Calculate the max speed limit for each intersected grid cell\n",
    "    arcpy.analysis.Statistics(intersect_layer, output_table, [[speed_limit_field, \"MAX\"]], \"IndexID\")\n",
    "    # Join the result back to the fishnet layer\n",
    "    arcpy.management.JoinField(fishnet_layer, \"IndexID\", output_table, \"IndexID\", [\"MAX_\" + speed_limit_field])\n",
    "    # Rename the field to Max_Speed_Limit\n",
    "    arcpy.management.AlterField(fishnet_layer, \"MAX_\" + speed_limit_field, \"Max_Speed_Limit\")\n",
    "    \n",
    "def calculate_average_density(fishnet_layer, density_raster, features_layer, output_table):\n",
    "    \"\"\"Calculate average density for each fishnet grid and save as a table.\"\"\"\n",
    "    arcpy.CheckOutExtension(\"Spatial\")\n",
    "    extracted_density = arcpy.sa.ExtractByMask(density_raster, features_layer)\n",
    "    temp_extracted_density = f\"{output_gdb}\\\\temp_extracted_density\"\n",
    "    extracted_density.save(temp_extracted_density)\n",
    "    arcpy.sa.ZonalStatisticsAsTable(fishnet_layer, \"IndexID\", temp_extracted_density, output_table, \"NODATA\", \"MEAN\")\n",
    "    arcpy.management.Delete(temp_extracted_density)\n",
    "    print(f\"Average density calculated and saved to {output_table}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating the Effective Slope\n",
    "\n",
    "Since the slope data is provided in a raster, I'll need to segment this data to use only the slope data pertinent to the layers in my dataset. In the below function, we'll take the raster data and mask it with the layers provided in comb_feats, a list of features that we'll use to calculate the average slope.\n",
    "\n",
    "Using these combined features we can determine the exact slope of the sidewalk in a fishnet grid, rather than use the average slope over a grid as a proxy for the slope of the infrastructure a person will actually be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted existing C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\walkscore_fishnet.\n",
      "Copied fishnet_clipped to walkscore_fishnet.\n",
      "Index field populated with unique values.\n",
      "Calculated total area for each fishnet grid cell.\n"
     ]
    }
   ],
   "source": [
    "arcpy.env.overwriteOutput = True  # Allow outputs to be overwritten\n",
    "\n",
    "# Get the spatial reference of the fishnet layer\n",
    "fishnet_sr = arcpy.Describe(fishnet_layer).spatialReference\n",
    "\n",
    "# Define the walkscore_fishnet_layer\n",
    "walkscore_fishnet_layer = f\"{output_gdb}\\\\walkscore_fishnet\"\n",
    "\n",
    "# Check if the walkscore_fishnet_layer exists and delete it if it does\n",
    "if arcpy.Exists(walkscore_fishnet_layer):\n",
    "    arcpy.management.Delete(walkscore_fishnet_layer)\n",
    "    print(f\"Deleted existing {walkscore_fishnet_layer}.\")\n",
    "\n",
    "# Create a copy of the fishnet layer to work on\n",
    "arcpy.management.CopyFeatures(fishnet_layer, walkscore_fishnet_layer)\n",
    "print(\"Copied fishnet_clipped to walkscore_fishnet.\")\n",
    "\n",
    "# Add a new field for indexing and populate it with unique values\n",
    "index_field = \"IndexID\"\n",
    "if not any(f.name == index_field for f in arcpy.ListFields(walkscore_fishnet_layer)):\n",
    "    arcpy.management.AddField(walkscore_fishnet_layer, index_field, \"LONG\")\n",
    "\n",
    "# Populate the new index field with unique values\n",
    "with arcpy.da.UpdateCursor(walkscore_fishnet_layer, [index_field]) as cursor:\n",
    "    for i, row in enumerate(cursor):\n",
    "        row[0] = i + 1\n",
    "        cursor.updateRow(row)\n",
    "\n",
    "print(\"Index field populated with unique values.\")\n",
    "\n",
    "# Add a new field for total area if it doesn't exist\n",
    "total_area_field = \"total_area\"\n",
    "if not any(f.name == total_area_field for f in arcpy.ListFields(walkscore_fishnet_layer)):\n",
    "    arcpy.management.AddField(walkscore_fishnet_layer, total_area_field, \"DOUBLE\")\n",
    "\n",
    "# Calculate the total area for each fishnet grid cell\n",
    "arcpy.management.CalculateGeometryAttributes(walkscore_fishnet_layer, [[total_area_field, \"AREA_GEODESIC\"]])\n",
    "print(\"Calculated total area for each fishnet grid cell.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Process Each Layer and Calculate Allocations for each Fishnet Grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Effective Area Scalers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# norm_fields = [sidewalk_score_field, park_score_field, trail_score_field, street_score_field, bike_score_field]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalers = {\n",
    "    \"parkinglots\": 1,\n",
    "    \"industrial\": 1,\n",
    "    \"golfcourse\": 1,\n",
    "    \"hospitals\": 1,\n",
    "    \"cemeteries\": 1\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing Public Amenity Data & Separating Out Amenity Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique fclass values: 108\n",
      "Unique fclass values:\n",
      "supermarket\n",
      "computer_shop\n",
      "library\n",
      "bank\n",
      "vending_any\n",
      "bookshop\n",
      "college\n",
      "recycling_clothes\n",
      "pitch\n",
      "community_centre\n",
      "laundry\n",
      "sports_centre\n",
      "video_shop\n",
      "arts_centre\n",
      "newsagent\n",
      "drinking_water\n",
      "recycling_glass\n",
      "fast_food\n",
      "university\n",
      "biergarten\n",
      "doityourself\n",
      "guesthouse\n",
      "tower\n",
      "attraction\n",
      "picnic_site\n",
      "greengrocer\n",
      "pharmacy\n",
      "furniture_shop\n",
      "jeweller\n",
      "school\n",
      "playground\n",
      "swimming_pool\n",
      "clothes\n",
      "embassy\n",
      "theatre\n",
      "recycling\n",
      "post_office\n",
      "artwork\n",
      "veterinary\n",
      "doctors\n",
      "travel_agent\n",
      "florist\n",
      "vending_machine\n",
      "dentist\n",
      "museum\n",
      "toilet\n",
      "bench\n",
      "garden_centre\n",
      "pub\n",
      "beverages\n",
      "restaurant\n",
      "water_well\n",
      "bar\n",
      "hospital\n",
      "comms_tower\n",
      "vending_parking\n",
      "sports_shop\n",
      "optician\n",
      "bakery\n",
      "car_dealership\n",
      "car_sharing\n",
      "toy_shop\n",
      "cafe\n",
      "dog_park\n",
      "nightclub\n",
      "bicycle_shop\n",
      "car_rental\n",
      "recycling_paper\n",
      "clinic\n",
      "mall\n",
      "hostel\n",
      "gift_shop\n",
      "monument\n",
      "courthouse\n",
      "shelter\n",
      "bicycle_rental\n",
      "atm\n",
      "stationery\n",
      "convenience\n",
      "fire_station\n",
      "wastewater_plant\n",
      "butcher\n",
      "chemist\n",
      "hairdresser\n",
      "camera_surveillance\n",
      "food_court\n",
      "beauty_shop\n",
      "general\n",
      "kindergarten\n",
      "mobile_phone_shop\n",
      "post_box\n",
      "outdoor_shop\n",
      "car_wash\n",
      "viewpoint\n",
      "ruins\n",
      "fountain\n",
      "wayside_shrine\n",
      "motel\n",
      "memorial\n",
      "tourist_info\n",
      "market_place\n",
      "telephone\n",
      "town_hall\n",
      "waste_basket\n",
      "shoe_shop\n",
      "cinema\n",
      "department_store\n",
      "hotel\n"
     ]
    }
   ],
   "source": [
    "workspace = r\"C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\"\n",
    "layer = r\"C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\PointsofInterest\"\n",
    "\n",
    "# Use a set to collect unique fclass values\n",
    "fclass_set = set()\n",
    "\n",
    "# Use a SearchCursor to iterate through the fclass field\n",
    "with arcpy.da.SearchCursor(layer, [\"fclass\"]) as cursor:\n",
    "    for row in cursor:\n",
    "        fclass_set.add(row[0])\n",
    "\n",
    "# Print the unique fclass values and their count\n",
    "unique_fclass_count = len(fclass_set)\n",
    "print(f\"Number of unique fclass values: {unique_fclass_count}\")\n",
    "print(\"Unique fclass values:\")\n",
    "for fclass in fclass_set:\n",
    "    print(fclass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "business_amenities = [\n",
    "    'supermarket', 'convenience', 'greengrocer', 'butcher', 'department_store', 'mall', \n",
    "    'gift_shop', 'shoe_shop', 'clothes', 'bookshop', 'stationery', 'furniture_shop', 'jeweller', \n",
    "    'computer_shop', 'mobile_phone_shop', 'outdoor_shop', 'general', 'florist', 'toy_shop',\n",
    "    'beauty_shop', 'laundry', 'bank', 'atm', 'cafe', 'restaurant', \n",
    "    'pub', 'bar', 'fast_food', 'bakery', 'food_court', 'beverages', 'nightclub', 'car_sharing',\n",
    "    'car_wash', 'video_shop', 'vending_any', 'theatre', 'museum', 'attraction', 'cinema',\n",
    "    'market_place', 'mobile_phone_shop', 'bookshop', 'laundry', 'mobile_phone_shop',\n",
    "    'garden_centre','doityourself','hairdresser','bicycle_shop','biergarten','sports_shop'\n",
    "]\n",
    "public_amenities = [\n",
    "    'bench', 'drinking_water', 'waste_basket', 'library', 'post_box','post_office', 'recycling', \n",
    "    'recycling_glass', 'recycling_paper', 'vending_machine', 'artwork', 'tourist_info',\n",
    "    'viewpoint', 'monument', 'picnic_site', 'memorial', 'fountain', 'shelter', 'public_building',\n",
    "    'arts_centre','courthouse','community_centre'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main Processing Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing layer: Business_Amenities\n",
      "Skipping point layer Business_Amenities for separate kernel density processing.\n",
      "Processing layer: Industrial\n",
      "Processing layer: ParkingLots\n",
      "Processing layer: GolfCourse\n",
      "Processing layer: Cemeteries\n",
      "Processing layer: Hospitals\n",
      "Processing layer: Slope\n",
      "Layer Slope does not have a shapeType attribute. Skipping.\n",
      "Processing layer: Bike_greenways\n",
      "Processing layer: Bike_protected\n",
      "Processing layer: Bike_buffer\n",
      "Processing layer: Healthy_Streets\n",
      "Processing layer: Parks\n",
      "Processing layer: Universities\n",
      "Processing layer: Sidewalks\n",
      "Processing layer: Plaza\n",
      "Processing layer: trails\n",
      "Processing layer: MultiUseTrails\n",
      "Processing layer: Streets\n",
      "Calculated effective area for C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\Streets_int and stored in Streets_effective_area.\n",
      "Processing layer: population\n",
      "Joined SUM_proportional_population to C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\walkscore_fishnet and created summary table C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\population_sum.\n",
      "Processing layer: crashes\n",
      "Joined COUNT_OBJECTID to C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\walkscore_fishnet and created summary table C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\crashes_sum.\n",
      "Processing layer: SPD_Crime_Data\n",
      "Area field SPD_Crime_Data_area was not created for SPD_Crime_Data, skipping summary statistics.\n",
      "Main processing complete for all layers.\n"
     ]
    }
   ],
   "source": [
    "# Main processing loop for preprocessing layers\n",
    "created_layers = []\n",
    "\n",
    "for layer_name in base_layers:\n",
    "    print(f\"Processing layer: {layer_name}\")\n",
    "\n",
    "    input_layer = f\"{base_layers_group}\\\\{layer_name}\"\n",
    "\n",
    "    if not arcpy.Exists(input_layer):\n",
    "        print(f\"Layer {input_layer} does not exist. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    desc = arcpy.Describe(input_layer)\n",
    "    if hasattr(desc, \"shapeType\"):\n",
    "        geometry_type = desc.shapeType\n",
    "    else:\n",
    "        print(f\"Layer {layer_name} does not have a shapeType attribute. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    # Handle crash layer\n",
    "    if layer_name.lower() == \"crashes\":\n",
    "        intersect_output = f\"{workspace}\\\\{layer_name}_intersect\"\n",
    "        summary_output = f\"{workspace}\\\\{layer_name}_sum\"\n",
    "        id_field = \"OBJECTID\"\n",
    "        calculate_counts(input_layer, intersect_output, walkscore_fishnet_layer, summary_output, id_field)\n",
    "        created_layers.append(summary_output)\n",
    "        continue\n",
    "\n",
    "    # Handle Population layer\n",
    "    if layer_name.lower() == \"population\":\n",
    "        intersect_output = f\"{workspace}\\\\{layer_name}_intersect\"\n",
    "        summary_output = f\"{workspace}\\\\{layer_name}_sum\"\n",
    "        id_field = \"OBJECTID\"\n",
    "        density_field = \"density_2023\"\n",
    "        calculate_population_density(input_layer, intersect_output, walkscore_fishnet_layer, summary_output, id_field, density_field)\n",
    "        created_layers.append(summary_output)\n",
    "        continue\n",
    "        \n",
    "    # Handle crash layer\n",
    "    if layer_name.lower() == \"SPD_Crime_Data\":\n",
    "        intersect_output = f\"{workspace}\\\\{layer_name}_intersect\"\n",
    "        summary_output = f\"{workspace}\\\\{layer_name}_sum\"\n",
    "        id_field = \"OBJECTID\"\n",
    "        calculate_counts(input_layer, intersect_output, walkscore_fishnet_layer, summary_output, id_field)\n",
    "        created_layers.append(summary_output)\n",
    "        continue\n",
    "\n",
    "    # Skip Point layers (e.g., business_amenities) for now; handle them later for Kernel Density\n",
    "    if geometry_type == \"Point\" and layer_name.endswith(\"_Amenities\"):\n",
    "        print(f\"Skipping point layer {layer_name} for separate kernel density processing.\")\n",
    "        continue\n",
    "\n",
    "    # Handle Polygons and Polylines as before\n",
    "    input_layer_sr = desc.spatialReference\n",
    "    fishnet_sr = arcpy.Describe(walkscore_fishnet_layer).spatialReference\n",
    "    projected_layer = f\"{workspace}\\\\{layer_name}_proj\"\n",
    "\n",
    "    if input_layer_sr.name != fishnet_sr.name:\n",
    "        if arcpy.Exists(projected_layer):\n",
    "            arcpy.management.Delete(projected_layer)\n",
    "        arcpy.management.Project(input_layer, projected_layer, fishnet_sr)\n",
    "    else:\n",
    "        projected_layer = input_layer\n",
    "\n",
    "    intersect_output = f\"{workspace}\\\\{layer_name}_int\"\n",
    "\n",
    "    if arcpy.Exists(intersect_output):\n",
    "        arcpy.management.Delete(intersect_output)\n",
    "\n",
    "    arcpy.analysis.Intersect([walkscore_fishnet_layer, projected_layer], intersect_output)\n",
    "\n",
    "    if layer_name.lower() == \"streets\":\n",
    "        effective_area_field = f\"{layer_name}_effective_area\"\n",
    "        calculate_effective_area(intersect_output, effective_area_field)\n",
    "        area_field = effective_area_field\n",
    "\n",
    "        # Run the calculate_max_speed_limit function here for the Streets layer\n",
    "        output_table = f\"{output_gdb}\\\\max_speed_limit_Streets_int\"\n",
    "        calculate_max_speed_limit(intersect_output, walkscore_fishnet_layer, output_table, \"effective_SPEEDLIMIT\")\n",
    "\n",
    "    elif layer_name.lower() in scalers.keys():\n",
    "        area_field = f\"{layer_name}_area\"\n",
    "        area_field = area_field.replace(\"-\", \"_\").replace(\" \", \"_\")\n",
    "        if geometry_type == \"Polygon\":\n",
    "            calculate_polygon_area(intersect_output, area_field)\n",
    "        effective_area_field = f\"{layer_name}_effective_area\"\n",
    "        if not any(f.name == effective_area_field for f in arcpy.ListFields(intersect_output)):\n",
    "            arcpy.management.AddField(intersect_output, effective_area_field, \"DOUBLE\")\n",
    "        scaler = scalers[layer_name.lower()]\n",
    "        with arcpy.da.UpdateCursor(intersect_output, [area_field, effective_area_field]) as cursor:\n",
    "            for row in cursor:\n",
    "                if row[0] is not None:\n",
    "                    row[1] = row[0] * scaler\n",
    "                else:\n",
    "                    row[1] = None\n",
    "                cursor.updateRow(row)\n",
    "        area_field = effective_area_field\n",
    "    else:\n",
    "        area_field = f\"{layer_name}_area\"\n",
    "        area_field = area_field.replace(\"-\", \"_\").replace(\" \", \"_\")\n",
    "        if geometry_type == \"Polygon\":\n",
    "            calculate_polygon_area(intersect_output, area_field)\n",
    "        elif geometry_type == \"Polyline\":\n",
    "            width_field = None\n",
    "            for field in arcpy.ListFields(intersect_output):\n",
    "                if field.name.lower().endswith(\"width\"):\n",
    "                    width_field = field.name\n",
    "            if width_field:\n",
    "                calculate_polyline_area_with_recalculated_length(intersect_output, area_field, width_field)\n",
    "            else:\n",
    "                print(f\"Width field not found for {layer_name}, skipping area calculation.\")\n",
    "\n",
    "    if not any(f.name.lower() == area_field.lower() for f in arcpy.ListFields(intersect_output)):\n",
    "        print(f\"Area field {area_field} was not created for {layer_name}, skipping summary statistics.\")\n",
    "        continue\n",
    "\n",
    "    summary_output = f\"{workspace}\\\\{layer_name}_sum\"\n",
    "    if arcpy.Exists(summary_output):\n",
    "        arcpy.management.Delete(summary_output)\n",
    "\n",
    "    if not any(f.name == index_field for f in arcpy.ListFields(intersect_output)):\n",
    "        arcpy.management.AddField(intersect_output, index_field, \"LONG\")\n",
    "        with arcpy.da.UpdateCursor(intersect_output, [index_field]) as cursor:\n",
    "            for i, row in enumerate(cursor):\n",
    "                row[0] = i + 1\n",
    "                cursor.updateRow(row)\n",
    "\n",
    "    arcpy.analysis.Statistics(intersect_output, summary_output, [[area_field, \"SUM\"]], index_field)\n",
    "    created_layers.append(summary_output)\n",
    "\n",
    "print(\"Main processing complete for all layers.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Slope Processing Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing slope for layer: Sidewalks\n",
      "Calculating slope for Sidewalks\n",
      "Processing slope for layer: Streets\n",
      "Calculating slope for Streets\n",
      "Processing slope for layer: MultiUseTrails\n",
      "Calculating slope for MultiUseTrails\n",
      "Processing slope for layer: trails\n",
      "Calculating slope for trails\n",
      "Slope processing complete for all layers.\n",
      "Sample data from all_data for debugging:\n",
      "IndexID: 1, Data: {'Sidewalks_Slope_Mean': 2.6028627527171175, 'Streets_Slope_Mean': 2.4719635209729587, 'MultiUseTrails_Slope_Mean': None, 'trails_Slope_Mean': None, 'Grid_Slope_MEAN': 3.0777811209360757}\n",
      "IndexID: 2, Data: {'Sidewalks_Slope_Mean': 1.8433810224135716, 'Streets_Slope_Mean': 0.9660569752256075, 'MultiUseTrails_Slope_Mean': None, 'trails_Slope_Mean': None, 'Grid_Slope_MEAN': 2.2087172894842113}\n",
      "IndexID: 3, Data: {'Sidewalks_Slope_Mean': None, 'Streets_Slope_Mean': 3.095643554415022, 'MultiUseTrails_Slope_Mean': None, 'trails_Slope_Mean': 2.7575330917651844, 'Grid_Slope_MEAN': 2.269062724378373}\n",
      "IndexID: 4, Data: {'Sidewalks_Slope_Mean': 4.099437493544359, 'Streets_Slope_Mean': None, 'MultiUseTrails_Slope_Mean': None, 'trails_Slope_Mean': None, 'Grid_Slope_MEAN': 4.542804545164107}\n",
      "IndexID: 5, Data: {'Sidewalks_Slope_Mean': None, 'Streets_Slope_Mean': None, 'MultiUseTrails_Slope_Mean': None, 'trails_Slope_Mean': None, 'Grid_Slope_MEAN': 32.985248031038225}\n",
      "Combined slope mean calculated and updated.\n"
     ]
    }
   ],
   "source": [
    "slope_layers = ['Sidewalks', 'Streets', 'MultiUseTrails', 'trails']\n",
    "\n",
    "# Calculate slope for entire grid\n",
    "grid_slope_output_table = f\"{output_gdb}\\\\grid_slope\"\n",
    "calculate_average_slope(walkscore_fishnet_layer, \"Slope\", walkscore_fishnet_layer, grid_slope_output_table)\n",
    "\n",
    "# Check if \"Grid_Slope_MEAN\" field already exists, and join or alter as necessary\n",
    "if not any(f.name == \"Grid_Slope_MEAN\" for f in arcpy.ListFields(walkscore_fishnet_layer)):\n",
    "    arcpy.management.JoinField(walkscore_fishnet_layer, \"IndexID\", grid_slope_output_table, \"IndexID\", \"MEAN\")\n",
    "    arcpy.management.AlterField(walkscore_fishnet_layer, \"MEAN\", \"Grid_Slope_MEAN\")\n",
    "else:\n",
    "    print(\"Grid_Slope_MEAN field already exists. Skipping join and alter operations.\")\n",
    "\n",
    "# Process slope for specific polyline layers\n",
    "for layer_name in slope_layers:\n",
    "    print(f\"Processing slope for layer: {layer_name}\")\n",
    "\n",
    "    input_layer = f\"{base_layers_group}\\\\{layer_name}\"\n",
    "    \n",
    "    if not arcpy.Exists(input_layer):\n",
    "        print(f\"Layer {input_layer} does not exist. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    desc = arcpy.Describe(input_layer)\n",
    "    if hasattr(desc, \"shapeType\"):\n",
    "        geometry_type = desc.shapeType\n",
    "        if geometry_type != \"Polyline\":\n",
    "            print(f\"Layer {layer_name} is not a polyline. Skipping slope calculation.\")\n",
    "            continue\n",
    "    else:\n",
    "        print(f\"Layer {layer_name} does not have a shapeType attribute. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    intersect_output = f\"{workspace}\\\\{layer_name}_int\"\n",
    "    slope_output_table = f\"{workspace}\\\\{layer_name}_slope\"\n",
    "\n",
    "    if arcpy.Exists(intersect_output):\n",
    "        print(f\"Calculating slope for {layer_name}\")\n",
    "        calculate_average_slope(intersect_output, \"Slope\", intersect_output, slope_output_table)\n",
    "\n",
    "        effective_slope_field = f\"{layer_name}_Slope_Mean\"\n",
    "\n",
    "        # Ensure the effective_slope_field exists\n",
    "        if not any(f.name == effective_slope_field for f in arcpy.ListFields(walkscore_fishnet_layer)):\n",
    "            arcpy.management.AddField(walkscore_fishnet_layer, effective_slope_field, \"DOUBLE\")\n",
    "\n",
    "        slope_df = arcpy.da.TableToNumPyArray(slope_output_table, [\"IndexID\", \"MEAN\"])\n",
    "        slope_dict = {row[\"IndexID\"]: row[\"MEAN\"] for row in slope_df}\n",
    "\n",
    "        with arcpy.da.UpdateCursor(walkscore_fishnet_layer, [\"IndexID\", effective_slope_field]) as cursor:\n",
    "            for row in cursor:\n",
    "                row[1] = slope_dict.get(row[0], None)\n",
    "                cursor.updateRow(row)\n",
    "    else:\n",
    "        print(f\"Intersect output for {layer_name} does not exist. Skipping.\")\n",
    "\n",
    "print(\"Slope processing complete for all layers.\")\n",
    "\n",
    "# Ensure that the combined slope mean is calculated correctly\n",
    "slope_fields = [f\"{layer}_Slope_Mean\" for layer in slope_layers if any(f.name == f\"{layer}_Slope_Mean\" for f in arcpy.ListFields(walkscore_fishnet_layer))]\n",
    "\n",
    "# Collect all necessary data in one go\n",
    "all_data = {\n",
    "    row[0]: {\n",
    "        field: row[idx + 1] for idx, field in enumerate(slope_fields + [\"Grid_Slope_MEAN\"])\n",
    "    } for row in arcpy.da.SearchCursor(walkscore_fishnet_layer, [\"IndexID\"] + slope_fields + [\"Grid_Slope_MEAN\"])\n",
    "}\n",
    "\n",
    "# Debug: Print a few entries to check data integrity\n",
    "print(\"Sample data from all_data for debugging:\")\n",
    "for idx, (key, value) in enumerate(all_data.items()):\n",
    "    if idx < 5:  # Print first 5 entries\n",
    "        print(f\"IndexID: {key}, Data: {value}\")\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# Update the effective_slope field based on the collected data\n",
    "if not any(f.name == \"effective_slope\" for f in arcpy.ListFields(walkscore_fishnet_layer)):\n",
    "    arcpy.management.AddField(walkscore_fishnet_layer, \"effective_slope\", \"DOUBLE\")\n",
    "\n",
    "with arcpy.da.UpdateCursor(walkscore_fishnet_layer, [\"IndexID\", \"effective_slope\"] + slope_fields + [\"Grid_Slope_MEAN\"]) as cursor:\n",
    "    for row in cursor:\n",
    "        index_id = row[0]\n",
    "        sidewalks_slope = all_data[index_id].get(\"Sidewalks_Slope_Mean\")\n",
    "        multiuse_trails_slope = all_data[index_id].get(\"MultiUseTrails_Slope_Mean\")\n",
    "        streets_slope = all_data[index_id].get(\"Streets_Slope_Mean\")\n",
    "        grid_slope_mean = all_data[index_id][\"Grid_Slope_MEAN\"]\n",
    "\n",
    "        if sidewalks_slope is not None:\n",
    "            slope_mean = sidewalks_slope\n",
    "        elif multiuse_trails_slope is not None:\n",
    "            slope_mean = multiuse_trails_slope\n",
    "        elif streets_slope is not None:\n",
    "            slope_mean = streets_slope\n",
    "        else:\n",
    "            slope_mean = grid_slope_mean\n",
    "\n",
    "        row[1] = slope_mean\n",
    "        cursor.updateRow(row)\n",
    "\n",
    "print(\"Combined slope mean calculated and updated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Business Density Processing Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Kernel Density for business layer: Business_Amenities\n",
      "Clipped points for Business_Amenities using the barrier layer.\n",
      "Generated kernel density heatmap for Business_Amenities using clipped features.\n",
      "Applied focal statistics to smooth the kernel density heatmap for Business_Amenities.\n",
      "Average density calculated and saved to C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\Business_Amenities_zonal_stats.\n",
      "Added new field 'business_density'.\n",
      "Deleted temporary field 'MEAN' after updating 'business_density'.\n",
      "Updated 'business_density' field with mean density values for Business_Amenities.\n",
      "Business density processing complete for all layers.\n"
     ]
    }
   ],
   "source": [
    "# List all business amenity point layers to apply kernel density\n",
    "business_layers = [layer_name for layer_name in base_layers if layer_name.endswith(\"_Amenities\")]\n",
    "\n",
    "# Define the path to the neighborhood layer that will be used as a barrier\n",
    "barrier_layer = \"neighborhoods\"\n",
    "\n",
    "for layer_name in business_layers:\n",
    "    print(f\"Processing Kernel Density for business layer: {layer_name}\")\n",
    "\n",
    "    input_layer = f\"{base_layers_group}\\\\{layer_name}\"\n",
    "\n",
    "    if not arcpy.Exists(input_layer):\n",
    "        print(f\"Layer {input_layer} does not exist. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    # Step 1: Clip the Point Features by the Barrier Layer\n",
    "    clipped_points = f\"{workspace}\\\\{layer_name}_clipped\"\n",
    "    arcpy.analysis.Clip(\n",
    "        in_features=input_layer,\n",
    "        clip_features=barrier_layer,\n",
    "        out_feature_class=clipped_points\n",
    "    )\n",
    "    print(f\"Clipped points for {layer_name} using the barrier layer.\")\n",
    "\n",
    "    # Step 2: Apply Kernel Density Tool to Generate Business Density Raster\n",
    "    kernel_density_output = f\"{workspace}\\\\{layer_name}_kernel_density\"\n",
    "    arcpy.sa.KernelDensity(\n",
    "        in_features=clipped_points,\n",
    "        population_field=\"NONE\",\n",
    "        out_cell_values=\"DENSITIES\",\n",
    "        method='GEODESIC',\n",
    "        cell_size=\"6.59609600103067E-04\",  # Match cell size to your fishnet grid resolution\n",
    "        search_radius=\"200\",  # Adjust based on the intended influence\n",
    "        area_unit_scale_factor=\"SQUARE_FEET\"\n",
    "    ).save(kernel_density_output)\n",
    "    print(f\"Generated kernel density heatmap for {layer_name} using clipped features.\")\n",
    "\n",
    "    # Step 3: Apply Smoothing Using Focal Statistics\n",
    "    smoothed_kernel_density_output = f\"{workspace}\\\\{layer_name}_smoothed_kernel_density\"\n",
    "    smoothed_raster = arcpy.sa.FocalStatistics(\n",
    "        in_raster=kernel_density_output,\n",
    "        neighborhood=arcpy.sa.NbrCircle(radius=8, units=\"CELL\"),\n",
    "        statistics_type=\"MEAN\"\n",
    "    )\n",
    "    smoothed_raster.save(smoothed_kernel_density_output)\n",
    "    print(f\"Applied focal statistics to smooth the kernel density heatmap for {layer_name}.\")\n",
    "\n",
    "    # Step 4: Calculate Average Density for Each Fishnet Grid Using Zonal Statistics\n",
    "    zonal_output_table = f\"{workspace}\\\\{layer_name}_zonal_stats\"\n",
    "    calculate_average_density(walkscore_fishnet_layer, smoothed_kernel_density_output, walkscore_fishnet_layer, zonal_output_table)\n",
    "\n",
    "    # Step 5: Join Zonal Statistics Back to Fishnet Grid\n",
    "    business_density_field = \"business_density\"\n",
    "\n",
    "    # Delete the existing \"business_density\" field if it already exists\n",
    "    if any(f.name == business_density_field for f in arcpy.ListFields(walkscore_fishnet_layer)):\n",
    "        arcpy.management.DeleteField(walkscore_fishnet_layer, business_density_field)\n",
    "        print(f\"Deleted existing field '{business_density_field}'.\")\n",
    "\n",
    "    # Add the new \"business_density\" field\n",
    "    arcpy.management.AddField(walkscore_fishnet_layer, business_density_field, \"DOUBLE\")\n",
    "    print(f\"Added new field '{business_density_field}'.\")\n",
    "\n",
    "    # Join the mean values from the zonal stats output back to the fishnet\n",
    "    arcpy.management.JoinField(\n",
    "        in_data=walkscore_fishnet_layer,\n",
    "        in_field=\"IndexID\",\n",
    "        join_table=zonal_output_table,\n",
    "        join_field=\"IndexID\",\n",
    "        fields=[\"MEAN\"]\n",
    "    )\n",
    "\n",
    "    # Update the \"business_density\" field using the \"MEAN\" field from the joined table, setting nulls to 0\n",
    "    with arcpy.da.UpdateCursor(walkscore_fishnet_layer, [\"MEAN\", business_density_field]) as cursor:\n",
    "        for row in cursor:\n",
    "            row[1] = row[0] if row[0] is not None else 0  # Copy the value from the \"MEAN\" field or set to 0 if None\n",
    "            cursor.updateRow(row)\n",
    "\n",
    "    # Delete the \"MEAN\" field after copying values\n",
    "    arcpy.management.DeleteField(walkscore_fishnet_layer, \"MEAN\")\n",
    "    print(f\"Deleted temporary field 'MEAN' after updating 'business_density'.\")\n",
    "\n",
    "    print(f\"Updated 'business_density' field with mean density values for {layer_name}.\")\n",
    "\n",
    "print(\"Business density processing complete for all layers.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min-max normalized 'business_density' field to a scale of 0-5.\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Rank Normalize the Business Density Field Between 0-5\n",
    "business_density_values = []\n",
    "\n",
    "with arcpy.da.SearchCursor(walkscore_fishnet_layer, [\"business_density\"]) as cursor:\n",
    "    for row in cursor:\n",
    "        if row[0] is not None:\n",
    "            business_density_values.append(row[0])\n",
    "\n",
    "# Step 4.2: Calculate min and max values of business density\n",
    "min_value = min(business_density_values) if business_density_values else 0\n",
    "max_value = max(business_density_values) if business_density_values else 1  # Avoid division by zero\n",
    "\n",
    "# Step 4.3: Update the \"business_density\" field using min-max normalization between 0-5\n",
    "with arcpy.da.UpdateCursor(walkscore_fishnet_layer, [\"business_density\"]) as cursor:\n",
    "    for row in cursor:\n",
    "        if row[0] is not None:\n",
    "            # Apply min-max normalization to scale between 0-5\n",
    "            if max_value > min_value:\n",
    "                row[0] = ((row[0] - min_value) / (max_value - min_value)) * 5\n",
    "            else:\n",
    "                row[0] = 0  # In case all values are the same, set normalized value to 0\n",
    "        else:\n",
    "            row[0] = 0  # Set to 0 for null business density\n",
    "        cursor.updateRow(row)\n",
    "\n",
    "print(\"Min-max normalized 'business_density' field to a scale of 0-5.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating an Index Field for walkscore_fishnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Populate the new index field with unique values\n",
    "with arcpy.da.UpdateCursor(walkscore_fishnet_layer, [index_field]) as cursor:\n",
    "    for i, row in enumerate(cursor):\n",
    "        row[0] = i + 1\n",
    "        cursor.updateRow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3:  Join Summary Statistic Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deleted existing summary table\n",
      "Created merged_sums table.\n"
     ]
    }
   ],
   "source": [
    "merged_summary = f\"{output_gdb}\\\\merged_sums\"\n",
    "\n",
    "if arcpy.Exists(merged_summary):\n",
    "    arcpy.management.Delete(merged_summary)\n",
    "    print('deleted existing summary table')\n",
    "    \n",
    "arcpy.management.CreateTable(output_gdb, \"merged_sums\")\n",
    "print(\"Created merged_sums table.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated merged summary table created successfully.\n"
     ]
    }
   ],
   "source": [
    "# Add IndexID field to the merged summary table if it doesn't exist\n",
    "if not any(f.name == index_field for f in arcpy.ListFields(merged_summary)):\n",
    "    arcpy.management.AddField(merged_summary, index_field, \"LONG\")\n",
    "\n",
    "# Create a dictionary to store the aggregated sums\n",
    "aggregated_sums = defaultdict(lambda: defaultdict(float))\n",
    "\n",
    "# Iterate through each summary table and aggregate values by IndexID\n",
    "for layer_name in base_layers:\n",
    "    summary_output = f\"{output_gdb}\\\\{layer_name}_sum\"\n",
    "    \n",
    "    # Verify if summary_output exists\n",
    "    if not arcpy.Exists(summary_output):\n",
    "        print(f\"Summary table {summary_output} does not exist. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    fields = arcpy.ListFields(summary_output)\n",
    "    field_names = [f.name for f in fields if f.name != index_field]\n",
    "    \n",
    "    # Aggregate the summary fields into the dictionary\n",
    "    with arcpy.da.SearchCursor(summary_output, [index_field] + field_names) as cursor:\n",
    "        for row in cursor:\n",
    "            idx = row[0]\n",
    "            for i, field_name in enumerate(field_names):\n",
    "                value = row[i+1] if row[i+1] is not None else 0\n",
    "                aggregated_sums[idx][f\"{layer_name}_{field_name}\"] += value\n",
    "\n",
    "# Add aggregated fields to the merged summary table\n",
    "for layer_name in base_layers:\n",
    "    summary_output = f\"{output_gdb}\\\\{layer_name}_sum\"\n",
    "    \n",
    "    # Verify if summary_output exists\n",
    "    if not arcpy.Exists(summary_output):\n",
    "        print(f\"Summary table {summary_output} does not exist. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    fields = arcpy.ListFields(summary_output)\n",
    "    for field in fields:\n",
    "        if field.name != index_field:\n",
    "            field_name = f\"{layer_name}_{field.name}\"\n",
    "            if not any(f.name == field_name for f in arcpy.ListFields(merged_summary)):\n",
    "                arcpy.management.AddField(merged_summary, field_name, \"DOUBLE\")\n",
    "\n",
    "# Insert the aggregated sums into the merged summary table\n",
    "field_names_to_insert = [index_field] + [f\"{layer_name}_{field.name}\" for layer_name in base_layers for field in arcpy.ListFields(f\"{output_gdb}\\\\{layer_name}_sum\") if field.name != index_field]\n",
    "with arcpy.da.InsertCursor(merged_summary, field_names_to_insert) as cursor:\n",
    "    for idx, fields in aggregated_sums.items():\n",
    "        row = [idx] + [fields.get(field_name, 0) for field_name in field_names_to_insert if field_name != index_field]\n",
    "        cursor.insertRow(row)\n",
    "\n",
    "print(\"Aggregated merged summary table created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fields in merged summary table: ['OBJECTID', 'IndexID', 'Business_Amenities_OBJECTID', 'Business_Amenities_FREQUENCY', 'Business_Amenities_COUNT_osm_business_id', 'Industrial_OBJECTID', 'Industrial_FREQUENCY', 'Industrial_SUM_Industrial_effective_area', 'ParkingLots_OBJECTID', 'ParkingLots_FREQUENCY', 'ParkingLots_SUM_ParkingLots_effective_area', 'GolfCourse_OBJECTID', 'GolfCourse_FREQUENCY', 'GolfCourse_SUM_GolfCourse_effective_area', 'Cemeteries_OBJECTID', 'Cemeteries_FREQUENCY', 'Cemeteries_SUM_Cemeteries_effective_area', 'Hospitals_OBJECTID', 'Hospitals_FREQUENCY', 'Hospitals_SUM_Hospitals_effective_area', 'Slope_OBJECTID', 'Slope_COUNT', 'Slope_AREA', 'Slope_MEAN', 'Bike_greenways_OBJECTID', 'Bike_greenways_FREQUENCY', 'Bike_greenways_SUM_Bike_greenways_area', 'Bike_protected_OBJECTID', 'Bike_protected_FREQUENCY', 'Bike_protected_SUM_Bike_protected_area', 'Bike_buffer_OBJECTID', 'Bike_buffer_FREQUENCY', 'Bike_buffer_SUM_Bike_buffer_area', 'Healthy_Streets_OBJECTID', 'Healthy_Streets_FREQUENCY', 'Healthy_Streets_SUM_Healthy_Streets_area', 'Parks_OBJECTID', 'Parks_FREQUENCY', 'Parks_SUM_Parks_area', 'Universities_OBJECTID', 'Universities_FREQUENCY', 'Universities_SUM_Universities_area', 'Sidewalks_OBJECTID', 'Sidewalks_FREQUENCY', 'Sidewalks_SUM_Sidewalks_area', 'Plaza_OBJECTID', 'Plaza_FREQUENCY', 'Plaza_SUM_Plaza_area', 'trails_OBJECTID', 'trails_FREQUENCY', 'trails_SUM_trails_area', 'MultiUseTrails_OBJECTID', 'MultiUseTrails_FREQUENCY', 'MultiUseTrails_SUM_MultiUseTrails_area', 'Streets_OBJECTID', 'Streets_FREQUENCY', 'Streets_SUM_Streets_effective_area', 'population_OBJECTID', 'population_FREQUENCY', 'population_SUM_proportional_population', 'crashes_OBJECTID', 'crashes_FREQUENCY', 'crashes_COUNT_OBJECTID', 'SPD_Crime_Data_OBJECTID', 'SPD_Crime_Data_FREQUENCY', 'SPD_Crime_Data_COUNT_Offense_ID']\n",
      "IndexID already exists in C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\merged_sums.\n",
      "Verified IndexID in merged_sums.\n"
     ]
    }
   ],
   "source": [
    "# Path to the merged summary table\n",
    "merged_summary = r\"C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\merged_sums\"\n",
    "index_field = \"IndexID\"\n",
    "\n",
    "# Check if merged summary table exists\n",
    "if not arcpy.Exists(merged_summary):\n",
    "    raise ValueError(f\"{merged_summary} does not exist.\")\n",
    "\n",
    "# List all fields in the merged summary table for debugging\n",
    "fields = arcpy.ListFields(merged_summary)\n",
    "field_names = [field.name for field in fields]\n",
    "print(\"Fields in merged summary table:\", field_names)\n",
    "\n",
    "# Ensure IndexID exists in merged_sums\n",
    "if not any(f.name == index_field for f in fields):\n",
    "    print(f\"Adding {index_field} to {merged_summary}.\")\n",
    "    arcpy.management.AddField(merged_summary, index_field, \"LONG\")\n",
    "else:\n",
    "    print(f\"{index_field} already exists in {merged_summary}.\")\n",
    "\n",
    "print(\"Verified IndexID in merged_sums.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Join the Summary Statistics to the Fishnet Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    arcpy.management.JoinField(walkscore_fishnet_layer, \"IndexID\", merged_summary, \"IndexID\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during join: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding a neighborhood field to Walkscore Fishnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intersected walkscore fishnet with neighborhoods to create fragments.\n",
      "Using fields: ['nested', 'is_tourist', 'is_industrial']\n",
      "Calculated area for each fragment.\n",
      "Overwritten C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\walkscore_fishnet with the intersected fragments, retaining neighborhood names.\n",
      "Fields in C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\walkscore_fishnet after processing:\n",
      "Name: OBJECTID, Type: OID\n",
      "Name: Shape, Type: Geometry\n",
      "Name: FID_walkscore_fishnet, Type: Integer\n",
      "Name: IndexID, Type: Integer\n",
      "Name: total_area, Type: Double\n",
      "Name: Max_Speed_Limit, Type: Double\n",
      "Name: SUM_proportional_population, Type: Double\n",
      "Name: COUNT_OBJECTID, Type: Integer\n",
      "Name: Grid_Slope_MEAN, Type: Double\n",
      "Name: Sidewalks_Slope_Mean, Type: Double\n",
      "Name: Streets_Slope_Mean, Type: Double\n",
      "Name: MultiUseTrails_Slope_Mean, Type: Double\n",
      "Name: trails_Slope_Mean, Type: Double\n",
      "Name: effective_slope, Type: Double\n",
      "Name: business_density, Type: Double\n",
      "Name: IndexID_1, Type: Integer\n",
      "Name: Business_Amenities_OBJECTID, Type: Double\n",
      "Name: Business_Amenities_FREQUENCY, Type: Double\n",
      "Name: Business_Amenities_COUNT_osm_business_id, Type: Double\n",
      "Name: Industrial_OBJECTID, Type: Double\n",
      "Name: Industrial_FREQUENCY, Type: Double\n",
      "Name: Industrial_SUM_Industrial_effective_area, Type: Double\n",
      "Name: ParkingLots_OBJECTID, Type: Double\n",
      "Name: ParkingLots_FREQUENCY, Type: Double\n",
      "Name: ParkingLots_SUM_ParkingLots_effective_area, Type: Double\n",
      "Name: GolfCourse_OBJECTID, Type: Double\n",
      "Name: GolfCourse_FREQUENCY, Type: Double\n",
      "Name: GolfCourse_SUM_GolfCourse_effective_area, Type: Double\n",
      "Name: Cemeteries_OBJECTID, Type: Double\n",
      "Name: Cemeteries_FREQUENCY, Type: Double\n",
      "Name: Cemeteries_SUM_Cemeteries_effective_area, Type: Double\n",
      "Name: Hospitals_OBJECTID, Type: Double\n",
      "Name: Hospitals_FREQUENCY, Type: Double\n",
      "Name: Hospitals_SUM_Hospitals_effective_area, Type: Double\n",
      "Name: Slope_OBJECTID, Type: Double\n",
      "Name: Slope_COUNT, Type: Double\n",
      "Name: Slope_AREA, Type: Double\n",
      "Name: Slope_MEAN, Type: Double\n",
      "Name: Bike_greenways_OBJECTID, Type: Double\n",
      "Name: Bike_greenways_FREQUENCY, Type: Double\n",
      "Name: Bike_greenways_SUM_Bike_greenways_area, Type: Double\n",
      "Name: Bike_protected_OBJECTID, Type: Double\n",
      "Name: Bike_protected_FREQUENCY, Type: Double\n",
      "Name: Bike_protected_SUM_Bike_protected_area, Type: Double\n",
      "Name: Bike_buffer_OBJECTID, Type: Double\n",
      "Name: Bike_buffer_FREQUENCY, Type: Double\n",
      "Name: Bike_buffer_SUM_Bike_buffer_area, Type: Double\n",
      "Name: Healthy_Streets_OBJECTID, Type: Double\n",
      "Name: Healthy_Streets_FREQUENCY, Type: Double\n",
      "Name: Healthy_Streets_SUM_Healthy_Streets_area, Type: Double\n",
      "Name: Parks_OBJECTID, Type: Double\n",
      "Name: Parks_FREQUENCY, Type: Double\n",
      "Name: Parks_SUM_Parks_area, Type: Double\n",
      "Name: Universities_OBJECTID, Type: Double\n",
      "Name: Universities_FREQUENCY, Type: Double\n",
      "Name: Universities_SUM_Universities_area, Type: Double\n",
      "Name: Sidewalks_OBJECTID, Type: Double\n",
      "Name: Sidewalks_FREQUENCY, Type: Double\n",
      "Name: Sidewalks_SUM_Sidewalks_area, Type: Double\n",
      "Name: Plaza_OBJECTID, Type: Double\n",
      "Name: Plaza_FREQUENCY, Type: Double\n",
      "Name: Plaza_SUM_Plaza_area, Type: Double\n",
      "Name: trails_OBJECTID, Type: Double\n",
      "Name: trails_FREQUENCY, Type: Double\n",
      "Name: trails_SUM_trails_area, Type: Double\n",
      "Name: MultiUseTrails_OBJECTID, Type: Double\n",
      "Name: MultiUseTrails_FREQUENCY, Type: Double\n",
      "Name: MultiUseTrails_SUM_MultiUseTrails_area, Type: Double\n",
      "Name: Streets_OBJECTID, Type: Double\n",
      "Name: Streets_FREQUENCY, Type: Double\n",
      "Name: Streets_SUM_Streets_effective_area, Type: Double\n",
      "Name: population_OBJECTID, Type: Double\n",
      "Name: population_FREQUENCY, Type: Double\n",
      "Name: population_SUM_proportional_population, Type: Double\n",
      "Name: crashes_OBJECTID, Type: Double\n",
      "Name: crashes_FREQUENCY, Type: Double\n",
      "Name: crashes_COUNT_OBJECTID, Type: Double\n",
      "Name: SPD_Crime_Data_OBJECTID, Type: Double\n",
      "Name: SPD_Crime_Data_FREQUENCY, Type: Double\n",
      "Name: SPD_Crime_Data_COUNT_Offense_ID, Type: Double\n",
      "Name: FID_neighborhoods, Type: Integer\n",
      "Name: city, Type: String\n",
      "Name: nested, Type: String\n",
      "Name: neighborhood_area, Type: Double\n",
      "Name: is_tourist, Type: Integer\n",
      "Name: latitude, Type: Double\n",
      "Name: longitude, Type: Double\n",
      "Name: is_industrial, Type: Integer\n",
      "Name: Shape_Length, Type: Double\n",
      "Name: Shape_Area, Type: Double\n",
      "Name: Fragment_Area, Type: Double\n",
      "Fishnet fragments assigned to neighborhoods and saved back to walkscore_fishnet.\n"
     ]
    }
   ],
   "source": [
    "# Set environment settings\n",
    "arcpy.env.overwriteOutput = True  # Allow outputs to be overwritten\n",
    "\n",
    "# Define the input layers\n",
    "walkscore_fishnet = r\"C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\walkscore_fishnet\"\n",
    "neighborhoods = r\"C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\neighborhoods\"\n",
    "fishnet_neighborhoods_intersect = \"fishnet_neighborhoods_intersect\"\n",
    "neighborhood_field = \"nested\"  # Field name to be used from neighborhoods layer\n",
    "\n",
    "# Step 1: Ensure the spatial reference systems match\n",
    "walkscore_sr = arcpy.Describe(walkscore_fishnet).spatialReference\n",
    "neighborhoods_sr = arcpy.Describe(neighborhoods).spatialReference\n",
    "\n",
    "if walkscore_sr.name != neighborhoods_sr.name:\n",
    "    raise ValueError(\"Spatial references do not match between walkscore_fishnet and neighborhoods.\")\n",
    "\n",
    "# Step 2: Intersect fishnet with neighborhoods to split grids at boundaries\n",
    "arcpy.analysis.Intersect([walkscore_fishnet, neighborhoods], fishnet_neighborhoods_intersect)\n",
    "print(\"Intersected walkscore fishnet with neighborhoods to create fragments.\")\n",
    "\n",
    "# Step 3: Verify that the 'nested', 'is_tourist', and 'is_industrial' fields are present in the intersected layer\n",
    "fields = arcpy.ListFields(fishnet_neighborhoods_intersect)\n",
    "field_names = [field.name for field in fields]\n",
    "\n",
    "required_fields = [neighborhood_field, \"is_tourist\", \"is_industrial\"]\n",
    "for field in required_fields:\n",
    "    if field not in field_names:\n",
    "        raise ValueError(f\"Field '{field}' not found in the intersected layer.\")\n",
    "\n",
    "print(f\"Using fields: {required_fields}\")\n",
    "\n",
    "# Optional: Calculate the area of each fragment for further analysis\n",
    "arcpy.management.AddField(fishnet_neighborhoods_intersect, \"Fragment_Area\", \"DOUBLE\")\n",
    "arcpy.management.CalculateGeometryAttributes(fishnet_neighborhoods_intersect, [[\"Fragment_Area\", \"AREA_GEODESIC\"]])\n",
    "print(\"Calculated area for each fragment.\")\n",
    "\n",
    "# Step 4: Use the intersected result directly as the new walkscore_fishnet\n",
    "# Rename the intersected layer to replace the original walkscore_fishnet\n",
    "arcpy.management.Delete(walkscore_fishnet)  # Delete the original fishnet to allow overwriting\n",
    "arcpy.management.Rename(fishnet_neighborhoods_intersect, walkscore_fishnet)\n",
    "print(f\"Overwritten {walkscore_fishnet} with the intersected fragments, retaining neighborhood names.\")\n",
    "\n",
    "# Verify the output\n",
    "print(f\"Fields in {walkscore_fishnet} after processing:\")\n",
    "fields = arcpy.ListFields(walkscore_fishnet)\n",
    "for field in fields:\n",
    "    print(f\"Name: {field.name}, Type: {field.type}\")\n",
    "\n",
    "print(\"Fishnet fragments assigned to neighborhoods and saved back to walkscore_fishnet.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaling MAX Speed Limit for Different Uses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "speed_limit_scalers = {\n",
    "    \"Industrial\": 1.25,\n",
    "    \"ParkingLots\": 1.1,\n",
    "    \"GolfCourse\": 1.1,\n",
    "    \"Cemeteries\": 1.1,\n",
    "    \"Hospitals\": 1.5,\n",
    "#     \"MultiUseTrails\": 0.9,\n",
    "#     \"Parks\": 0.5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add binary fields to walkscore_fishnet_layer for each area type\n",
    "for area_type in speed_limit_scalers.keys():\n",
    "    binary_field = f\"Is{area_type.replace(' ', '')}\"\n",
    "    if not any(f.name == binary_field for f in arcpy.ListFields(walkscore_fishnet_layer)):\n",
    "        arcpy.management.AddField(walkscore_fishnet_layer, binary_field, \"SHORT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary fields updated based on effective area presence.\n"
     ]
    }
   ],
   "source": [
    "# Populate binary fields based on effective area presence\n",
    "for area_type in speed_limit_scalers.keys():\n",
    "    effective_area_field = f\"{area_type}_SUM_{area_type.replace(' ', '')}_effective_area\"\n",
    "    binary_field = f\"Is{area_type.replace(' ', '')}\"\n",
    "    \n",
    "    if any(f.name == effective_area_field for f in arcpy.ListFields(walkscore_fishnet_layer)):\n",
    "        with arcpy.da.UpdateCursor(walkscore_fishnet_layer, [effective_area_field, binary_field]) as cursor:\n",
    "            for row in cursor:\n",
    "                effective_area = row[0] if row[0] is not None else 0\n",
    "                row[1] = 1 if effective_area > 0 else 0\n",
    "                cursor.updateRow(row)\n",
    "\n",
    "print(\"Binary fields updated based on effective area presence.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max_Speed_Limit adjusted based on area type scalers with a ceiling and floor on scaling.\n"
     ]
    }
   ],
   "source": [
    "# Set the maximum and minimum allowable scaler values\n",
    "max_scaler_value = speed_limit_scalers[\"Hospitals\"]  # Maximum allowable scaler value\n",
    "min_scaler_value = 1    # Minimum allowable scaler value\n",
    "\n",
    "# Adjust Max_Speed_Limit using the binary fields and scalers with a cap on the scaling\n",
    "with arcpy.da.UpdateCursor(walkscore_fishnet_layer, [\"Max_Speed_Limit\"] + [f\"Is{area_type.replace(' ', '')}\" for area_type in speed_limit_scalers.keys()]) as cursor:\n",
    "    for row in cursor:\n",
    "        max_speed_limit = row[0]\n",
    "        applied_scaler = 1.0\n",
    "        \n",
    "        # Apply scalers based on binary fields\n",
    "        if max_speed_limit is not None:\n",
    "            for i, area_type in enumerate(speed_limit_scalers.keys(), start=1):\n",
    "                if row[i] == 1:  # If binary field is 1, apply the scaler\n",
    "                    applied_scaler *= speed_limit_scalers[area_type]\n",
    "\n",
    "                    # Ensure applied_scaler does not exceed the max_scaler_value\n",
    "                    if applied_scaler > max_scaler_value:\n",
    "                        applied_scaler = max_scaler_value\n",
    "                        break  # No need to continue if we've hit the max scaler\n",
    "\n",
    "            # Ensure applied_scaler does not fall below the min_scaler_value\n",
    "            if applied_scaler < min_scaler_value:\n",
    "                applied_scaler = min_scaler_value\n",
    "\n",
    "            # Apply the final scaler to the max speed limit\n",
    "            max_speed_limit *= applied_scaler\n",
    "            row[0] = max_speed_limit\n",
    "            cursor.updateRow(row)\n",
    "\n",
    "print(\"Max_Speed_Limit adjusted based on area type scalers with a ceiling and floor on scaling.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Finalizing Walkscore Fishnet\n",
    "\n",
    "Finally, we'll take the fishnet (walkscore_fishnet) and trim the fields down to only the mandatory fields (and permanent fields). This will include the calculation of the amenity density, which allows me to remove the count fields before passing the fishnet to the next notebook for walkscore calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "walkscore_fishnet = f\"{output_gdb}\\\\walkscore_fishnet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "arcpy.env.overwriteOutput = True  # Allow outputs to be overwritten\n",
    "\n",
    "# Define the input layers\n",
    "neighborhoods_layer = r\"C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\neighborhoods\"\n",
    "fishnet_layer = r\"C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\walkscore_fishnet\"\n",
    "tree_canopy_layer = r\"C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\TreeCanopy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_normalize(value, values):\n",
    "    sorted_values = sorted(values)\n",
    "    rank = sorted_values.index(value) + 1\n",
    "    return rank / len(values) * 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculation of Crime Density Using Kernel Density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clipped points for SPD_Crime_Data using the barrier layer.\n",
      "Generated kernel density heatmap for SPD_Crime_Data using clipped features.\n",
      "Applied focal statistics to smooth the kernel density heatmap for SPD_Crime_Data.\n",
      "Average density calculated and saved to C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\SPD_Crime_Data_zonal_stats.\n",
      "Added new field 'crime_density'.\n",
      "Deleted temporary field 'MEAN' after updating 'crime_density'.\n",
      "Updated 'crime_density' field with mean density values for SPD_Crime_Data.\n",
      "Crime density processing complete.\n"
     ]
    }
   ],
   "source": [
    "# Define the path to the neighborhood layer that will be used as a barrier\n",
    "barrier_layer = \"citylimits\"\n",
    "\n",
    "# Input layer for crime points (e.g., \"crime_data_points\")\n",
    "crime_layer_name = \"SPD_Crime_Data\"\n",
    "input_layer = f\"{base_layers_group}\\\\{crime_layer_name}\"\n",
    "\n",
    "# Check if the input layer exists\n",
    "if not arcpy.Exists(input_layer):\n",
    "    print(f\"Layer {input_layer} does not exist. Skipping.\")\n",
    "else:\n",
    "    # Step 1: Clip the Crime Points by the Barrier Layer\n",
    "    clipped_crime_points = f\"{workspace}\\\\{crime_layer_name}_clipped\"\n",
    "    arcpy.analysis.Clip(\n",
    "        in_features=input_layer,\n",
    "        clip_features=barrier_layer,\n",
    "        out_feature_class=clipped_crime_points\n",
    "    )\n",
    "    print(f\"Clipped points for {crime_layer_name} using the barrier layer.\")\n",
    "\n",
    "    # Step 2: Apply Kernel Density Tool to Generate Crime Density Raster\n",
    "    kernel_density_output = f\"{workspace}\\\\{crime_layer_name}_kernel_density\"\n",
    "    arcpy.sa.KernelDensity(\n",
    "        in_features=clipped_crime_points,\n",
    "        population_field=\"NONE\",  # Assuming the point occurrences are counts, so no separate population field\n",
    "        out_cell_values=\"DENSITIES\",\n",
    "        method='GEODESIC',\n",
    "        cell_size=\"7.45462399999951E-04\",  # Match cell size to your fishnet grid resolution\n",
    "        search_radius=\"500\",  # Adjust based on the intended influence\n",
    "        area_unit_scale_factor=\"SQUARE_FEET\"\n",
    "    ).save(kernel_density_output)\n",
    "    print(f\"Generated kernel density heatmap for {crime_layer_name} using clipped features.\")\n",
    "\n",
    "    # Step 3: Apply Smoothing Using Focal Statistics (optional)\n",
    "    smoothed_kernel_density_output = f\"{workspace}\\\\smoothed_crime_kernel_density\"\n",
    "    smoothed_crime_raster = arcpy.sa.FocalStatistics(\n",
    "        in_raster=kernel_density_output,\n",
    "        neighborhood=arcpy.sa.NbrCircle(radius=3, units=\"CELL\"),  # Circular neighborhood with radius of 3 cells\n",
    "        statistics_type=\"MEAN\"\n",
    "    )\n",
    "    smoothed_crime_raster.save(smoothed_kernel_density_output)\n",
    "    print(f\"Applied focal statistics to smooth the kernel density heatmap for {crime_layer_name}.\")\n",
    "\n",
    "    # Step 4: Calculate Average Crime Density for Each Fishnet Grid Using Zonal Statistics\n",
    "    zonal_output_table = f\"{workspace}\\\\{crime_layer_name}_zonal_stats\"\n",
    "    calculate_average_density(walkscore_fishnet_layer, smoothed_kernel_density_output, walkscore_fishnet_layer, zonal_output_table)\n",
    "\n",
    "    # Step 5: Join Zonal Statistics Back to Fishnet Grid\n",
    "    crime_density_field = \"crime_density\"\n",
    "\n",
    "    # Delete the existing \"crime_density\" field if it already exists\n",
    "    if any(f.name == crime_density_field for f in arcpy.ListFields(walkscore_fishnet_layer)):\n",
    "        arcpy.management.DeleteField(walkscore_fishnet_layer, crime_density_field)\n",
    "        print(f\"Deleted existing field '{crime_density_field}'.\")\n",
    "\n",
    "    # Add the new \"crime_density\" field\n",
    "    arcpy.management.AddField(walkscore_fishnet_layer, crime_density_field, \"DOUBLE\")\n",
    "    print(f\"Added new field '{crime_density_field}'.\")\n",
    "\n",
    "    # Join the mean values from the zonal stats output back to the fishnet\n",
    "    arcpy.management.JoinField(\n",
    "        in_data=walkscore_fishnet_layer,\n",
    "        in_field=\"IndexID\",\n",
    "        join_table=zonal_output_table,\n",
    "        join_field=\"IndexID\",\n",
    "        fields=[\"MEAN\"]\n",
    "    )\n",
    "\n",
    "    # Update the \"crime_density\" field using the \"MEAN\" field from the joined table, setting nulls to 0\n",
    "    with arcpy.da.UpdateCursor(walkscore_fishnet_layer, [\"MEAN\", crime_density_field]) as cursor:\n",
    "        for row in cursor:\n",
    "            row[1] = row[0] if row[0] is not None else 0  # Copy the value from the \"MEAN\" field or set to 0 if None\n",
    "            cursor.updateRow(row)\n",
    "\n",
    "    # Delete the \"MEAN\" field after copying values\n",
    "    arcpy.management.DeleteField(walkscore_fishnet_layer, \"MEAN\")\n",
    "    print(f\"Deleted temporary field 'MEAN' after updating 'crime_density'.\")\n",
    "\n",
    "    print(f\"Updated 'crime_density' field with mean density values for {crime_layer_name}.\")\n",
    "\n",
    "print(\"Crime density processing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 'crime_density_normalized' field to 'C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\walkscore_fishnet'.\n",
      "Normalized 'crime_density' and updated 'crime_density_normalized' on a scale of 0-5 for each fishnet grid.\n"
     ]
    }
   ],
   "source": [
    "# Simple Normalization of Crime Density to a Range of 0-5\n",
    "crime_density_field = \"crime_density\"\n",
    "normalized_field = \"crime_density_normalized\"\n",
    "\n",
    "# Ensure the normalized field does not already exist\n",
    "if not any(f.name == normalized_field for f in arcpy.ListFields(walkscore_fishnet_layer)):\n",
    "    arcpy.management.AddField(walkscore_fishnet_layer, normalized_field, \"DOUBLE\")\n",
    "    print(f\"Added '{normalized_field}' field to '{walkscore_fishnet_layer}'.\")\n",
    "\n",
    "# Step 1: Fetch all crime density values to calculate min and max\n",
    "crime_density_values = []\n",
    "with arcpy.da.SearchCursor(walkscore_fishnet_layer, [crime_density_field]) as cursor:\n",
    "    for row in cursor:\n",
    "        if row[0] is not None:\n",
    "            crime_density_values.append(row[0])\n",
    "\n",
    "# Calculate the min and max values\n",
    "min_value = min(crime_density_values) if crime_density_values else 0\n",
    "max_value = max(crime_density_values) if crime_density_values else 1  # Avoid division by zero\n",
    "\n",
    "# Step 2: Update the normalized field in the fishnet grid\n",
    "with arcpy.da.UpdateCursor(walkscore_fishnet_layer, [crime_density_field, normalized_field]) as cursor:\n",
    "    for row in cursor:\n",
    "        if row[0] is not None:\n",
    "            # Apply min-max normalization to scale between 0-5\n",
    "            if max_value > min_value:\n",
    "                row[1] = ((row[0] - min_value) / (max_value - min_value)) * 5\n",
    "            else:\n",
    "                row[1] = 0  # In case all values are the same, set normalized value to 0\n",
    "        else:\n",
    "            row[1] = 0  # Set to 0 for null or zero crime density\n",
    "        cursor.updateRow(row)\n",
    "\n",
    "print(f\"Normalized '{crime_density_field}' and updated '{normalized_field}' on a scale of 0-5 for each fishnet grid.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Crash Density Calculation for Entire Fishnet Grid using Kernel Density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clipped points for crashes using the barrier layer.\n",
      "Generated kernel density heatmap for crashes using clipped features.\n",
      "Applied focal statistics to smooth the kernel density heatmap for crashes.\n",
      "Average density calculated and saved to C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\crashes_zonal_stats.\n",
      "Added new field 'crash_density'.\n",
      "Deleted temporary field 'MEAN' after updating 'crash_density'.\n",
      "Updated 'crash_density' field with mean density values for crashes.\n",
      "Crash density processing complete.\n"
     ]
    }
   ],
   "source": [
    "# Define the path to the neighborhood layer that will be used as a barrier\n",
    "barrier_layer = \"citylimits\"\n",
    "\n",
    "# Input layer for crash points (e.g., \"crash_data_points\")\n",
    "crash_layer_name = \"crashes\"\n",
    "input_layer = f\"{base_layers_group}\\\\{crash_layer_name}\"\n",
    "\n",
    "# Check if the input layer exists\n",
    "if not arcpy.Exists(input_layer):\n",
    "    print(f\"Layer {input_layer} does not exist. Skipping.\")\n",
    "else:\n",
    "    # Step 1: Clip the Crash Points by the Barrier Layer\n",
    "    clipped_crash_points = f\"{workspace}\\\\{crash_layer_name}_clipped\"\n",
    "    arcpy.analysis.Clip(\n",
    "        in_features=input_layer,\n",
    "        clip_features=barrier_layer,\n",
    "        out_feature_class=clipped_crash_points\n",
    "    )\n",
    "    print(f\"Clipped points for {crash_layer_name} using the barrier layer.\")\n",
    "\n",
    "    # Step 2: Apply Kernel Density Tool to Generate Crash Density Raster\n",
    "    kernel_density_output = f\"{workspace}\\\\{crash_layer_name}_kernel_density\"\n",
    "    arcpy.sa.KernelDensity(\n",
    "        in_features=clipped_crash_points,\n",
    "        population_field=\"FATALITIES\",\n",
    "        out_cell_values=\"DENSITIES\",\n",
    "        method='GEODESIC',\n",
    "        cell_size=\"7.45462399999951E-04\",  # Match cell size to your fishnet grid resolution\n",
    "        search_radius=\"500\",  # Adjust based on the intended influence\n",
    "        area_unit_scale_factor=\"SQUARE_FEET\"\n",
    "    ).save(kernel_density_output)\n",
    "    print(f\"Generated kernel density heatmap for {crash_layer_name} using clipped features.\")\n",
    "\n",
    "    # Step 3: Apply Smoothing Using Focal Statistics (optional)\n",
    "    smoothed_kernel_density_output = f\"{workspace}\\\\smoothed_crash_kernel_density\"\n",
    "    smoothed_crash_raster = arcpy.sa.FocalStatistics(\n",
    "        in_raster=kernel_density_output,\n",
    "        neighborhood=arcpy.sa.NbrCircle(radius=3, units=\"CELL\"),  # Circular neighborhood with radius of 3 cells\n",
    "        statistics_type=\"MEAN\"\n",
    "    )\n",
    "    smoothed_crash_raster.save(smoothed_kernel_density_output)\n",
    "    print(f\"Applied focal statistics to smooth the kernel density heatmap for {crash_layer_name}.\")\n",
    "\n",
    "    # Step 4: Calculate Average Crash Density for Each Fishnet Grid Using Zonal Statistics\n",
    "    zonal_output_table = f\"{workspace}\\\\{crash_layer_name}_zonal_stats\"\n",
    "    calculate_average_density(walkscore_fishnet_layer, smoothed_kernel_density_output, walkscore_fishnet_layer, zonal_output_table)\n",
    "\n",
    "    # Step 5: Join Zonal Statistics Back to Fishnet Grid\n",
    "    crash_density_field = \"crash_density\"\n",
    "\n",
    "    # Delete the existing \"crash_density\" field if it already exists\n",
    "    if any(f.name == crash_density_field for f in arcpy.ListFields(walkscore_fishnet_layer)):\n",
    "        arcpy.management.DeleteField(walkscore_fishnet_layer, crash_density_field)\n",
    "        print(f\"Deleted existing field '{crash_density_field}'.\")\n",
    "\n",
    "    # Add the new \"crash_density\" field\n",
    "    arcpy.management.AddField(walkscore_fishnet_layer, crash_density_field, \"DOUBLE\")\n",
    "    print(f\"Added new field '{crash_density_field}'.\")\n",
    "\n",
    "    # Join the mean values from the zonal stats output back to the fishnet\n",
    "    arcpy.management.JoinField(\n",
    "        in_data=walkscore_fishnet_layer,\n",
    "        in_field=\"IndexID\",\n",
    "        join_table=zonal_output_table,\n",
    "        join_field=\"IndexID\",\n",
    "        fields=[\"MEAN\"]\n",
    "    )\n",
    "\n",
    "    # Update the \"crash_density\" field using the \"MEAN\" field from the joined table, setting nulls to 0\n",
    "    with arcpy.da.UpdateCursor(walkscore_fishnet_layer, [\"MEAN\", crash_density_field]) as cursor:\n",
    "        for row in cursor:\n",
    "            row[1] = row[0] if row[0] is not None else 0  # Copy the value from the \"MEAN\" field or set to 0 if None\n",
    "            cursor.updateRow(row)\n",
    "\n",
    "    # Delete the \"MEAN\" field after copying values\n",
    "    arcpy.management.DeleteField(walkscore_fishnet_layer, \"MEAN\")\n",
    "    print(f\"Deleted temporary field 'MEAN' after updating 'crash_density'.\")\n",
    "\n",
    "    print(f\"Updated 'crash_density' field with mean density values for {crash_layer_name}.\")\n",
    "\n",
    "print(\"Crash density processing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 'crash_density_normalized' field to 'C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\walkscore_fishnet'.\n",
      "Normalized 'crash_density' and updated 'crash_density_normalized' on a scale of 0-5 for each fishnet grid.\n"
     ]
    }
   ],
   "source": [
    "# Simple Normalization of Crash Density to a Range of 0-5\n",
    "crash_density_field = \"crash_density\"\n",
    "normalized_field = \"crash_density_normalized\"\n",
    "\n",
    "# Ensure the normalized field does not already exist\n",
    "if not any(f.name == normalized_field for f in arcpy.ListFields(walkscore_fishnet_layer)):\n",
    "    arcpy.management.AddField(walkscore_fishnet_layer, normalized_field, \"DOUBLE\")\n",
    "    print(f\"Added '{normalized_field}' field to '{walkscore_fishnet_layer}'.\")\n",
    "\n",
    "# Step 6: Fetch all crash density values to calculate min and max\n",
    "crash_density_values = []\n",
    "with arcpy.da.SearchCursor(walkscore_fishnet_layer, [crash_density_field]) as cursor:\n",
    "    for row in cursor:\n",
    "        if row[0] is not None:\n",
    "            crash_density_values.append(row[0])\n",
    "\n",
    "# Calculate the min and max values\n",
    "min_value = min(crash_density_values) if crash_density_values else 0\n",
    "max_value = max(crash_density_values) if crash_density_values else 1  # Avoid division by zero\n",
    "\n",
    "# Step 7: Update the normalized field in the fishnet grid\n",
    "with arcpy.da.UpdateCursor(walkscore_fishnet_layer, [crash_density_field, normalized_field]) as cursor:\n",
    "    for row in cursor:\n",
    "        if row[0] is not None:\n",
    "            # Apply min-max normalization to scale between 0-5\n",
    "            if max_value > min_value:\n",
    "                row[1] = ((row[0] - min_value) / (max_value - min_value)) * 5\n",
    "            else:\n",
    "                row[1] = 0  # In case all values are the same, set normalized value to 0\n",
    "        else:\n",
    "            row[1] = 0  # Set to 0 for null or zero crash density\n",
    "        cursor.updateRow(row)\n",
    "\n",
    "print(f\"Normalized '{crash_density_field}' and updated '{normalized_field}' on a scale of 0-5 for each fishnet grid.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fields in C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\walkscore_fishnet after dropping specified fields:\n",
      "Name: OBJECTID, Type: OID\n",
      "Name: Shape, Type: Geometry\n",
      "Name: FID_walkscore_fishnet, Type: Integer\n",
      "Name: IndexID, Type: Integer\n",
      "Name: total_area, Type: Double\n",
      "Name: Max_Speed_Limit, Type: Double\n",
      "Name: SUM_proportional_population, Type: Double\n",
      "Name: Grid_Slope_MEAN, Type: Double\n",
      "Name: Sidewalks_Slope_Mean, Type: Double\n",
      "Name: Streets_Slope_Mean, Type: Double\n",
      "Name: MultiUseTrails_Slope_Mean, Type: Double\n",
      "Name: trails_Slope_Mean, Type: Double\n",
      "Name: effective_slope, Type: Double\n",
      "Name: business_density, Type: Double\n",
      "Name: Industrial_SUM_Industrial_effective_area, Type: Double\n",
      "Name: ParkingLots_SUM_ParkingLots_effective_area, Type: Double\n",
      "Name: GolfCourse_SUM_GolfCourse_effective_area, Type: Double\n",
      "Name: Cemeteries_SUM_Cemeteries_effective_area, Type: Double\n",
      "Name: Hospitals_SUM_Hospitals_effective_area, Type: Double\n",
      "Name: Slope_MEAN, Type: Double\n",
      "Name: Bike_greenways_SUM_Bike_greenways_area, Type: Double\n",
      "Name: Bike_protected_SUM_Bike_protected_area, Type: Double\n",
      "Name: Bike_buffer_SUM_Bike_buffer_area, Type: Double\n",
      "Name: Healthy_Streets_SUM_Healthy_Streets_area, Type: Double\n",
      "Name: Parks_SUM_Parks_area, Type: Double\n",
      "Name: Universities_SUM_Universities_area, Type: Double\n",
      "Name: Sidewalks_SUM_Sidewalks_area, Type: Double\n",
      "Name: Plaza_SUM_Plaza_area, Type: Double\n",
      "Name: trails_SUM_trails_area, Type: Double\n",
      "Name: MultiUseTrails_SUM_MultiUseTrails_area, Type: Double\n",
      "Name: Streets_SUM_Streets_effective_area, Type: Double\n",
      "Name: population_SUM_proportional_population, Type: Double\n",
      "Name: SPD_Crime_Data_COUNT_Offense_ID, Type: Double\n",
      "Name: FID_neighborhoods, Type: Integer\n",
      "Name: city, Type: String\n",
      "Name: nested, Type: String\n",
      "Name: neighborhood_area, Type: Double\n",
      "Name: is_tourist, Type: Integer\n",
      "Name: latitude, Type: Double\n",
      "Name: longitude, Type: Double\n",
      "Name: is_industrial, Type: Integer\n",
      "Name: Shape_Length, Type: Double\n",
      "Name: Shape_Area, Type: Double\n",
      "Name: Fragment_Area, Type: Double\n",
      "Name: IsIndustrial, Type: SmallInteger\n",
      "Name: IsParkingLots, Type: SmallInteger\n",
      "Name: IsGolfCourse, Type: SmallInteger\n",
      "Name: IsCemeteries, Type: SmallInteger\n",
      "Name: IsHospitals, Type: SmallInteger\n",
      "Name: crime_density, Type: Double\n",
      "Name: crime_density_normalized, Type: Double\n",
      "Name: crash_density, Type: Double\n",
      "Name: crash_density_normalized, Type: Double\n",
      "Fields dropped successfully.\n"
     ]
    }
   ],
   "source": [
    "# Drop specified fields\n",
    "fields_to_drop = []\n",
    "for field in arcpy.ListFields(walkscore_fishnet):\n",
    "    if field.name.endswith(\"FREQUENCY\") or field.name.endswith(\"_OBJECTID\") or field.name.endswith(\"Slope_AREA\") or field.name.endswith(\"Slope_COUNT\") or field.name.endswith(\"IndexID_1\") or field.name.endswith(\"_id\"):\n",
    "        fields_to_drop.append(field.name)\n",
    "\n",
    "if fields_to_drop:\n",
    "    arcpy.management.DeleteField(walkscore_fishnet, fields_to_drop)\n",
    "\n",
    "# Verify fields in walkscore_fishnet after dropping specified fields\n",
    "walkscore_fishnet = f\"{output_gdb}\\\\walkscore_fishnet\"\n",
    "print(f\"\\nFields in {walkscore_fishnet} after dropping specified fields:\")\n",
    "fields = arcpy.ListFields(walkscore_fishnet)\n",
    "for field in fields:\n",
    "    print(f\"Name: {field.name}, Type: {field.type}\")\n",
    "\n",
    "print(\"Fields dropped successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Cleaning Contents Pane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_created_layers(layers_list):\n",
    "    for layer in layers_list:\n",
    "        if arcpy.Exists(layer):\n",
    "            arcpy.management.Delete(layer)\n",
    "            print(f\"Deleted layer: {layer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_layers = [\n",
    "    \"Industrial\",\n",
    "    \"ParkingLots\",\n",
    "    \"GolfCourse\",\n",
    "    \"Cemeteries\",\n",
    "    \"Hospitals\",\n",
    "#     \"Slope\",\n",
    "    \"Bike_greenways\",\n",
    "    \"Bike_protected\",\n",
    "    \"Bike_buffer\",\n",
    "    \"Healthy_Streets\",\n",
    "    \"Parks\",\n",
    "    \"Universities\",\n",
    "    \"Sidewalks\",\n",
    "    \"Plaza\",\n",
    "    \"trails\",\n",
    "    \"MultiUseTrails\",\n",
    "    \"Streets\",\n",
    "    \"fishnet_clipped\",\n",
    "    \"Marked_Crosswalks\",\n",
    "    \"fishnet_clipped\",\n",
    "    \"neighborhoods\",\n",
    "    \"population\",\n",
    "    \"crashes\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing layer: Industrial\n",
      "C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\Industrial is already in the target spatial reference.\n",
      "Processing layer: ParkingLots\n",
      "C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\ParkingLots is already in the target spatial reference.\n",
      "Processing layer: GolfCourse\n",
      "C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\GolfCourse is already in the target spatial reference.\n",
      "Processing layer: Cemeteries\n",
      "C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\Cemeteries is already in the target spatial reference.\n",
      "Processing layer: Hospitals\n",
      "C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\Hospitals is already in the target spatial reference.\n",
      "Processing layer: Bike_greenways\n",
      "C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\Bike_greenways is already in the target spatial reference.\n",
      "Processing layer: Bike_protected\n",
      "C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\Bike_protected is already in the target spatial reference.\n",
      "Processing layer: Bike_buffer\n",
      "C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\Bike_buffer is already in the target spatial reference.\n",
      "Processing layer: Healthy_Streets\n",
      "C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\Healthy_Streets is already in the target spatial reference.\n",
      "Processing layer: Parks\n",
      "C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\Parks is already in the target spatial reference.\n",
      "Processing layer: Universities\n",
      "C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\Universities is already in the target spatial reference.\n",
      "Processing layer: Sidewalks\n",
      "C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\Sidewalks is already in the target spatial reference.\n",
      "Processing layer: Plaza\n",
      "C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\Plaza is already in the target spatial reference.\n",
      "Processing layer: trails\n",
      "C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\trails is already in the target spatial reference.\n",
      "Processing layer: MultiUseTrails\n",
      "C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\MultiUseTrails is already in the target spatial reference.\n",
      "Processing layer: Streets\n",
      "C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\Streets is already in the target spatial reference.\n",
      "Processing layer: fishnet_clipped\n",
      "C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\fishnet_clipped is already in the target spatial reference.\n",
      "Processing layer: Marked_Crosswalks\n",
      "C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\Marked_Crosswalks is already in the target spatial reference.\n",
      "Processing layer: fishnet_clipped\n",
      "C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\fishnet_clipped is already in the target spatial reference.\n",
      "Processing layer: neighborhoods\n",
      "C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\neighborhoods is already in the target spatial reference.\n",
      "Processing layer: population\n",
      "C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\population is already in the target spatial reference.\n",
      "Processing layer: crashes\n",
      "Projected C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\crashes to C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\scratch.gdb\\crashes_proj.\n",
      "Replaced original C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\crashes with projected version.\n",
      "All layers have been projected to the target spatial reference.\n"
     ]
    }
   ],
   "source": [
    "base_layers_group = r\"C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\"\n",
    "target_spatial_reference = arcpy.SpatialReference(3857)  # WGS 1984 Web Mercator (auxiliary sphere)\n",
    "\n",
    "def project_layer(input_layer, target_sr):\n",
    "    input_layer_sr = arcpy.Describe(input_layer).spatialReference\n",
    "    \n",
    "    if input_layer_sr.name != target_sr.name:\n",
    "        temp_projected_layer = os.path.join(arcpy.env.scratchGDB, f\"{os.path.basename(input_layer)}_proj\")\n",
    "        arcpy.management.Project(input_layer, temp_projected_layer, target_sr)\n",
    "        print(f\"Projected {input_layer} to {temp_projected_layer}.\")\n",
    "        \n",
    "        # Overwrite the original layer with the projected version\n",
    "        arcpy.management.Delete(input_layer)\n",
    "        arcpy.management.CopyFeatures(temp_projected_layer, input_layer)\n",
    "        arcpy.management.Delete(temp_projected_layer)\n",
    "        print(f\"Replaced original {input_layer} with projected version.\")\n",
    "    else:\n",
    "        print(f\"{input_layer} is already in the target spatial reference.\")\n",
    "\n",
    "# Process each base layer\n",
    "for layer_name in base_layers:\n",
    "    print(f\"Processing layer: {layer_name}\")  # Debugging statement\n",
    "\n",
    "    # Access the layer\n",
    "    input_layer = f\"{base_layers_group}\\\\{layer_name}\"\n",
    "    \n",
    "    # Verify if the input_layer exists\n",
    "    if not arcpy.Exists(input_layer):\n",
    "        print(f\"Layer {input_layer} does not exist. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    # Project the input layer to the target spatial reference\n",
    "    project_layer(input_layer, target_spatial_reference)\n",
    "\n",
    "print(\"All layers have been projected to the target spatial reference.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "file_extension": ".py",
   "name": "python",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
