{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PreProcessing\n",
    "\n",
    "Now that we have the base layers organized into a base_layers group in the contents frame, we'll use python to iterate through each layer, preparing them to be used in the calculation of the walkscore in the next notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arcpy\n",
    "import arcpy.mp\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import os\n",
    "import numpy as np\n",
    "from arcpy.sa import Int\n",
    "\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0: Formatting Fishnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "fishnet_layer = r\"C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\fishnet_clipped\"\n",
    "base_layers_group = r\"C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\"\n",
    "output_gdb = r\"C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\"\n",
    "geodatabase_path = r\"C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\"\n",
    "fishnet_clipped = r\"C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\fishnet_clipped\"\n",
    "\n",
    "arcpy.env.workspace = r\"C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\"\n",
    "fishnet_area_field = \"total_area\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = r\"C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\output\"\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Field Name: OBJECTID, Field Type: OID\n",
      "Field Name: Shape, Field Type: Geometry\n",
      "Field Name: Shape_Length, Field Type: Double\n",
      "Field Name: Shape_Area, Field Type: Double\n",
      "Field Name: IndexID, Field Type: Integer\n"
     ]
    }
   ],
   "source": [
    "fields = arcpy.ListFields(fishnet_layer)\n",
    "for field in fields:\n",
    "    print(f\"Field Name: {field.name}, Field Type: {field.type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IndexID field created and populated in fishnet_clipped.\n"
     ]
    }
   ],
   "source": [
    "# Add the IndexID field if it doesn't exist\n",
    "index_field = \"IndexID\"\n",
    "if not any(f.name == index_field for f in arcpy.ListFields(fishnet_clipped)):\n",
    "    arcpy.management.AddField(fishnet_clipped, index_field, \"LONG\")\n",
    "\n",
    "# Populate the IndexID field with unique values\n",
    "with arcpy.da.UpdateCursor(fishnet_clipped, [index_field]) as cursor:\n",
    "    for i, row in enumerate(cursor):\n",
    "        row[0] = i + 1\n",
    "        cursor.updateRow(row)\n",
    "\n",
    "print(\"IndexID field created and populated in fishnet_clipped.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mandatory Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_layers = [\n",
    "#     \"TreeCanopy\",\n",
    "#     \"Public_Amenities\",\n",
    "    \"Business_Amenities\",\n",
    "    \"Industrial\",\n",
    "    \"ParkingLots\",\n",
    "    \"GolfCourse\",\n",
    "    \"Cemeteries\",\n",
    "    \"Hospitals\",\n",
    "    \"Slope\",\n",
    "    \"Bike_greenways\",\n",
    "    \"Bike_protected\",\n",
    "    \"Bike_buffer\",\n",
    "    \"Healthy_Streets\",\n",
    "    \"Parks\",\n",
    "    \"Universities\",\n",
    "    \"Sidewalks\",\n",
    "    \"Plaza\",\n",
    "    \"trails\",\n",
    "    \"MultiUseTrails\",\n",
    "    \"Streets\",\n",
    "    \"population\",\n",
    "    \"SPD_Crime_Data\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for layer: C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\Business_Amenities\n",
      "Checking for layer: C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\Industrial\n",
      "Checking for layer: C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\ParkingLots\n",
      "Checking for layer: C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\GolfCourse\n",
      "Checking for layer: C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\Cemeteries\n",
      "Checking for layer: C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\Hospitals\n",
      "Checking for layer: C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\Slope\n",
      "Checking for layer: C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\Bike_greenways\n",
      "Checking for layer: C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\Bike_protected\n",
      "Checking for layer: C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\Bike_buffer\n",
      "Checking for layer: C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\Healthy_Streets\n",
      "Checking for layer: C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\Parks\n",
      "Checking for layer: C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\Universities\n",
      "Checking for layer: C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\Sidewalks\n",
      "Checking for layer: C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\Plaza\n",
      "Checking for layer: C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\trails\n",
      "Checking for layer: C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\MultiUseTrails\n",
      "Checking for layer: C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\Streets\n",
      "Checking for layer: C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\population\n",
      "Checking for layer: C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\SPD_Crime_Data\n"
     ]
    }
   ],
   "source": [
    "for layer_name in base_layers:\n",
    "    input_layer = f\"{base_layers_group}\\\\{layer_name}\"\n",
    "    print(f\"Checking for layer: {input_layer}\")  # Add this line\n",
    "    if not arcpy.Exists(input_layer):\n",
    "        print(f\"Layer {input_layer} does not exist. Skipping.\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_average_slope(fishnet_layer, slope_raster, features_layer, output_table):\n",
    "    arcpy.CheckOutExtension(\"Spatial\")\n",
    "    extracted_slope = arcpy.sa.ExtractByMask(slope_raster, features_layer)\n",
    "    temp_extracted_slope = f\"{output_gdb}\\\\temp_extracted_slope\"\n",
    "    extracted_slope.save(temp_extracted_slope)\n",
    "    arcpy.sa.ZonalStatisticsAsTable(fishnet_layer, \"IndexID\", temp_extracted_slope, output_table, \"NODATA\", \"MEAN\")\n",
    "    arcpy.management.Delete(temp_extracted_slope)\n",
    "\n",
    "def integrate_slope_and_area(intersect_output, slope_output_table, area_field, effective_slope_field):\n",
    "    arcpy.management.AddField(intersect_output, effective_slope_field, \"DOUBLE\")\n",
    "    slope_df = arcpy.da.TableToNumPyArray(slope_output_table, [\"IndexID\", \"MEAN\"])\n",
    "    slope_df = pd.DataFrame(slope_df)\n",
    "    with arcpy.da.UpdateCursor(intersect_output, [\"IndexID\", area_field, effective_slope_field]) as cursor:\n",
    "        for row in cursor:\n",
    "            mean_slope = slope_df.loc[slope_df[\"IndexID\"] == row[0], \"MEAN\"]\n",
    "            row[2] = (mean_slope.values[0] * row[1]) if not mean_slope.empty and row[1] is not None else 0\n",
    "            cursor.updateRow(row)\n",
    "\n",
    "def calculate_polygon_area(layer, area_field):\n",
    "    if not any(f.name.lower() == area_field.lower() for f in arcpy.ListFields(layer)):\n",
    "        arcpy.management.AddField(layer, area_field, \"DOUBLE\")\n",
    "    arcpy.management.CalculateGeometryAttributes(layer, [[area_field, \"AREA_GEODESIC\"]], area_unit=\"SQUARE_FEET_US\")\n",
    "\n",
    "def calculate_polyline_area_with_recalculated_length(layer, area_field, width_field):\n",
    "    recalculated_length_field = f\"{area_field}_len\"\n",
    "    if not any(f.name.lower() == recalculated_length_field.lower() for f in arcpy.ListFields(layer)):\n",
    "        arcpy.management.AddField(layer, recalculated_length_field, \"DOUBLE\")\n",
    "    arcpy.management.CalculateGeometryAttributes(layer, [[recalculated_length_field, \"LENGTH_GEODESIC\"]], length_unit=\"FEET_US\")\n",
    "    if not any(f.name.lower() == area_field.lower() for f in arcpy.ListFields(layer)):\n",
    "        arcpy.management.AddField(layer, area_field, \"DOUBLE\")\n",
    "    with arcpy.da.UpdateCursor(layer, [width_field, recalculated_length_field, area_field]) as cursor:\n",
    "        for row in cursor:\n",
    "            row[2] = row[0] * row[1] if row[0] is not None and row[1] is not None else 0\n",
    "            cursor.updateRow(row)\n",
    "\n",
    "def calculate_effective_area(layer, effective_area_field, length_field=\"Shape_Length\", width_field=\"street_width\", speed_limit_field=\"SPEEDLIMIT\", at_grade_field=\"at_grade_scalar\"):\n",
    "    recalculated_length_field = f\"{effective_area_field}_len\"\n",
    "    if not any(f.name.lower() == recalculated_length_field.lower() for f in arcpy.ListFields(layer)):\n",
    "        arcpy.management.AddField(layer, recalculated_length_field, \"DOUBLE\")\n",
    "    arcpy.management.CalculateGeometryAttributes(layer, [[recalculated_length_field, \"LENGTH_GEODESIC\"]], length_unit=\"FEET_US\")\n",
    "\n",
    "    if not any(f.name == effective_area_field for f in arcpy.ListFields(layer)):\n",
    "        arcpy.management.AddField(layer, effective_area_field, \"DOUBLE\")\n",
    "\n",
    "    with arcpy.da.UpdateCursor(layer, [recalculated_length_field, width_field, speed_limit_field, at_grade_field, effective_area_field]) as cursor:\n",
    "        for row in cursor:\n",
    "            if row[0] is not None and row[1] is not None and row[2] is not None and row[3] is not None:\n",
    "                row[4] = row[0] * row[1] * row[2] * row[3]\n",
    "            else:\n",
    "                row[4] = None\n",
    "            cursor.updateRow(row)\n",
    "    print(f\"Calculated effective area for {layer} and stored in {effective_area_field}.\")\n",
    "    \n",
    "def create_layer(input_layer, fclass_list, output_layer_name):\n",
    "    # Create a query to filter the input layer based on fclass values\n",
    "    fclass_query = f\"\"\"fclass IN ({','.join([f\"'{fc}'\" for fc in fclass_list])})\"\"\"\n",
    "    \n",
    "    # Create the output layer\n",
    "    arcpy.management.MakeFeatureLayer(input_layer, \"temp_layer\", fclass_query)\n",
    "    output_layer = f\"{workspace}\\\\{output_layer_name}\"\n",
    "    \n",
    "    # Check if the output layer already exists and delete it if it does\n",
    "    if arcpy.Exists(output_layer):\n",
    "        arcpy.management.Delete(output_layer)\n",
    "    \n",
    "    # Save the filtered features to a new feature class\n",
    "    arcpy.management.CopyFeatures(\"temp_layer\", output_layer)\n",
    "    print(f\"Created {output_layer_name} layer with {len(fclass_list)} fclass values.\")\n",
    "    \n",
    "def calculate_counts(input_layer, intersect_output, fishnet_layer, summary_output, id_field):\n",
    "    # Intersect the input crime data layer with the fishnet\n",
    "    arcpy.analysis.Intersect([input_layer, fishnet_layer], intersect_output)\n",
    "\n",
    "    # Add IndexID to intersect output if it doesn't exist\n",
    "    if not any(f.name == \"IndexID\" for f in arcpy.ListFields(intersect_output)):\n",
    "        arcpy.management.AddField(intersect_output, \"IndexID\", \"LONG\")\n",
    "        # Populate the IndexID field in intersect output\n",
    "        with arcpy.da.UpdateCursor(intersect_output, [\"IndexID\"]) as cursor:\n",
    "            for i, row in enumerate(cursor):\n",
    "                row[0] = i + 1\n",
    "                cursor.updateRow(row)\n",
    "\n",
    "    # Calculate the count of crimes within each fishnet grid cell\n",
    "    arcpy.analysis.Statistics(intersect_output, summary_output, [[id_field, \"COUNT\"]], \"IndexID\")\n",
    "\n",
    "    # Join the summary table back to the fishnet layer\n",
    "    count_field = f\"COUNT_{id_field}\"\n",
    "    arcpy.management.JoinField(fishnet_layer, \"IndexID\", summary_output, \"IndexID\", [count_field])\n",
    "\n",
    "    # Update null values in the joined count field to 0\n",
    "    with arcpy.da.UpdateCursor(fishnet_layer, [count_field]) as cursor:\n",
    "        for row in cursor:\n",
    "            if row[0] is None:\n",
    "                row[0] = 0  # Set null counts to 0\n",
    "            cursor.updateRow(row)\n",
    "\n",
    "    print(f\"Joined {count_field} to {fishnet_layer} and created summary table {summary_output}.\")\n",
    "    \n",
    "def calculate_crime_density(input_layer, intersect_output, fishnet_layer, summary_output, id_field):\n",
    "    # Intersect the input crime data layer with the fishnet\n",
    "    arcpy.analysis.Intersect([input_layer, fishnet_layer], intersect_output)\n",
    "\n",
    "    # Add IndexID to intersect output if it doesn't exist\n",
    "    if not any(f.name == \"IndexID\" for f in arcpy.ListFields(intersect_output)):\n",
    "        arcpy.management.AddField(intersect_output, \"IndexID\", \"LONG\")\n",
    "        # Populate the IndexID field in intersect output\n",
    "        with arcpy.da.UpdateCursor(intersect_output, [\"IndexID\"]) as cursor:\n",
    "            for i, row in enumerate(cursor):\n",
    "                row[0] = i + 1\n",
    "                cursor.updateRow(row)\n",
    "\n",
    "    # Calculate the count of crimes within each fishnet grid cell\n",
    "    arcpy.analysis.Statistics(intersect_output, summary_output, [[id_field, \"COUNT\"]], \"IndexID\")\n",
    "\n",
    "    # Join the summary table back to the fishnet layer\n",
    "    count_field = f\"COUNT_{id_field}\"\n",
    "    arcpy.management.JoinField(fishnet_layer, \"IndexID\", summary_output, \"IndexID\", [count_field])\n",
    "\n",
    "    # Update null values in the joined count field to 0\n",
    "    with arcpy.da.UpdateCursor(fishnet_layer, [count_field]) as cursor:\n",
    "        for row in cursor:\n",
    "            if row[0] is None:\n",
    "                row[0] = 0  # Set null counts to 0\n",
    "            cursor.updateRow(row)\n",
    "\n",
    "    print(f\"Joined {count_field} to {fishnet_layer} and created summary table {summary_output}.\")\n",
    "    \n",
    "def calculate_population_density(input_layer, intersect_output, fishnet_layer, summary_output, id_field, density_field):\n",
    "    # Intersect the input population layer with the fishnet\n",
    "    arcpy.analysis.Intersect([input_layer, fishnet_layer], intersect_output)\n",
    "\n",
    "    # Add IndexID to intersect output if it doesn't exist\n",
    "    if not any(f.name == \"IndexID\" for f in arcpy.ListFields(intersect_output)):\n",
    "        arcpy.management.AddField(intersect_output, \"IndexID\", \"LONG\")\n",
    "        # Populate the IndexID field in intersect output\n",
    "        with arcpy.da.UpdateCursor(intersect_output, [\"IndexID\"]) as cursor:\n",
    "            for i, row in enumerate(cursor):\n",
    "                row[0] = i + 1\n",
    "                cursor.updateRow(row)\n",
    "\n",
    "    # Calculate the area of each intersected polygon (geodesic area in ftÂ²)\n",
    "    area_field = \"intersect_area\"\n",
    "    if not any(f.name == area_field for f in arcpy.ListFields(intersect_output)):\n",
    "        arcpy.management.AddField(intersect_output, area_field, \"DOUBLE\")\n",
    "\n",
    "        try:\n",
    "            # Try calculating geometry attributes with geodesic area\n",
    "            arcpy.management.CalculateGeometryAttributes(intersect_output, [[area_field, \"AREA_GEODESIC\"]], area_unit=\"SQUARE_FEET_US\")\n",
    "        except arcpy.ExecuteError:\n",
    "            # Fallback: Use Add Geometry Attributes tool\n",
    "            arcpy.management.AddGeometryAttributes(intersect_output, \"AREA_GEODESIC\", Area_Unit=\"SQUARE_FEET_US\")\n",
    "            arcpy.management.CalculateField(intersect_output, area_field, \"!POLY_AREA!\", \"PYTHON3\")\n",
    "\n",
    "    # Calculate the proportional population for each intersected area\n",
    "    proportional_population_field = \"proportional_population\"\n",
    "    if not any(f.name == proportional_population_field for f in arcpy.ListFields(intersect_output)):\n",
    "        arcpy.management.AddField(intersect_output, proportional_population_field, \"DOUBLE\")\n",
    "\n",
    "    with arcpy.da.UpdateCursor(intersect_output, [area_field, density_field, proportional_population_field]) as cursor:\n",
    "        for row in cursor:\n",
    "            if row[0] is not None and row[1] is not None:\n",
    "                row[2] = row[0] * row[1]  # proportional_population = intersect_area * population_density\n",
    "            else:\n",
    "                row[2] = 0\n",
    "            cursor.updateRow(row)\n",
    "\n",
    "    # Summarize the proportional population by fishnet grid (using IndexID)\n",
    "    arcpy.analysis.Statistics(intersect_output, summary_output, [[proportional_population_field, \"SUM\"]], \"IndexID\")\n",
    "\n",
    "    # Determine the correct name of the output field from summary statistics\n",
    "    summary_field_name = f\"SUM_{proportional_population_field}\"\n",
    "\n",
    "    # Join the summary table back to the fishnet layer\n",
    "    arcpy.management.JoinField(fishnet_layer, \"IndexID\", summary_output, \"IndexID\", [summary_field_name])\n",
    "\n",
    "    # Update null values in the joined population field to 0\n",
    "    with arcpy.da.UpdateCursor(fishnet_layer, [summary_field_name]) as cursor:\n",
    "        for row in cursor:\n",
    "            if row[0] is None:\n",
    "                row[0] = 0  # Set null population to 0\n",
    "            cursor.updateRow(row)\n",
    "\n",
    "    print(f\"Joined {summary_field_name} to {fishnet_layer} and created summary table {summary_output}.\")\n",
    "\n",
    "\n",
    "def calculate_max_speed_limit(intersect_layer, fishnet_layer, output_table, speed_limit_field):\n",
    "    # Calculate the max speed limit for each intersected grid cell\n",
    "    arcpy.analysis.Statistics(intersect_layer, output_table, [[speed_limit_field, \"MAX\"]], \"IndexID\")\n",
    "    # Join the result back to the fishnet layer\n",
    "    arcpy.management.JoinField(fishnet_layer, \"IndexID\", output_table, \"IndexID\", [\"MAX_\" + speed_limit_field])\n",
    "    # Rename the field to Max_Speed_Limit\n",
    "    arcpy.management.AlterField(fishnet_layer, \"MAX_\" + speed_limit_field, \"Max_Speed_Limit\")\n",
    "    \n",
    "def calculate_average_density(fishnet_layer, density_raster, features_layer, output_table):\n",
    "    \"\"\"Calculate average density for each fishnet grid and save as a table.\"\"\"\n",
    "    arcpy.CheckOutExtension(\"Spatial\")\n",
    "    extracted_density = arcpy.sa.ExtractByMask(density_raster, features_layer)\n",
    "    temp_extracted_density = f\"{output_gdb}\\\\temp_extracted_density\"\n",
    "    extracted_density.save(temp_extracted_density)\n",
    "    arcpy.sa.ZonalStatisticsAsTable(fishnet_layer, \"IndexID\", temp_extracted_density, output_table, \"NODATA\", \"MEAN\")\n",
    "    arcpy.management.Delete(temp_extracted_density)\n",
    "    print(f\"Average density calculated and saved to {output_table}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating the Effective Slope\n",
    "\n",
    "Since the slope data is provided in a raster, I'll need to segment this data to use only the slope data pertinent to the layers in my dataset. In the below function, we'll take the raster data and mask it with the layers provided in comb_feats, a list of features that we'll use to calculate the average slope.\n",
    "\n",
    "Using these combined features we can determine the exact slope of the sidewalk in a fishnet grid, rather than use the average slope over a grid as a proxy for the slope of the infrastructure a person will actually be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted existing C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\walkscore_fishnet.\n",
      "Copied fishnet_clipped to walkscore_fishnet.\n",
      "Index field populated with unique values.\n",
      "Calculated total area for each fishnet grid cell.\n"
     ]
    }
   ],
   "source": [
    "arcpy.env.overwriteOutput = True  # Allow outputs to be overwritten\n",
    "\n",
    "# Get the spatial reference of the fishnet layer\n",
    "fishnet_sr = arcpy.Describe(fishnet_layer).spatialReference\n",
    "\n",
    "# Define the walkscore_fishnet_layer\n",
    "walkscore_fishnet_layer = f\"{output_gdb}\\\\walkscore_fishnet\"\n",
    "\n",
    "# Check if the walkscore_fishnet_layer exists and delete it if it does\n",
    "if arcpy.Exists(walkscore_fishnet_layer):\n",
    "    arcpy.management.Delete(walkscore_fishnet_layer)\n",
    "    print(f\"Deleted existing {walkscore_fishnet_layer}.\")\n",
    "\n",
    "# Create a copy of the fishnet layer to work on\n",
    "arcpy.management.CopyFeatures(fishnet_layer, walkscore_fishnet_layer)\n",
    "print(\"Copied fishnet_clipped to walkscore_fishnet.\")\n",
    "\n",
    "# Add a new field for indexing and populate it with unique values\n",
    "index_field = \"IndexID\"\n",
    "if not any(f.name == index_field for f in arcpy.ListFields(walkscore_fishnet_layer)):\n",
    "    arcpy.management.AddField(walkscore_fishnet_layer, index_field, \"LONG\")\n",
    "\n",
    "# Populate the new index field with unique values\n",
    "with arcpy.da.UpdateCursor(walkscore_fishnet_layer, [index_field]) as cursor:\n",
    "    for i, row in enumerate(cursor):\n",
    "        row[0] = i + 1\n",
    "        cursor.updateRow(row)\n",
    "\n",
    "print(\"Index field populated with unique values.\")\n",
    "\n",
    "# Add a new field for total area if it doesn't exist\n",
    "total_area_field = \"total_area\"\n",
    "if not any(f.name == total_area_field for f in arcpy.ListFields(walkscore_fishnet_layer)):\n",
    "    arcpy.management.AddField(walkscore_fishnet_layer, total_area_field, \"DOUBLE\")\n",
    "\n",
    "# Calculate the total area for each fishnet grid cell\n",
    "arcpy.management.CalculateGeometryAttributes(walkscore_fishnet_layer, [[total_area_field, \"AREA_GEODESIC\"]])\n",
    "print(\"Calculated total area for each fishnet grid cell.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Process Each Layer and Calculate Allocations for each Fishnet Grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Effective Area Scalers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# norm_fields = [sidewalk_score_field, park_score_field, trail_score_field, street_score_field, bike_score_field]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalers = {\n",
    "    \"parkinglots\": 1,\n",
    "    \"industrial\": 1,\n",
    "    \"golfcourse\": 1,\n",
    "    \"hospitals\": 1,\n",
    "    \"cemeteries\": 1\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing Public Amenity Data & Separating Out Amenity Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique fclass values: 108\n",
      "Unique fclass values:\n",
      "kindergarten\n",
      "hairdresser\n",
      "general\n",
      "college\n",
      "community_centre\n",
      "atm\n",
      "beverages\n",
      "stationery\n",
      "jeweller\n",
      "playground\n",
      "hostel\n",
      "department_store\n",
      "swimming_pool\n",
      "university\n",
      "wastewater_plant\n",
      "travel_agent\n",
      "bicycle_rental\n",
      "pub\n",
      "biergarten\n",
      "gift_shop\n",
      "courthouse\n",
      "chemist\n",
      "recycling_paper\n",
      "post_box\n",
      "market_place\n",
      "beauty_shop\n",
      "sports_centre\n",
      "newsagent\n",
      "embassy\n",
      "vending_machine\n",
      "motel\n",
      "bicycle_shop\n",
      "shelter\n",
      "optician\n",
      "furniture_shop\n",
      "recycling_clothes\n",
      "doityourself\n",
      "mall\n",
      "doctors\n",
      "car_rental\n",
      "telephone\n",
      "outdoor_shop\n",
      "post_office\n",
      "cinema\n",
      "fast_food\n",
      "supermarket\n",
      "theatre\n",
      "hotel\n",
      "vending_any\n",
      "picnic_site\n",
      "school\n",
      "food_court\n",
      "clinic\n",
      "toy_shop\n",
      "memorial\n",
      "artwork\n",
      "greengrocer\n",
      "bar\n",
      "sports_shop\n",
      "ruins\n",
      "dog_park\n",
      "veterinary\n",
      "shoe_shop\n",
      "comms_tower\n",
      "dentist\n",
      "butcher\n",
      "monument\n",
      "drinking_water\n",
      "fountain\n",
      "viewpoint\n",
      "water_well\n",
      "laundry\n",
      "bakery\n",
      "recycling_glass\n",
      "car_sharing\n",
      "guesthouse\n",
      "vending_parking\n",
      "arts_centre\n",
      "nightclub\n",
      "mobile_phone_shop\n",
      "fire_station\n",
      "pitch\n",
      "video_shop\n",
      "bank\n",
      "bookshop\n",
      "wayside_shrine\n",
      "waste_basket\n",
      "toilet\n",
      "tourist_info\n",
      "clothes\n",
      "convenience\n",
      "car_wash\n",
      "pharmacy\n",
      "restaurant\n",
      "recycling\n",
      "florist\n",
      "museum\n",
      "hospital\n",
      "library\n",
      "computer_shop\n",
      "car_dealership\n",
      "bench\n",
      "tower\n",
      "garden_centre\n",
      "camera_surveillance\n",
      "attraction\n",
      "town_hall\n",
      "cafe\n"
     ]
    }
   ],
   "source": [
    "workspace = r\"C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\"\n",
    "layer = r\"C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\PointsofInterest\"\n",
    "\n",
    "# Use a set to collect unique fclass values\n",
    "fclass_set = set()\n",
    "\n",
    "# Use a SearchCursor to iterate through the fclass field\n",
    "with arcpy.da.SearchCursor(layer, [\"fclass\"]) as cursor:\n",
    "    for row in cursor:\n",
    "        fclass_set.add(row[0])\n",
    "\n",
    "# Print the unique fclass values and their count\n",
    "unique_fclass_count = len(fclass_set)\n",
    "print(f\"Number of unique fclass values: {unique_fclass_count}\")\n",
    "print(\"Unique fclass values:\")\n",
    "for fclass in fclass_set:\n",
    "    print(fclass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "business_amenities = [\n",
    "    'supermarket', 'convenience', 'greengrocer', 'butcher', 'department_store', 'mall', \n",
    "    'gift_shop', 'shoe_shop', 'clothes', 'bookshop', 'stationery', 'furniture_shop', 'jeweller', \n",
    "    'computer_shop', 'mobile_phone_shop', 'outdoor_shop', 'general', 'florist', 'toy_shop',\n",
    "    'beauty_shop', 'laundry', 'bank', 'atm', 'cafe', 'restaurant', \n",
    "    'pub', 'bar', 'fast_food', 'bakery', 'food_court', 'beverages', 'nightclub', 'car_sharing',\n",
    "    'car_wash', 'video_shop', 'vending_any', 'theatre', 'museum', 'attraction', 'cinema',\n",
    "    'market_place', 'mobile_phone_shop', 'bookshop', 'laundry', 'mobile_phone_shop',\n",
    "    'garden_centre','doityourself','hairdresser','bicycle_shop','biergarten','sports_shop'\n",
    "]\n",
    "public_amenities = [\n",
    "    'bench', 'drinking_water', 'waste_basket', 'library', 'post_box','post_office', 'recycling', \n",
    "    'recycling_glass', 'recycling_paper', 'vending_machine', 'artwork', 'tourist_info',\n",
    "    'viewpoint', 'monument', 'picnic_site', 'memorial', 'fountain', 'shelter', 'public_building',\n",
    "    'arts_centre','courthouse','community_centre'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main Processing Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing layer: Business_Amenities\n",
      "Skipping point layer Business_Amenities for separate kernel density processing.\n",
      "Processing layer: Industrial\n",
      "Processing layer: ParkingLots\n",
      "Processing layer: GolfCourse\n",
      "Processing layer: Cemeteries\n",
      "Processing layer: Hospitals\n",
      "Processing layer: Slope\n",
      "Layer Slope does not have a shapeType attribute. Skipping.\n",
      "Processing layer: Bike_greenways\n",
      "Processing layer: Bike_protected\n",
      "Processing layer: Bike_buffer\n",
      "Processing layer: Healthy_Streets\n",
      "Processing layer: Parks\n",
      "Processing layer: Universities\n",
      "Processing layer: Sidewalks\n",
      "Processing layer: Plaza\n",
      "Processing layer: trails\n",
      "Processing layer: MultiUseTrails\n",
      "Processing layer: Streets\n",
      "Calculated effective area for C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\Streets_int and stored in Streets_effective_area.\n",
      "Processing layer: population\n",
      "Joined SUM_proportional_population to C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\walkscore_fishnet and created summary table C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\population_sum.\n",
      "Processing layer: SPD_Crime_Data\n",
      "Joined COUNT_Offense_ID to C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\walkscore_fishnet and created summary table C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\SPD_Crime_Data_sum.\n",
      "Main processing complete for all layers.\n"
     ]
    }
   ],
   "source": [
    "# Main processing loop for preprocessing layers\n",
    "created_layers = []\n",
    "\n",
    "for layer_name in base_layers:\n",
    "    print(f\"Processing layer: {layer_name}\")\n",
    "\n",
    "    input_layer = f\"{base_layers_group}\\\\{layer_name}\"\n",
    "\n",
    "    if not arcpy.Exists(input_layer):\n",
    "        print(f\"Layer {input_layer} does not exist. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    desc = arcpy.Describe(input_layer)\n",
    "    if hasattr(desc, \"shapeType\"):\n",
    "        geometry_type = desc.shapeType\n",
    "    else:\n",
    "        print(f\"Layer {layer_name} does not have a shapeType attribute. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    # Handle SPD_crime_data layer\n",
    "    if layer_name.lower() == \"spd_crime_data\":\n",
    "        intersect_output = f\"{workspace}\\\\{layer_name}_intersect\"\n",
    "        summary_output = f\"{workspace}\\\\{layer_name}_sum\"\n",
    "        id_field = \"Offense_ID\"\n",
    "        calculate_counts(input_layer, intersect_output, walkscore_fishnet_layer, summary_output, id_field)\n",
    "        created_layers.append(summary_output)\n",
    "        continue\n",
    "\n",
    "    # Handle Population layer\n",
    "    if layer_name.lower() == \"population\":\n",
    "        intersect_output = f\"{workspace}\\\\{layer_name}_intersect\"\n",
    "        summary_output = f\"{workspace}\\\\{layer_name}_sum\"\n",
    "        id_field = \"OBJECTID\"\n",
    "        density_field = \"density_2023\"\n",
    "        calculate_population_density(input_layer, intersect_output, walkscore_fishnet_layer, summary_output, id_field, density_field)\n",
    "        created_layers.append(summary_output)\n",
    "        continue\n",
    "\n",
    "    # Skip Point layers (e.g., business_amenities) for now; handle them later for Kernel Density\n",
    "    if geometry_type == \"Point\" and layer_name.endswith(\"_Amenities\"):\n",
    "        print(f\"Skipping point layer {layer_name} for separate kernel density processing.\")\n",
    "        continue\n",
    "\n",
    "    # Handle Polygons and Polylines as before\n",
    "    input_layer_sr = desc.spatialReference\n",
    "    fishnet_sr = arcpy.Describe(walkscore_fishnet_layer).spatialReference\n",
    "    projected_layer = f\"{workspace}\\\\{layer_name}_proj\"\n",
    "\n",
    "    if input_layer_sr.name != fishnet_sr.name:\n",
    "        if arcpy.Exists(projected_layer):\n",
    "            arcpy.management.Delete(projected_layer)\n",
    "        arcpy.management.Project(input_layer, projected_layer, fishnet_sr)\n",
    "    else:\n",
    "        projected_layer = input_layer\n",
    "\n",
    "    intersect_output = f\"{workspace}\\\\{layer_name}_int\"\n",
    "\n",
    "    if arcpy.Exists(intersect_output):\n",
    "        arcpy.management.Delete(intersect_output)\n",
    "\n",
    "    arcpy.analysis.Intersect([walkscore_fishnet_layer, projected_layer], intersect_output)\n",
    "\n",
    "    if layer_name.lower() == \"streets\":\n",
    "        effective_area_field = f\"{layer_name}_effective_area\"\n",
    "        calculate_effective_area(intersect_output, effective_area_field)\n",
    "        area_field = effective_area_field\n",
    "\n",
    "        # Run the calculate_max_speed_limit function here for the Streets layer\n",
    "        output_table = f\"{output_gdb}\\\\max_speed_limit_Streets_int\"\n",
    "        calculate_max_speed_limit(intersect_output, walkscore_fishnet_layer, output_table, \"effective_SPEEDLIMIT\")\n",
    "\n",
    "    elif layer_name.lower() in scalers.keys():\n",
    "        area_field = f\"{layer_name}_area\"\n",
    "        area_field = area_field.replace(\"-\", \"_\").replace(\" \", \"_\")\n",
    "        if geometry_type == \"Polygon\":\n",
    "            calculate_polygon_area(intersect_output, area_field)\n",
    "        effective_area_field = f\"{layer_name}_effective_area\"\n",
    "        if not any(f.name == effective_area_field for f in arcpy.ListFields(intersect_output)):\n",
    "            arcpy.management.AddField(intersect_output, effective_area_field, \"DOUBLE\")\n",
    "        scaler = scalers[layer_name.lower()]\n",
    "        with arcpy.da.UpdateCursor(intersect_output, [area_field, effective_area_field]) as cursor:\n",
    "            for row in cursor:\n",
    "                if row[0] is not None:\n",
    "                    row[1] = row[0] * scaler\n",
    "                else:\n",
    "                    row[1] = None\n",
    "                cursor.updateRow(row)\n",
    "        area_field = effective_area_field\n",
    "    else:\n",
    "        area_field = f\"{layer_name}_area\"\n",
    "        area_field = area_field.replace(\"-\", \"_\").replace(\" \", \"_\")\n",
    "        if geometry_type == \"Polygon\":\n",
    "            calculate_polygon_area(intersect_output, area_field)\n",
    "        elif geometry_type == \"Polyline\":\n",
    "            width_field = None\n",
    "            for field in arcpy.ListFields(intersect_output):\n",
    "                if field.name.lower().endswith(\"width\"):\n",
    "                    width_field = field.name\n",
    "            if width_field:\n",
    "                calculate_polyline_area_with_recalculated_length(intersect_output, area_field, width_field)\n",
    "            else:\n",
    "                print(f\"Width field not found for {layer_name}, skipping area calculation.\")\n",
    "\n",
    "    if not any(f.name.lower() == area_field.lower() for f in arcpy.ListFields(intersect_output)):\n",
    "        print(f\"Area field {area_field} was not created for {layer_name}, skipping summary statistics.\")\n",
    "        continue\n",
    "\n",
    "    summary_output = f\"{workspace}\\\\{layer_name}_sum\"\n",
    "    if arcpy.Exists(summary_output):\n",
    "        arcpy.management.Delete(summary_output)\n",
    "\n",
    "    if not any(f.name == index_field for f in arcpy.ListFields(intersect_output)):\n",
    "        arcpy.management.AddField(intersect_output, index_field, \"LONG\")\n",
    "        with arcpy.da.UpdateCursor(intersect_output, [index_field]) as cursor:\n",
    "            for i, row in enumerate(cursor):\n",
    "                row[0] = i + 1\n",
    "                cursor.updateRow(row)\n",
    "\n",
    "    arcpy.analysis.Statistics(intersect_output, summary_output, [[area_field, \"SUM\"]], index_field)\n",
    "    created_layers.append(summary_output)\n",
    "\n",
    "print(\"Main processing complete for all layers.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Slope Processing Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing slope for layer: Sidewalks\n",
      "Calculating slope for Sidewalks\n",
      "Processing slope for layer: Streets\n",
      "Calculating slope for Streets\n",
      "Processing slope for layer: MultiUseTrails\n",
      "Calculating slope for MultiUseTrails\n",
      "Processing slope for layer: trails\n",
      "Calculating slope for trails\n",
      "Slope processing complete for all layers.\n",
      "Sample data from all_data for debugging:\n",
      "IndexID: 1, Data: {'Sidewalks_Slope_Mean': 2.6028627527171175, 'Streets_Slope_Mean': 2.4719635209729587, 'MultiUseTrails_Slope_Mean': None, 'trails_Slope_Mean': None, 'Grid_Slope_MEAN': 3.0777811209360757}\n",
      "IndexID: 2, Data: {'Sidewalks_Slope_Mean': 1.8433810224135716, 'Streets_Slope_Mean': 0.9660569752256075, 'MultiUseTrails_Slope_Mean': None, 'trails_Slope_Mean': None, 'Grid_Slope_MEAN': 2.2087172894842113}\n",
      "IndexID: 3, Data: {'Sidewalks_Slope_Mean': None, 'Streets_Slope_Mean': 3.095643554415022, 'MultiUseTrails_Slope_Mean': None, 'trails_Slope_Mean': 2.7575330917651844, 'Grid_Slope_MEAN': 2.269062724378373}\n",
      "IndexID: 4, Data: {'Sidewalks_Slope_Mean': 4.099437493544359, 'Streets_Slope_Mean': None, 'MultiUseTrails_Slope_Mean': None, 'trails_Slope_Mean': None, 'Grid_Slope_MEAN': 4.542804545164107}\n",
      "IndexID: 5, Data: {'Sidewalks_Slope_Mean': None, 'Streets_Slope_Mean': None, 'MultiUseTrails_Slope_Mean': None, 'trails_Slope_Mean': None, 'Grid_Slope_MEAN': 32.985248031038225}\n",
      "Combined slope mean calculated and updated.\n"
     ]
    }
   ],
   "source": [
    "slope_layers = ['Sidewalks', 'Streets', 'MultiUseTrails', 'trails']\n",
    "\n",
    "# Calculate slope for entire grid\n",
    "grid_slope_output_table = f\"{output_gdb}\\\\grid_slope\"\n",
    "calculate_average_slope(walkscore_fishnet_layer, \"Slope\", walkscore_fishnet_layer, grid_slope_output_table)\n",
    "\n",
    "# Check if \"Grid_Slope_MEAN\" field already exists, and join or alter as necessary\n",
    "if not any(f.name == \"Grid_Slope_MEAN\" for f in arcpy.ListFields(walkscore_fishnet_layer)):\n",
    "    arcpy.management.JoinField(walkscore_fishnet_layer, \"IndexID\", grid_slope_output_table, \"IndexID\", \"MEAN\")\n",
    "    arcpy.management.AlterField(walkscore_fishnet_layer, \"MEAN\", \"Grid_Slope_MEAN\")\n",
    "else:\n",
    "    print(\"Grid_Slope_MEAN field already exists. Skipping join and alter operations.\")\n",
    "\n",
    "# Process slope for specific polyline layers\n",
    "for layer_name in slope_layers:\n",
    "    print(f\"Processing slope for layer: {layer_name}\")\n",
    "\n",
    "    input_layer = f\"{base_layers_group}\\\\{layer_name}\"\n",
    "    \n",
    "    if not arcpy.Exists(input_layer):\n",
    "        print(f\"Layer {input_layer} does not exist. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    desc = arcpy.Describe(input_layer)\n",
    "    if hasattr(desc, \"shapeType\"):\n",
    "        geometry_type = desc.shapeType\n",
    "        if geometry_type != \"Polyline\":\n",
    "            print(f\"Layer {layer_name} is not a polyline. Skipping slope calculation.\")\n",
    "            continue\n",
    "    else:\n",
    "        print(f\"Layer {layer_name} does not have a shapeType attribute. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    intersect_output = f\"{workspace}\\\\{layer_name}_int\"\n",
    "    slope_output_table = f\"{workspace}\\\\{layer_name}_slope\"\n",
    "\n",
    "    if arcpy.Exists(intersect_output):\n",
    "        print(f\"Calculating slope for {layer_name}\")\n",
    "        calculate_average_slope(intersect_output, \"Slope\", intersect_output, slope_output_table)\n",
    "\n",
    "        effective_slope_field = f\"{layer_name}_Slope_Mean\"\n",
    "\n",
    "        # Ensure the effective_slope_field exists\n",
    "        if not any(f.name == effective_slope_field for f in arcpy.ListFields(walkscore_fishnet_layer)):\n",
    "            arcpy.management.AddField(walkscore_fishnet_layer, effective_slope_field, \"DOUBLE\")\n",
    "\n",
    "        slope_df = arcpy.da.TableToNumPyArray(slope_output_table, [\"IndexID\", \"MEAN\"])\n",
    "        slope_dict = {row[\"IndexID\"]: row[\"MEAN\"] for row in slope_df}\n",
    "\n",
    "        with arcpy.da.UpdateCursor(walkscore_fishnet_layer, [\"IndexID\", effective_slope_field]) as cursor:\n",
    "            for row in cursor:\n",
    "                row[1] = slope_dict.get(row[0], None)\n",
    "                cursor.updateRow(row)\n",
    "    else:\n",
    "        print(f\"Intersect output for {layer_name} does not exist. Skipping.\")\n",
    "\n",
    "print(\"Slope processing complete for all layers.\")\n",
    "\n",
    "# Ensure that the combined slope mean is calculated correctly\n",
    "slope_fields = [f\"{layer}_Slope_Mean\" for layer in slope_layers if any(f.name == f\"{layer}_Slope_Mean\" for f in arcpy.ListFields(walkscore_fishnet_layer))]\n",
    "\n",
    "# Collect all necessary data in one go\n",
    "all_data = {\n",
    "    row[0]: {\n",
    "        field: row[idx + 1] for idx, field in enumerate(slope_fields + [\"Grid_Slope_MEAN\"])\n",
    "    } for row in arcpy.da.SearchCursor(walkscore_fishnet_layer, [\"IndexID\"] + slope_fields + [\"Grid_Slope_MEAN\"])\n",
    "}\n",
    "\n",
    "# Debug: Print a few entries to check data integrity\n",
    "print(\"Sample data from all_data for debugging:\")\n",
    "for idx, (key, value) in enumerate(all_data.items()):\n",
    "    if idx < 5:  # Print first 5 entries\n",
    "        print(f\"IndexID: {key}, Data: {value}\")\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# Update the effective_slope field based on the collected data\n",
    "if not any(f.name == \"effective_slope\" for f in arcpy.ListFields(walkscore_fishnet_layer)):\n",
    "    arcpy.management.AddField(walkscore_fishnet_layer, \"effective_slope\", \"DOUBLE\")\n",
    "\n",
    "with arcpy.da.UpdateCursor(walkscore_fishnet_layer, [\"IndexID\", \"effective_slope\"] + slope_fields + [\"Grid_Slope_MEAN\"]) as cursor:\n",
    "    for row in cursor:\n",
    "        index_id = row[0]\n",
    "        sidewalks_slope = all_data[index_id].get(\"Sidewalks_Slope_Mean\")\n",
    "        multiuse_trails_slope = all_data[index_id].get(\"MultiUseTrails_Slope_Mean\")\n",
    "        streets_slope = all_data[index_id].get(\"Streets_Slope_Mean\")\n",
    "        grid_slope_mean = all_data[index_id][\"Grid_Slope_MEAN\"]\n",
    "\n",
    "        if sidewalks_slope is not None:\n",
    "            slope_mean = sidewalks_slope\n",
    "        elif multiuse_trails_slope is not None:\n",
    "            slope_mean = multiuse_trails_slope\n",
    "        elif streets_slope is not None:\n",
    "            slope_mean = streets_slope\n",
    "        else:\n",
    "            slope_mean = grid_slope_mean\n",
    "\n",
    "        row[1] = slope_mean\n",
    "        cursor.updateRow(row)\n",
    "\n",
    "print(\"Combined slope mean calculated and updated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Business Density Processing Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Kernel Density for business layer: Business_Amenities\n",
      "Clipped points for Business_Amenities using the barrier layer.\n",
      "Generated kernel density heatmap for Business_Amenities using clipped features.\n",
      "Applied focal statistics to smooth the kernel density heatmap for Business_Amenities.\n",
      "Average density calculated and saved to C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\Business_Amenities_zonal_stats.\n",
      "Added new field 'business_density'.\n",
      "Deleted temporary field 'MEAN' after updating 'business_density'.\n",
      "Updated 'business_density' field with mean density values for Business_Amenities.\n",
      "Business density processing complete for all layers.\n"
     ]
    }
   ],
   "source": [
    "# Business Density Calculation for Entire Fishnet Grid\n",
    "\n",
    "# List all business amenity point layers to apply kernel density\n",
    "business_layers = [layer_name for layer_name in base_layers if layer_name.endswith(\"_Amenities\")]\n",
    "\n",
    "# Define the path to the neighborhood layer that will be used as a barrier\n",
    "barrier_layer = \"neighborhoods\"\n",
    "\n",
    "for layer_name in business_layers:\n",
    "    print(f\"Processing Kernel Density for business layer: {layer_name}\")\n",
    "\n",
    "    input_layer = f\"{base_layers_group}\\\\{layer_name}\"\n",
    "\n",
    "    if not arcpy.Exists(input_layer):\n",
    "        print(f\"Layer {input_layer} does not exist. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    # Step 1: Clip the Point Features by the Barrier Layer\n",
    "    clipped_points = f\"{workspace}\\\\{layer_name}_clipped\"\n",
    "    arcpy.analysis.Clip(\n",
    "        in_features=input_layer,\n",
    "        clip_features=barrier_layer,\n",
    "        out_feature_class=clipped_points\n",
    "    )\n",
    "    print(f\"Clipped points for {layer_name} using the barrier layer.\")\n",
    "\n",
    "    # Step 2: Apply Kernel Density Tool to Generate Business Density Raster\n",
    "    kernel_density_output = f\"{workspace}\\\\{layer_name}_kernel_density\"\n",
    "    arcpy.sa.KernelDensity(\n",
    "        in_features=clipped_points,\n",
    "        population_field=\"NONE\",\n",
    "        out_cell_values=\"DENSITIES\",\n",
    "        method='GEODESIC',\n",
    "        cell_size=\"6.59609600103067E-04\",  # Match cell size to your fishnet grid resolution\n",
    "        search_radius=\"50\",  # Adjust based on the intended influence\n",
    "        area_unit_scale_factor=\"SQUARE_FEET\"\n",
    "    ).save(kernel_density_output)\n",
    "    print(f\"Generated kernel density heatmap for {layer_name} using clipped features.\")\n",
    "\n",
    "    # Step 3: Apply Smoothing Using Focal Statistics\n",
    "    smoothed_kernel_density_output = f\"{workspace}\\\\{layer_name}_smoothed_kernel_density\"\n",
    "    smoothed_raster = arcpy.sa.FocalStatistics(\n",
    "        in_raster=kernel_density_output,\n",
    "        neighborhood=arcpy.sa.NbrCircle(radius=3, units=\"CELL\"),  # Circular neighborhood with radius of 8 cells\n",
    "        statistics_type=\"MEAN\"\n",
    "    )\n",
    "    smoothed_raster.save(smoothed_kernel_density_output)\n",
    "    print(f\"Applied focal statistics to smooth the kernel density heatmap for {layer_name}.\")\n",
    "\n",
    "    # Step 4: Calculate Average Density for Each Fishnet Grid Using Zonal Statistics\n",
    "    zonal_output_table = f\"{workspace}\\\\{layer_name}_zonal_stats\"\n",
    "    calculate_average_density(walkscore_fishnet_layer, smoothed_kernel_density_output, walkscore_fishnet_layer, zonal_output_table)\n",
    "\n",
    "    # Step 5: Join Zonal Statistics Back to Fishnet Grid\n",
    "    business_density_field = \"business_density\"\n",
    "\n",
    "    # Delete the existing \"business_density\" field if it already exists\n",
    "    if any(f.name == business_density_field for f in arcpy.ListFields(walkscore_fishnet_layer)):\n",
    "        arcpy.management.DeleteField(walkscore_fishnet_layer, business_density_field)\n",
    "        print(f\"Deleted existing field '{business_density_field}'.\")\n",
    "\n",
    "    # Add the new \"business_density\" field\n",
    "    arcpy.management.AddField(walkscore_fishnet_layer, business_density_field, \"DOUBLE\")\n",
    "    print(f\"Added new field '{business_density_field}'.\")\n",
    "\n",
    "    # Join the mean values from the zonal stats output back to the fishnet\n",
    "    arcpy.management.JoinField(\n",
    "        in_data=walkscore_fishnet_layer,\n",
    "        in_field=\"IndexID\",\n",
    "        join_table=zonal_output_table,\n",
    "        join_field=\"IndexID\",\n",
    "        fields=[\"MEAN\"]\n",
    "    )\n",
    "\n",
    "    # Update the \"business_density\" field using the \"MEAN\" field from the joined table, setting nulls to 0\n",
    "    with arcpy.da.UpdateCursor(walkscore_fishnet_layer, [\"MEAN\", business_density_field]) as cursor:\n",
    "        for row in cursor:\n",
    "            row[1] = row[0] if row[0] is not None else 0  # Copy the value from the \"MEAN\" field or set to 0 if None\n",
    "            cursor.updateRow(row)\n",
    "\n",
    "    # Delete the \"MEAN\" field after copying values\n",
    "    arcpy.management.DeleteField(walkscore_fishnet_layer, \"MEAN\")\n",
    "    print(f\"Deleted temporary field 'MEAN' after updating 'business_density'.\")\n",
    "\n",
    "    print(f\"Updated 'business_density' field with mean density values for {layer_name}.\")\n",
    "\n",
    "print(\"Business density processing complete for all layers.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank-normalized 'business_density' field to a scale of 0-5.\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Rank Normalize the Business Density Field Between 0-5\n",
    "# Step 4.1: Extract all business density values\n",
    "business_density_values = []\n",
    "\n",
    "with arcpy.da.SearchCursor(walkscore_fishnet_layer, [\"business_density\"]) as cursor:\n",
    "    for row in cursor:\n",
    "        if row[0] is not None:\n",
    "            business_density_values.append(row[0])\n",
    "\n",
    "# Step 4.2: Rank values and normalize to a scale of 0-5\n",
    "sorted_values = sorted(set(business_density_values))  # Remove duplicates and sort values\n",
    "ranks = {value: rank for rank, value in enumerate(sorted_values, start=1)}  # Create a dictionary with ranks\n",
    "\n",
    "# Get the maximum rank to normalize ranks between 0 and 5\n",
    "max_rank = max(ranks.values())\n",
    "\n",
    "# Update the \"business_density\" field with rank normalization between 0-5\n",
    "with arcpy.da.UpdateCursor(walkscore_fishnet_layer, [\"business_density\"]) as cursor:\n",
    "    for row in cursor:\n",
    "        if row[0] is not None:\n",
    "            # Normalize the rank to a scale of 0-5\n",
    "            rank = ranks[row[0]]  # Fetch rank from dictionary\n",
    "            row[0] = (rank - 1) / (max_rank - 1) * 5 if max_rank > 1 else 0  # Ensure scale is 0-5\n",
    "        cursor.updateRow(row)\n",
    "\n",
    "print(\"Rank-normalized 'business_density' field to a scale of 0-5.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating an Index Field for walkscore_fishnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Populate the new index field with unique values\n",
    "with arcpy.da.UpdateCursor(walkscore_fishnet_layer, [index_field]) as cursor:\n",
    "    for i, row in enumerate(cursor):\n",
    "        row[0] = i + 1\n",
    "        cursor.updateRow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3:  Join Summary Statistic Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deleted existing summary table\n",
      "Created merged_sums table.\n"
     ]
    }
   ],
   "source": [
    "merged_summary = f\"{output_gdb}\\\\merged_sums\"\n",
    "\n",
    "if arcpy.Exists(merged_summary):\n",
    "    arcpy.management.Delete(merged_summary)\n",
    "    print('deleted existing summary table')\n",
    "    \n",
    "arcpy.management.CreateTable(output_gdb, \"merged_sums\")\n",
    "print(\"Created merged_sums table.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated merged summary table created successfully.\n"
     ]
    }
   ],
   "source": [
    "# Add IndexID field to the merged summary table if it doesn't exist\n",
    "if not any(f.name == index_field for f in arcpy.ListFields(merged_summary)):\n",
    "    arcpy.management.AddField(merged_summary, index_field, \"LONG\")\n",
    "\n",
    "# Create a dictionary to store the aggregated sums\n",
    "aggregated_sums = defaultdict(lambda: defaultdict(float))\n",
    "\n",
    "# Iterate through each summary table and aggregate values by IndexID\n",
    "for layer_name in base_layers:\n",
    "    summary_output = f\"{output_gdb}\\\\{layer_name}_sum\"\n",
    "    \n",
    "    # Verify if summary_output exists\n",
    "    if not arcpy.Exists(summary_output):\n",
    "        print(f\"Summary table {summary_output} does not exist. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    fields = arcpy.ListFields(summary_output)\n",
    "    field_names = [f.name for f in fields if f.name != index_field]\n",
    "    \n",
    "    # Aggregate the summary fields into the dictionary\n",
    "    with arcpy.da.SearchCursor(summary_output, [index_field] + field_names) as cursor:\n",
    "        for row in cursor:\n",
    "            idx = row[0]\n",
    "            for i, field_name in enumerate(field_names):\n",
    "                value = row[i+1] if row[i+1] is not None else 0\n",
    "                aggregated_sums[idx][f\"{layer_name}_{field_name}\"] += value\n",
    "\n",
    "# Add aggregated fields to the merged summary table\n",
    "for layer_name in base_layers:\n",
    "    summary_output = f\"{output_gdb}\\\\{layer_name}_sum\"\n",
    "    \n",
    "    # Verify if summary_output exists\n",
    "    if not arcpy.Exists(summary_output):\n",
    "        print(f\"Summary table {summary_output} does not exist. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    fields = arcpy.ListFields(summary_output)\n",
    "    for field in fields:\n",
    "        if field.name != index_field:\n",
    "            field_name = f\"{layer_name}_{field.name}\"\n",
    "            if not any(f.name == field_name for f in arcpy.ListFields(merged_summary)):\n",
    "                arcpy.management.AddField(merged_summary, field_name, \"DOUBLE\")\n",
    "\n",
    "# Insert the aggregated sums into the merged summary table\n",
    "field_names_to_insert = [index_field] + [f\"{layer_name}_{field.name}\" for layer_name in base_layers for field in arcpy.ListFields(f\"{output_gdb}\\\\{layer_name}_sum\") if field.name != index_field]\n",
    "with arcpy.da.InsertCursor(merged_summary, field_names_to_insert) as cursor:\n",
    "    for idx, fields in aggregated_sums.items():\n",
    "        row = [idx] + [fields.get(field_name, 0) for field_name in field_names_to_insert if field_name != index_field]\n",
    "        cursor.insertRow(row)\n",
    "\n",
    "print(\"Aggregated merged summary table created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fields in merged summary table: ['OBJECTID', 'IndexID', 'Business_Amenities_OBJECTID', 'Business_Amenities_FREQUENCY', 'Business_Amenities_COUNT_osm_business_id', 'Industrial_OBJECTID', 'Industrial_FREQUENCY', 'Industrial_SUM_Industrial_effective_area', 'ParkingLots_OBJECTID', 'ParkingLots_FREQUENCY', 'ParkingLots_SUM_ParkingLots_effective_area', 'GolfCourse_OBJECTID', 'GolfCourse_FREQUENCY', 'GolfCourse_SUM_GolfCourse_effective_area', 'Cemeteries_OBJECTID', 'Cemeteries_FREQUENCY', 'Cemeteries_SUM_Cemeteries_effective_area', 'Hospitals_OBJECTID', 'Hospitals_FREQUENCY', 'Hospitals_SUM_Hospitals_effective_area', 'Slope_OBJECTID', 'Slope_COUNT', 'Slope_AREA', 'Slope_MEAN', 'Bike_greenways_OBJECTID', 'Bike_greenways_FREQUENCY', 'Bike_greenways_SUM_Bike_greenways_area', 'Bike_protected_OBJECTID', 'Bike_protected_FREQUENCY', 'Bike_protected_SUM_Bike_protected_area', 'Bike_buffer_OBJECTID', 'Bike_buffer_FREQUENCY', 'Bike_buffer_SUM_Bike_buffer_area', 'Healthy_Streets_OBJECTID', 'Healthy_Streets_FREQUENCY', 'Healthy_Streets_SUM_Healthy_Streets_area', 'Parks_OBJECTID', 'Parks_FREQUENCY', 'Parks_SUM_Parks_area', 'Universities_OBJECTID', 'Universities_FREQUENCY', 'Universities_SUM_Universities_area', 'Sidewalks_OBJECTID', 'Sidewalks_FREQUENCY', 'Sidewalks_SUM_Sidewalks_area', 'Plaza_OBJECTID', 'Plaza_FREQUENCY', 'Plaza_SUM_Plaza_area', 'trails_OBJECTID', 'trails_FREQUENCY', 'trails_SUM_trails_area', 'MultiUseTrails_OBJECTID', 'MultiUseTrails_FREQUENCY', 'MultiUseTrails_SUM_MultiUseTrails_area', 'Streets_OBJECTID', 'Streets_FREQUENCY', 'Streets_SUM_Streets_effective_area', 'population_OBJECTID', 'population_FREQUENCY', 'population_SUM_proportional_population', 'SPD_Crime_Data_OBJECTID', 'SPD_Crime_Data_FREQUENCY', 'SPD_Crime_Data_COUNT_Offense_ID']\n",
      "IndexID already exists in C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\merged_sums.\n",
      "Verified IndexID in merged_sums.\n"
     ]
    }
   ],
   "source": [
    "# Path to the merged summary table\n",
    "merged_summary = r\"C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\merged_sums\"\n",
    "index_field = \"IndexID\"\n",
    "\n",
    "# Check if merged summary table exists\n",
    "if not arcpy.Exists(merged_summary):\n",
    "    raise ValueError(f\"{merged_summary} does not exist.\")\n",
    "\n",
    "# List all fields in the merged summary table for debugging\n",
    "fields = arcpy.ListFields(merged_summary)\n",
    "field_names = [field.name for field in fields]\n",
    "print(\"Fields in merged summary table:\", field_names)\n",
    "\n",
    "# Ensure IndexID exists in merged_sums\n",
    "if not any(f.name == index_field for f in fields):\n",
    "    print(f\"Adding {index_field} to {merged_summary}.\")\n",
    "    arcpy.management.AddField(merged_summary, index_field, \"LONG\")\n",
    "else:\n",
    "    print(f\"{index_field} already exists in {merged_summary}.\")\n",
    "\n",
    "print(\"Verified IndexID in merged_sums.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Join the Summary Statistics to the Fishnet Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    arcpy.management.JoinField(walkscore_fishnet_layer, \"IndexID\", merged_summary, \"IndexID\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during join: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding a neighborhood field to Walkscore Fishnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intersected walkscore fishnet with neighborhoods to create fragments.\n",
      "Using fields: ['nested', 'is_tourist', 'is_industrial']\n",
      "Calculated area for each fragment.\n",
      "Overwritten C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\walkscore_fishnet with the intersected fragments, retaining neighborhood names.\n",
      "Fields in C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\walkscore_fishnet after processing:\n",
      "Name: OBJECTID, Type: OID\n",
      "Name: Shape, Type: Geometry\n",
      "Name: FID_walkscore_fishnet, Type: Integer\n",
      "Name: IndexID, Type: Integer\n",
      "Name: total_area, Type: Double\n",
      "Name: Max_Speed_Limit, Type: Double\n",
      "Name: SUM_proportional_population, Type: Double\n",
      "Name: COUNT_Offense_ID, Type: Integer\n",
      "Name: Grid_Slope_MEAN, Type: Double\n",
      "Name: Sidewalks_Slope_Mean, Type: Double\n",
      "Name: Streets_Slope_Mean, Type: Double\n",
      "Name: MultiUseTrails_Slope_Mean, Type: Double\n",
      "Name: trails_Slope_Mean, Type: Double\n",
      "Name: effective_slope, Type: Double\n",
      "Name: business_density, Type: Double\n",
      "Name: IndexID_1, Type: Integer\n",
      "Name: Business_Amenities_OBJECTID, Type: Double\n",
      "Name: Business_Amenities_FREQUENCY, Type: Double\n",
      "Name: Business_Amenities_COUNT_osm_business_id, Type: Double\n",
      "Name: Industrial_OBJECTID, Type: Double\n",
      "Name: Industrial_FREQUENCY, Type: Double\n",
      "Name: Industrial_SUM_Industrial_effective_area, Type: Double\n",
      "Name: ParkingLots_OBJECTID, Type: Double\n",
      "Name: ParkingLots_FREQUENCY, Type: Double\n",
      "Name: ParkingLots_SUM_ParkingLots_effective_area, Type: Double\n",
      "Name: GolfCourse_OBJECTID, Type: Double\n",
      "Name: GolfCourse_FREQUENCY, Type: Double\n",
      "Name: GolfCourse_SUM_GolfCourse_effective_area, Type: Double\n",
      "Name: Cemeteries_OBJECTID, Type: Double\n",
      "Name: Cemeteries_FREQUENCY, Type: Double\n",
      "Name: Cemeteries_SUM_Cemeteries_effective_area, Type: Double\n",
      "Name: Hospitals_OBJECTID, Type: Double\n",
      "Name: Hospitals_FREQUENCY, Type: Double\n",
      "Name: Hospitals_SUM_Hospitals_effective_area, Type: Double\n",
      "Name: Slope_OBJECTID, Type: Double\n",
      "Name: Slope_COUNT, Type: Double\n",
      "Name: Slope_AREA, Type: Double\n",
      "Name: Slope_MEAN, Type: Double\n",
      "Name: Bike_greenways_OBJECTID, Type: Double\n",
      "Name: Bike_greenways_FREQUENCY, Type: Double\n",
      "Name: Bike_greenways_SUM_Bike_greenways_area, Type: Double\n",
      "Name: Bike_protected_OBJECTID, Type: Double\n",
      "Name: Bike_protected_FREQUENCY, Type: Double\n",
      "Name: Bike_protected_SUM_Bike_protected_area, Type: Double\n",
      "Name: Bike_buffer_OBJECTID, Type: Double\n",
      "Name: Bike_buffer_FREQUENCY, Type: Double\n",
      "Name: Bike_buffer_SUM_Bike_buffer_area, Type: Double\n",
      "Name: Healthy_Streets_OBJECTID, Type: Double\n",
      "Name: Healthy_Streets_FREQUENCY, Type: Double\n",
      "Name: Healthy_Streets_SUM_Healthy_Streets_area, Type: Double\n",
      "Name: Parks_OBJECTID, Type: Double\n",
      "Name: Parks_FREQUENCY, Type: Double\n",
      "Name: Parks_SUM_Parks_area, Type: Double\n",
      "Name: Universities_OBJECTID, Type: Double\n",
      "Name: Universities_FREQUENCY, Type: Double\n",
      "Name: Universities_SUM_Universities_area, Type: Double\n",
      "Name: Sidewalks_OBJECTID, Type: Double\n",
      "Name: Sidewalks_FREQUENCY, Type: Double\n",
      "Name: Sidewalks_SUM_Sidewalks_area, Type: Double\n",
      "Name: Plaza_OBJECTID, Type: Double\n",
      "Name: Plaza_FREQUENCY, Type: Double\n",
      "Name: Plaza_SUM_Plaza_area, Type: Double\n",
      "Name: trails_OBJECTID, Type: Double\n",
      "Name: trails_FREQUENCY, Type: Double\n",
      "Name: trails_SUM_trails_area, Type: Double\n",
      "Name: MultiUseTrails_OBJECTID, Type: Double\n",
      "Name: MultiUseTrails_FREQUENCY, Type: Double\n",
      "Name: MultiUseTrails_SUM_MultiUseTrails_area, Type: Double\n",
      "Name: Streets_OBJECTID, Type: Double\n",
      "Name: Streets_FREQUENCY, Type: Double\n",
      "Name: Streets_SUM_Streets_effective_area, Type: Double\n",
      "Name: population_OBJECTID, Type: Double\n",
      "Name: population_FREQUENCY, Type: Double\n",
      "Name: population_SUM_proportional_population, Type: Double\n",
      "Name: SPD_Crime_Data_OBJECTID, Type: Double\n",
      "Name: SPD_Crime_Data_FREQUENCY, Type: Double\n",
      "Name: SPD_Crime_Data_COUNT_Offense_ID, Type: Double\n",
      "Name: FID_neighborhoods, Type: Integer\n",
      "Name: city, Type: String\n",
      "Name: nested, Type: String\n",
      "Name: neighborhood_area, Type: Double\n",
      "Name: is_tourist, Type: Integer\n",
      "Name: latitude, Type: Double\n",
      "Name: longitude, Type: Double\n",
      "Name: is_industrial, Type: Integer\n",
      "Name: Shape_Length, Type: Double\n",
      "Name: Shape_Area, Type: Double\n",
      "Name: Fragment_Area, Type: Double\n",
      "Fishnet fragments assigned to neighborhoods and saved back to walkscore_fishnet.\n"
     ]
    }
   ],
   "source": [
    "# Set environment settings\n",
    "arcpy.env.overwriteOutput = True  # Allow outputs to be overwritten\n",
    "\n",
    "# Define the input layers\n",
    "walkscore_fishnet = r\"C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\walkscore_fishnet\"\n",
    "neighborhoods = r\"C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\neighborhoods\"\n",
    "fishnet_neighborhoods_intersect = \"fishnet_neighborhoods_intersect\"\n",
    "neighborhood_field = \"nested\"  # Field name to be used from neighborhoods layer\n",
    "\n",
    "# Step 1: Ensure the spatial reference systems match\n",
    "walkscore_sr = arcpy.Describe(walkscore_fishnet).spatialReference\n",
    "neighborhoods_sr = arcpy.Describe(neighborhoods).spatialReference\n",
    "\n",
    "if walkscore_sr.name != neighborhoods_sr.name:\n",
    "    raise ValueError(\"Spatial references do not match between walkscore_fishnet and neighborhoods.\")\n",
    "\n",
    "# Step 2: Intersect fishnet with neighborhoods to split grids at boundaries\n",
    "arcpy.analysis.Intersect([walkscore_fishnet, neighborhoods], fishnet_neighborhoods_intersect)\n",
    "print(\"Intersected walkscore fishnet with neighborhoods to create fragments.\")\n",
    "\n",
    "# Step 3: Verify that the 'nested', 'is_tourist', and 'is_industrial' fields are present in the intersected layer\n",
    "fields = arcpy.ListFields(fishnet_neighborhoods_intersect)\n",
    "field_names = [field.name for field in fields]\n",
    "\n",
    "required_fields = [neighborhood_field, \"is_tourist\", \"is_industrial\"]\n",
    "for field in required_fields:\n",
    "    if field not in field_names:\n",
    "        raise ValueError(f\"Field '{field}' not found in the intersected layer.\")\n",
    "\n",
    "print(f\"Using fields: {required_fields}\")\n",
    "\n",
    "# Optional: Calculate the area of each fragment for further analysis\n",
    "arcpy.management.AddField(fishnet_neighborhoods_intersect, \"Fragment_Area\", \"DOUBLE\")\n",
    "arcpy.management.CalculateGeometryAttributes(fishnet_neighborhoods_intersect, [[\"Fragment_Area\", \"AREA_GEODESIC\"]])\n",
    "print(\"Calculated area for each fragment.\")\n",
    "\n",
    "# Step 4: Use the intersected result directly as the new walkscore_fishnet\n",
    "# Rename the intersected layer to replace the original walkscore_fishnet\n",
    "arcpy.management.Delete(walkscore_fishnet)  # Delete the original fishnet to allow overwriting\n",
    "arcpy.management.Rename(fishnet_neighborhoods_intersect, walkscore_fishnet)\n",
    "print(f\"Overwritten {walkscore_fishnet} with the intersected fragments, retaining neighborhood names.\")\n",
    "\n",
    "# Verify the output\n",
    "print(f\"Fields in {walkscore_fishnet} after processing:\")\n",
    "fields = arcpy.ListFields(walkscore_fishnet)\n",
    "for field in fields:\n",
    "    print(f\"Name: {field.name}, Type: {field.type}\")\n",
    "\n",
    "print(\"Fishnet fragments assigned to neighborhoods and saved back to walkscore_fishnet.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaling MAX Speed Limit for Different Uses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "speed_limit_scalers = {\n",
    "    \"Industrial\": 1.5,\n",
    "    \"ParkingLots\": 1.25,\n",
    "    \"GolfCourse\": 1.1,\n",
    "    \"Cemeteries\": 1.1,\n",
    "    \"Hospitals\": 2.0,\n",
    "    \"MultiUseTrails\": 0.9,\n",
    "    \"Parks\": 0.5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add binary fields to walkscore_fishnet_layer for each area type\n",
    "for area_type in speed_limit_scalers.keys():\n",
    "    binary_field = f\"Is{area_type.replace(' ', '')}\"\n",
    "    if not any(f.name == binary_field for f in arcpy.ListFields(walkscore_fishnet_layer)):\n",
    "        arcpy.management.AddField(walkscore_fishnet_layer, binary_field, \"SHORT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary fields updated based on effective area presence.\n"
     ]
    }
   ],
   "source": [
    "# Populate binary fields based on effective area presence\n",
    "for area_type in speed_limit_scalers.keys():\n",
    "    effective_area_field = f\"{area_type}_SUM_{area_type.replace(' ', '')}_effective_area\"\n",
    "    binary_field = f\"Is{area_type.replace(' ', '')}\"\n",
    "    \n",
    "    if any(f.name == effective_area_field for f in arcpy.ListFields(walkscore_fishnet_layer)):\n",
    "        with arcpy.da.UpdateCursor(walkscore_fishnet_layer, [effective_area_field, binary_field]) as cursor:\n",
    "            for row in cursor:\n",
    "                effective_area = row[0] if row[0] is not None else 0\n",
    "                row[1] = 1 if effective_area > 0 else 0\n",
    "                cursor.updateRow(row)\n",
    "\n",
    "print(\"Binary fields updated based on effective area presence.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max_Speed_Limit adjusted based on area type scalers with a ceiling and floor on scaling.\n"
     ]
    }
   ],
   "source": [
    "# Set the maximum and minimum allowable scaler values\n",
    "max_scaler_value = speed_limit_scalers[\"Hospitals\"]  # Maximum allowable scaler value\n",
    "min_scaler_value = speed_limit_scalers[\"Parks\"]      # Minimum allowable scaler value\n",
    "\n",
    "# Adjust Max_Speed_Limit using the binary fields and scalers with a cap on the scaling\n",
    "with arcpy.da.UpdateCursor(walkscore_fishnet_layer, [\"Max_Speed_Limit\"] + [f\"Is{area_type.replace(' ', '')}\" for area_type in speed_limit_scalers.keys()]) as cursor:\n",
    "    for row in cursor:\n",
    "        max_speed_limit = row[0]\n",
    "        applied_scaler = 1.0\n",
    "        \n",
    "        # Apply scalers based on binary fields\n",
    "        if max_speed_limit is not None:\n",
    "            for i, area_type in enumerate(speed_limit_scalers.keys(), start=1):\n",
    "                if row[i] == 1:  # If binary field is 1, apply the scaler\n",
    "                    applied_scaler *= speed_limit_scalers[area_type]\n",
    "\n",
    "                    # Ensure applied_scaler does not exceed the max_scaler_value\n",
    "                    if applied_scaler > max_scaler_value:\n",
    "                        applied_scaler = max_scaler_value\n",
    "                        break  # No need to continue if we've hit the max scaler\n",
    "\n",
    "            # Ensure applied_scaler does not fall below the min_scaler_value\n",
    "            if applied_scaler < min_scaler_value:\n",
    "                applied_scaler = min_scaler_value\n",
    "\n",
    "            # Apply the final scaler to the max speed limit\n",
    "            max_speed_limit *= applied_scaler\n",
    "            row[0] = max_speed_limit\n",
    "            cursor.updateRow(row)\n",
    "\n",
    "print(\"Max_Speed_Limit adjusted based on area type scalers with a ceiling and floor on scaling.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Finalizing Walkscore Fishnet\n",
    "\n",
    "Finally, we'll take the fishnet (walkscore_fishnet) and trim the fields down to only the mandatory fields (and permanent fields). This will include the calculation of the amenity density, which allows me to remove the count fields before passing the fishnet to the next notebook for walkscore calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "walkscore_fishnet = f\"{output_gdb}\\\\walkscore_fishnet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "arcpy.env.overwriteOutput = True  # Allow outputs to be overwritten\n",
    "\n",
    "# Define the input layers\n",
    "neighborhoods_layer = r\"C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\neighborhoods\"\n",
    "fishnet_layer = r\"C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\walkscore_fishnet\"\n",
    "tree_canopy_layer = r\"C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\TreeCanopy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_normalize(value, values):\n",
    "    sorted_values = sorted(values)\n",
    "    rank = sorted_values.index(value) + 1\n",
    "    return rank / len(values) * 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tree Canopy Cover By Fishnet Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 1: Summarize the Canopy Area by fishnet grid\n",
    "# canopy_summary_table = \"fishnet_canopy_density_summary\"\n",
    "# arcpy.analysis.Statistics(fishnet_layer, canopy_summary_table, \n",
    "#                           [[\"TreeCanopy_SUM_TreeCanopy_area\", \"SUM\"]], \n",
    "#                           [\"IndexID\"])  # Assuming 'IndexID' uniquely identifies each fishnet cell\n",
    "\n",
    "# print(\"Summarized CanopyArea by fishnet grid.\")\n",
    "\n",
    "# # Step 2: Add tree_density field to fishnet layer\n",
    "# tree_density_field = \"tree_density\"\n",
    "# if not any(f.name == tree_density_field for f in arcpy.ListFields(fishnet_layer)):\n",
    "#     arcpy.management.AddField(fishnet_layer, tree_density_field, \"DOUBLE\")\n",
    "\n",
    "# # Step 3: Join the canopy summary table back to the fishnet layer\n",
    "# arcpy.management.JoinField(fishnet_layer, \"IndexID\", canopy_summary_table, \"IndexID\", \n",
    "#                            [\"SUM_TreeCanopy_SUM_TreeCanopy_area\"])\n",
    "\n",
    "# # Step 4: Calculate tree density as a factor of the fishnet area\n",
    "# tree_density_values = []\n",
    "# fishnet_area_field = \"fragment_area\"\n",
    "\n",
    "# # Ensure that the fishnet area field is calculated\n",
    "# if not any(f.name == fishnet_area_field for f in arcpy.ListFields(fishnet_layer)):\n",
    "#     arcpy.management.AddField(fishnet_layer, fishnet_area_field, \"DOUBLE\")\n",
    "#     arcpy.management.CalculateGeometryAttributes(fishnet_layer, [[fishnet_area_field, \"AREA_GEODESIC\"]])\n",
    "\n",
    "# exponent = 0.5  # Adjust the exponent as needed for normalization\n",
    "\n",
    "# with arcpy.da.UpdateCursor(fishnet_layer, [\"SUM_TreeCanopy_SUM_TreeCanopy_area\", fishnet_area_field, tree_density_field]) as cursor:\n",
    "#     for row in cursor:\n",
    "#         # Calculate tree density with power normalization\n",
    "#         if row[0] is not None and row[1] is not None and row[1] > 0:\n",
    "#             row[2] = row[0] / math.pow(row[1], exponent)  # SUM_CanopyArea / fishnet_area^exponent\n",
    "#             tree_density_values.append(row[2])\n",
    "#         else:\n",
    "#             row[2] = None\n",
    "        \n",
    "#         cursor.updateRow(row)\n",
    "\n",
    "# print(\"Calculated power-normalized tree density for each fishnet grid.\")\n",
    "\n",
    "# # Step 5: Apply rank normalization to the tree_density field\n",
    "# with arcpy.da.UpdateCursor(fishnet_layer, [tree_density_field]) as cursor:\n",
    "#     for row in cursor:\n",
    "#         if row[0] is not None:\n",
    "#             row[0] = rank_normalize(row[0], tree_density_values)\n",
    "        \n",
    "#         cursor.updateRow(row)\n",
    "\n",
    "# print(\"Rank-normalized tree density for each fishnet grid.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Amenity Density Calculation by Fishnet Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 1: Summarize public amenity counts by fishnet grid\n",
    "# amenity_summary_table = \"fishnet_amenity_density_summary\"\n",
    "# arcpy.analysis.Statistics(fishnet_layer, amenity_summary_table, \n",
    "#                           [[\"COUNT_osm_public_id\", \"SUM\"]], \n",
    "#                           [\"IndexID\"])  # Assuming 'IndexID' uniquely identifies each fishnet cell\n",
    "\n",
    "# print(\"Summarized public amenity counts by fishnet grid.\")\n",
    "\n",
    "# # Step 2: Add amenity_density field to fishnet layer\n",
    "# amenity_density_field = \"amenity_density\"\n",
    "# if not any(f.name == amenity_density_field for f in arcpy.ListFields(fishnet_layer)):\n",
    "#     arcpy.management.AddField(fishnet_layer, amenity_density_field, \"DOUBLE\")\n",
    "\n",
    "# # Step 3: Join the amenity summary table back to the fishnet layer\n",
    "# arcpy.management.JoinField(fishnet_layer, \"IndexID\", amenity_summary_table, \"IndexID\", \n",
    "#                            [\"SUM_COUNT_osm_public_id\"])\n",
    "\n",
    "# # Step 4: Calculate amenity density as a factor of the fishnet area\n",
    "# amenity_density_values = []\n",
    "\n",
    "# with arcpy.da.UpdateCursor(fishnet_layer, [\"SUM_COUNT_osm_public_id\", fishnet_area_field, amenity_density_field]) as cursor:\n",
    "#     for row in cursor:\n",
    "#         # Calculate amenity density with power normalization\n",
    "#         if row[0] is not None and row[1] is not None and row[1] > 0:\n",
    "#             row[2] = row[0] / math.pow(row[1], exponent)  # SUM_COUNT_osm_public_id / fishnet_area^exponent\n",
    "#             amenity_density_values.append(row[2])\n",
    "#         else:\n",
    "#             row[2] = None\n",
    "        \n",
    "#         cursor.updateRow(row)\n",
    "\n",
    "# print(\"Calculated power-normalized amenity density for each fishnet grid.\")\n",
    "\n",
    "# # Step 5: Apply rank normalization to the amenity_density field\n",
    "# with arcpy.da.UpdateCursor(fishnet_layer, [amenity_density_field]) as cursor:\n",
    "#     for row in cursor:\n",
    "#         if row[0] is not None:\n",
    "#             row[0] = rank_normalize(row[0], amenity_density_values)\n",
    "        \n",
    "#         cursor.updateRow(row)\n",
    "\n",
    "# print(\"Rank-normalized amenity density for each fishnet grid.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Business Density Calculation by Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 1: Summarize business counts by fishnet grid\n",
    "# summary_table = \"fishnet_business_density_summary\"\n",
    "# arcpy.analysis.Statistics(fishnet_layer, summary_table, \n",
    "#                           [[\"COUNT_osm_business_id\", \"SUM\"]], \n",
    "#                           [\"IndexID\"])  # Assuming 'fishnet_id' is the field indicating each fishnet grid\n",
    "\n",
    "# print(\"Summarized business counts by fishnet grid.\")\n",
    "\n",
    "# # Step 2: Add business_density field to fishnet layer\n",
    "# business_density_field = \"business_density\"\n",
    "# if not any(f.name == business_density_field for f in arcpy.ListFields(fishnet_layer)):\n",
    "#     arcpy.management.AddField(fishnet_layer, business_density_field, \"DOUBLE\")\n",
    "\n",
    "# # Step 3: Join the business summary table back to the fishnet layer\n",
    "# arcpy.management.JoinField(fishnet_layer, \"IndexID\", summary_table, \"IndexID\", \n",
    "#                            [\"SUM_COUNT_osm_business_id\"])\n",
    "\n",
    "# # Step 4: Calculate business density and update the fishnet layer\n",
    "# business_density_values = []\n",
    "# exponent = 0.65  # Adjust the exponent as needed for normalization\n",
    "\n",
    "# with arcpy.da.UpdateCursor(fishnet_layer, [\"SUM_COUNT_osm_business_id\", \"total_area\", business_density_field]) as cursor:\n",
    "#     for row in cursor:\n",
    "#         # Calculate business density with power normalization\n",
    "#         if row[0] is not None and row[1] is not None and row[1] > 0:\n",
    "#             row[2] = row[0] / math.pow(row[1], exponent)  # SUM_COUNT_osm_business_id / NORM_total_area^exponent\n",
    "#             business_density_values.append(row[2])\n",
    "#         else:\n",
    "#             row[2] = None\n",
    "        \n",
    "#         cursor.updateRow(row)\n",
    "\n",
    "# print(\"Calculated power-normalized business density for each fishnet grid.\")\n",
    "\n",
    "# # Step 5: Apply rank normalization to the business_density field on a scale of 0-100\n",
    "# # Sort the business_density_values to get ranks\n",
    "# sorted_values = sorted(set(business_density_values))  # Remove duplicates for ranking\n",
    "# ranks = {value: rank for rank, value in enumerate(sorted_values, start=1)}  # Create a dictionary with ranks\n",
    "\n",
    "# # Get the maximum rank to normalize ranks between 0 and 100\n",
    "# max_rank = max(ranks.values())\n",
    "\n",
    "# # Update the business_density field with rank normalization\n",
    "# with arcpy.da.UpdateCursor(fishnet_layer, [business_density_field]) as cursor:\n",
    "#     for row in cursor:\n",
    "#         if row[0] is not None:\n",
    "#             # Normalize the rank to a scale of 0-100\n",
    "#             rank = ranks[row[0]] if row[0] in ranks else 1  # Fetch rank from dictionary\n",
    "#             row[0] = (rank - 1) / (max_rank - 1) * 10 if max_rank > 1 else 0  # Ensure scale is 0-100\n",
    "#         cursor.updateRow(row)\n",
    "\n",
    "# print(\"Rank-normalized business density for each fishnet grid on a scale of 0-100.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 6: Convert Business Density to Raster\n",
    "# business_density_raster = \"business_density_raster\"\n",
    "# arcpy.conversion.PolygonToRaster(\n",
    "#     in_features=fishnet_layer,\n",
    "#     value_field=business_density_field,\n",
    "#     out_rasterdataset=business_density_raster,\n",
    "#     cell_assignment=\"MAXIMUM_AREA\",\n",
    "#     priority_field=None,\n",
    "#     cellsize=\"250\"  # Match the cell size to the fishnet grid resolution\n",
    "# )\n",
    "# print(\"Converted business density to raster.\")\n",
    "\n",
    "# # Step 7: Smooth the Business Density Raster Using Focal Statistics\n",
    "# smoothed_raster = \"smoothed_business_density_raster\"\n",
    "# arcpy.sa.FocalStatistics(\n",
    "#     in_raster=business_density_raster,\n",
    "#     neighborhood=arcpy.sa.NbrRectangle(3, 3, \"CELL\"),  # 3x3 neighborhood for smoothing\n",
    "#     statistics_type=\"MEAN\"\n",
    "# ).save(smoothed_raster)\n",
    "# print(\"Applied focal statistics to smooth the business density raster.\")\n",
    "\n",
    "# # Step 7.1: Convert Smoothed Raster to Integer Raster\n",
    "# smoothed_integer_raster = \"smoothed_business_density_raster_int\"\n",
    "# int_raster = arcpy.sa.Int(smoothed_raster)\n",
    "# int_raster.save(smoothed_integer_raster)\n",
    "# print(\"Converted smoothed raster to integer raster.\")\n",
    "\n",
    "# # Step 7.2: Build Raster Attribute Table for Integer Raster\n",
    "# arcpy.BuildRasterAttributeTable_management(smoothed_integer_raster, \"OVERWRITE\")\n",
    "# print(\"Built raster attribute table for the integer raster.\")\n",
    "\n",
    "# # Step 8: Convert the Smoothed Integer Raster Back to Points for Joining\n",
    "# smoothed_points = \"smoothed_points\"\n",
    "# arcpy.conversion.RasterToPoint(\n",
    "#     in_raster=smoothed_integer_raster,\n",
    "#     out_point_features=smoothed_points,\n",
    "#     raster_field=\"VALUE\"\n",
    "# )\n",
    "# print(\"Converted smoothed integer raster to points for joining.\")\n",
    "\n",
    "# # Step 9: Spatial Join Points to Fishnet to Assign Smoothed Values\n",
    "# smoothed_business_density_field = \"smoothed_business_density\"\n",
    "# if not any(f.name == smoothed_business_density_field for f in arcpy.ListFields(fishnet_layer)):\n",
    "#     arcpy.management.AddField(fishnet_layer, smoothed_business_density_field, \"DOUBLE\")\n",
    "\n",
    "# # Perform a spatial join to bring the smoothed values to the fishnet grid\n",
    "# smoothed_output_layer = \"smoothed_fishnet_output\"\n",
    "# arcpy.analysis.SpatialJoin(\n",
    "#     target_features=fishnet_layer,\n",
    "#     join_features=smoothed_points,\n",
    "#     out_feature_class=smoothed_output_layer,\n",
    "#     join_type=\"KEEP_ALL\",\n",
    "#     match_option=\"CLOSEST\"\n",
    "# )\n",
    "# print(\"Performed spatial join to assign smoothed business density values to fishnet grids.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# smoothed_integer_raster = \"smoothed_business_density_raster_int\"\n",
    "# int_raster = Int(smoothed_raster)\n",
    "# int_raster.save(smoothed_integer_raster)\n",
    "# print(\"Converted smoothed raster to integer raster.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arcpy.BuildRasterAttributeTable_management(smoothed_integer_raster, \"OVERWRITE\")\n",
    "# print(\"Built raster attribute table for the integer raster.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 10: Update Fishnet with Smoothed Values\n",
    "# # Add the smoothed value to the original fishnet layer\n",
    "# with arcpy.da.UpdateCursor(smoothed_output_layer, [\"TARGET_FID\", \"VALUE\"]) as smoothed_cursor:\n",
    "#     # Create a dictionary to store the smoothed values by TARGET_FID\n",
    "#     smoothed_values_dict = {}\n",
    "    \n",
    "#     # Populate the dictionary with values from the smoothed output layer\n",
    "#     for row in smoothed_cursor:\n",
    "#         target_fid = row[0]  # Get the feature ID of the target fishnet grid\n",
    "#         smoothed_value = row[1]  # Get the smoothed value from the raster\n",
    "#         smoothed_values_dict[target_fid] = smoothed_value  # Store in the dictionary\n",
    "\n",
    "# # Update the original fishnet layer with the smoothed business density values\n",
    "# with arcpy.da.UpdateCursor(fishnet_layer, [\"OBJECTID\", \"smoothed_business_density\"]) as fishnet_cursor:\n",
    "#     for row in fishnet_cursor:\n",
    "#         objectid = row[0]\n",
    "#         if objectid in smoothed_values_dict:\n",
    "#             row[1] = smoothed_values_dict[objectid]  # Set the smoothed business density value\n",
    "#         fishnet_cursor.updateRow(row)\n",
    "\n",
    "# print(\"Updated fishnet layer with smoothed business density values.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Business Density Calculation by Neighborhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 1: Summarize business counts by neighborhood\n",
    "# summary_table = \"neighborhood_business_density_summary\"\n",
    "# arcpy.analysis.Statistics(fishnet_layer, summary_table, \n",
    "#                           [[\"COUNT_osm_business_id\", \"SUM\"]], \n",
    "#                           [\"nested\"])  # Assuming 'nested' is the field indicating neighborhood\n",
    "\n",
    "# print(\"Summarized business counts by neighborhood.\")\n",
    "\n",
    "# # Step 2: Calculate the area for each neighborhood if not already present\n",
    "# area_field = \"neighborhood_area\"\n",
    "# if not any(f.name == area_field for f in arcpy.ListFields(neighborhoods_layer)):\n",
    "#     arcpy.management.AddField(neighborhoods_layer, area_field, \"DOUBLE\")\n",
    "#     arcpy.management.CalculateGeometryAttributes(neighborhoods_layer, [[area_field, \"AREA_GEODESIC\"]])\n",
    "#     print(\"Calculated area for each neighborhood.\")\n",
    "\n",
    "# # Step 3: Add business_density field to neighborhoods layer\n",
    "# business_density_field = \"business_density\"\n",
    "# if not any(f.name == business_density_field for f in arcpy.ListFields(neighborhoods_layer)):\n",
    "#     arcpy.management.AddField(neighborhoods_layer, business_density_field, \"DOUBLE\")\n",
    "\n",
    "# # Step 4: Join the business summary table back to the neighborhoods layer\n",
    "# arcpy.management.JoinField(neighborhoods_layer, \"nested\", summary_table, \"nested\", \n",
    "#                            [\"SUM_COUNT_osm_business_id\"])\n",
    "\n",
    "# # Step 5: Calculate business density and update the neighborhoods layer\n",
    "# business_density_values = []\n",
    "# exponent = 0.65  # Adjust the exponent as needed for normalization\n",
    "\n",
    "# with arcpy.da.UpdateCursor(neighborhoods_layer, [\"SUM_COUNT_osm_business_id\", area_field, business_density_field]) as cursor:\n",
    "#     for row in cursor:\n",
    "#         # Calculate business density with power normalization\n",
    "#         if row[0] is not None and row[1] is not None and row[1] > 0:\n",
    "#             row[2] = row[0] / math.pow(row[1], exponent)  # SUM_COUNT_osm_business_id / neighborhood_area^exponent\n",
    "#             business_density_values.append(row[2])\n",
    "#         else:\n",
    "#             row[2] = None\n",
    "        \n",
    "#         cursor.updateRow(row)\n",
    "\n",
    "# print(\"Calculated power-normalized business density for each neighborhood.\")\n",
    "\n",
    "# # Step 6: Apply rank normalization to the business_density field\n",
    "# with arcpy.da.UpdateCursor(neighborhoods_layer, [business_density_field]) as cursor:\n",
    "#     for row in cursor:\n",
    "#         if row[0] is not None:\n",
    "#             row[0] = rank_normalize(row[0], business_density_values)\n",
    "        \n",
    "#         cursor.updateRow(row)\n",
    "\n",
    "# print(\"Rank-normalized business density for each neighborhood.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Merging Neighborhood Business Density into Walkscore Fishnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fields_to_join = [\"business_density\"]\n",
    "\n",
    "# arcpy.management.JoinField(walkscore_fishnet, \"nested\", neighborhoods_layer, \"nested\", fields_to_join)\n",
    "# print(f\"Joined fields {fields_to_join} from neighborhoods to walkscore_fishnet based on the 'nested' field.\")\n",
    "\n",
    "# print(\"Business densities successfully merged into walkscore_fishnet.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging Tree Canopy Density  into Walkscore Fishnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fields_to_join = [\"tree_density\"]\n",
    "\n",
    "# arcpy.management.JoinField(walkscore_fishnet, \"nested\", neighborhoods_layer, \"nested\", fields_to_join)\n",
    "# print(f\"Joined fields {fields_to_join} from neighborhoods to walkscore_fishnet based on the 'nested' field.\")\n",
    "\n",
    "# print(\"Business and public amenity densities successfully merged into walkscore_fishnet.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging Coordinates into Walkscore Fishnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fields_to_join = [\"latitude\", \"longitude\"]\n",
    "\n",
    "# arcpy.management.JoinField(walkscore_fishnet, \"nested\", neighborhoods_layer, \"nested\", fields_to_join)\n",
    "# print(f\"Joined fields {fields_to_join} from neighborhoods to fishnet based on the 'nested' field.\")\n",
    "\n",
    "# print(\"Coordinates successfully merged into walkscore_fishnet.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculation of Crime Density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added adjusted_population field to C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\walkscore_fishnet.\n",
      "Calculated adjusted_population for each fishnet grid.\n"
     ]
    }
   ],
   "source": [
    "population_scaler = {\n",
    "    \"is_tourist\": 1.50,       # Higher value for tourist areas to reflect increased transient population\n",
    "    \"is_industrial\": 0.75     # Lower value for industrial areas as fewer people typically reside there\n",
    "}\n",
    "\n",
    "# Add the adjusted population field if it doesn't exist\n",
    "adjusted_population_field = \"adjusted_population\"\n",
    "if not any(f.name == adjusted_population_field for f in arcpy.ListFields(walkscore_fishnet_layer)):\n",
    "    arcpy.management.AddField(walkscore_fishnet_layer, adjusted_population_field, \"DOUBLE\")\n",
    "    print(f\"Added {adjusted_population_field} field to {walkscore_fishnet_layer}.\")\n",
    "\n",
    "# Update the population field based on the scaler dictionary\n",
    "epsilon = 1  # Small value to avoid division by zero or very small population values\n",
    "\n",
    "with arcpy.da.UpdateCursor(walkscore_fishnet_layer, [\"SUM_proportional_population\", \"is_tourist\", \"is_industrial\", adjusted_population_field]) as cursor:\n",
    "    for row in cursor:\n",
    "        population = row[0]  # Original population\n",
    "        if population is not None:\n",
    "            # Default scaler is 1.0\n",
    "            scaler = 1.0\n",
    "\n",
    "            # Apply the appropriate scaler based on the area type flags\n",
    "            if row[1] == 1:  # 'is_tourist' field is 1, indicating a tourist area\n",
    "                scaler = population_scaler[\"is_tourist\"]\n",
    "            elif row[2] == 1:  # 'is_industrial' field is 1, indicating an industrial area\n",
    "                scaler = population_scaler[\"is_industrial\"]\n",
    "\n",
    "            # Calculate the adjusted population, adding epsilon to avoid very small values\n",
    "            row[3] = (population * scaler) + epsilon\n",
    "        else:\n",
    "            row[3] = epsilon  # Set adjusted population to epsilon if population is None\n",
    "\n",
    "        # Update the row with the adjusted population value\n",
    "        cursor.updateRow(row)\n",
    "\n",
    "print(f\"Calculated {adjusted_population_field} for each fishnet grid.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added crime_density field to C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\walkscore_fishnet.\n",
      "Calculated fishnet-level crime density for each fishnet grid.\n"
     ]
    }
   ],
   "source": [
    "crime_density_field = \"crime_density\"\n",
    "if not any(f.name == crime_density_field for f in arcpy.ListFields(walkscore_fishnet_layer)):\n",
    "    arcpy.management.AddField(walkscore_fishnet_layer, crime_density_field, \"DOUBLE\")\n",
    "    print(f\"Added {crime_density_field} field to {walkscore_fishnet_layer}.\")\n",
    "\n",
    "# Calculate crime density: crimes per adjusted population\n",
    "with arcpy.da.UpdateCursor(walkscore_fishnet_layer, [\"COUNT_Offense_ID\", adjusted_population_field, crime_density_field]) as cursor:\n",
    "    for row in cursor:\n",
    "        count_crimes = row[0]\n",
    "        adjusted_population = row[1]\n",
    "\n",
    "        if count_crimes is not None and adjusted_population is not None and adjusted_population > 0:\n",
    "            row[2] = count_crimes / adjusted_population  # crime_density = count of crimes / adjusted population\n",
    "        else:\n",
    "            row[2] = 0  # Set to 0 if no crimes or no valid population\n",
    "        cursor.updateRow(row)\n",
    "\n",
    "print(\"Calculated fishnet-level crime density for each fishnet grid.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank-normalized crime_density for each fishnet grid on a scale of 0-5.\n"
     ]
    }
   ],
   "source": [
    "crime_density_values = []\n",
    "\n",
    "with arcpy.da.SearchCursor(walkscore_fishnet_layer, [crime_density_field]) as cursor:\n",
    "    for row in cursor:\n",
    "        if row[0] is not None and row[0] > 0:\n",
    "            crime_density_values.append(row[0])\n",
    "\n",
    "# Step 4: Apply logarithmic transformation to crime_density_values\n",
    "log_crime_density_values = [math.log(value + 1) for value in crime_density_values]  # Adding 1 to avoid log(0)\n",
    "\n",
    "# Step 5: Sort the log-transformed crime_density_values to get ranks\n",
    "sorted_values = sorted(set(log_crime_density_values))  # Remove duplicates for ranking\n",
    "ranks = {value: rank for rank, value in enumerate(sorted_values, start=1)}  # Create a dictionary with ranks\n",
    "\n",
    "# Get the maximum rank to normalize ranks between 0 and 100\n",
    "max_rank = max(ranks.values())\n",
    "\n",
    "# Step 6: Update the crime_density field with rank normalization using percentile-based normalization\n",
    "with arcpy.da.UpdateCursor(walkscore_fishnet_layer, [crime_density_field]) as cursor:\n",
    "    for row in cursor:\n",
    "        if row[0] is not None and row[0] > 0:\n",
    "            # Apply logarithmic transformation to current crime density value\n",
    "            log_value = math.log(row[0] + 1)\n",
    "\n",
    "            # Normalize the rank to a scale of 0-5 using percentile normalization\n",
    "            rank = ranks.get(log_value, 1)  # Fetch rank from dictionary\n",
    "            row[0] = (rank / max_rank) * 5 if max_rank > 1 else 0  # Normalize to 0-5\n",
    "        cursor.updateRow(row)\n",
    "\n",
    "print(f\"Rank-normalized {crime_density_field} for each fishnet grid on a scale of 0-5.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neighborhood_crime_density_summary_table = r\"C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\neighborhood_crime_density_summary\"\n",
    "\n",
    "# # Delete the existing summary table if it exists (optional)\n",
    "# if arcpy.Exists(neighborhood_crime_density_summary_table):\n",
    "#     arcpy.management.Delete(neighborhood_crime_density_summary_table)\n",
    "\n",
    "# # Calculate the sum of crimes and adjusted population for each neighborhood\n",
    "# arcpy.analysis.Statistics(\n",
    "#     in_table=walkscore_fishnet_layer,\n",
    "#     out_table=neighborhood_crime_density_summary_table,\n",
    "#     statistics_fields=[\n",
    "#         [\"COUNT_Offense_ID\", \"SUM\"],  # SUM of crimes\n",
    "#         [adjusted_population_field, \"SUM\"]  # SUM of adjusted population\n",
    "#     ],\n",
    "#     case_field=\"nested\"  # Group by 'nested' field which indicates neighborhood association\n",
    "# )\n",
    "# print(\"Calculated total crimes and total population by neighborhood.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Add the neighborhood-level crime density field to walkscore_fishnet if it doesn't exist\n",
    "# neighborhood_crime_density_field = \"neighborhood_crime_density\"\n",
    "# if not any(f.name == neighborhood_crime_density_field for f in arcpy.ListFields(walkscore_fishnet_layer)):\n",
    "#     arcpy.management.AddField(walkscore_fishnet_layer, neighborhood_crime_density_field, \"DOUBLE\")\n",
    "#     print(f\"Added {neighborhood_crime_density_field} field to {walkscore_fishnet_layer}.\")\n",
    "\n",
    "# # Join the neighborhood-level statistics summary to the fishnet layer\n",
    "# arcpy.management.JoinField(\n",
    "#     in_data=walkscore_fishnet_layer,\n",
    "#     in_field=\"nested\",\n",
    "#     join_table=neighborhood_crime_density_summary_table,\n",
    "#     join_field=\"nested\",\n",
    "#     fields=[\"SUM_COUNT_Offense_ID\", \"SUM_\" + adjusted_population_field]\n",
    "# )\n",
    "\n",
    "# epsilon = 500  # Small value to smooth out small population effects at the neighborhood level\n",
    "# with arcpy.da.UpdateCursor(walkscore_fishnet_layer, [\"SUM_COUNT_Offense_ID\", \"SUM_\" + adjusted_population_field, neighborhood_crime_density_field]) as cursor:\n",
    "#     for row in cursor:\n",
    "#         total_crimes = row[0]\n",
    "#         total_population = row[1]\n",
    "\n",
    "#         if total_crimes is not None and total_population is not None and total_population > 0:\n",
    "#             row[2] = total_crimes / (total_population + epsilon)  # neighborhood_crime_density = total crimes / (total population + epsilon)\n",
    "#         else:\n",
    "#             row[2] = 0  # Set to 0 if no crimes or no valid population\n",
    "#         cursor.updateRow(row)\n",
    "\n",
    "# print(\"Calculated neighborhood-level crime density for each fishnet grid.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crime_density_values = [row[0] for row in arcpy.da.SearchCursor(walkscore_fishnet_layer, [neighborhood_crime_density_field]) if row[0] is not None]\n",
    "\n",
    "# if crime_density_values:\n",
    "#     # Cap the maximum value at 10\n",
    "#     max_crime_density = min(max(crime_density_values), 8)\n",
    "#     min_crime_density = min(crime_density_values)\n",
    "\n",
    "#     # Rank normalization\n",
    "#     sorted_values = sorted(set(crime_density_values))\n",
    "#     ranks = {value: rank for rank, value in enumerate(sorted_values, start=1)}\n",
    "#     max_rank = max(ranks.values())\n",
    "\n",
    "#     # Ensure min and max values are not the same to avoid division by zero\n",
    "#     if min_crime_density != max_crime_density:\n",
    "#         with arcpy.da.UpdateCursor(walkscore_fishnet_layer, [neighborhood_crime_density_field]) as cursor:\n",
    "#             for row in cursor:\n",
    "#                 # Cap the value at 10 if it exceeds the cap\n",
    "#                 capped_value = min(row[0], 10)\n",
    "\n",
    "#                 # Normalize to range [0, 5]\n",
    "#                 normalized_value = (capped_value - min_crime_density) / (max_crime_density - min_crime_density) * 5\n",
    "\n",
    "#                 # Apply rank normalization\n",
    "#                 rank = ranks.get(row[0], 1)\n",
    "#                 rank_normalized_value = (rank - 1) / (max_rank - 1) * 5 if max_rank > 1 else 0\n",
    "\n",
    "#                 # Average both normalizations and cap to range [0, 5]\n",
    "#                 combined_value = (normalized_value + rank_normalized_value) / 2\n",
    "#                 row[0] = min(combined_value, 5)  # Cap at 5 to ensure it stays in range [0, 5]\n",
    "#                 cursor.updateRow(row)\n",
    "\n",
    "#         print(f\"Normalized neighborhood-level crime density to range [0, 5] for each fishnet grid with a cap at 10, including rank normalization.\")\n",
    "#     else:\n",
    "#         print(f\"All neighborhood-level crime densities are the same value ({min_crime_density}), normalization is not needed.\")\n",
    "# else:\n",
    "#     print(\"No valid crime density values found for normalization.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with arcpy.da.UpdateCursor(walkscore_fishnet_layer, [\"MEDIAN_crime_density\", median_crime_density_field]) as cursor:\n",
    "#     for row in cursor:\n",
    "#         row[1] = row[0]  # Assign the median value to the median_crime_density field\n",
    "#         cursor.updateRow(row)\n",
    "# print(f\"Updated {median_crime_density_field} field in walkscore_fishnet layer with median values.\")\n",
    "\n",
    "# #### Removing Unnecessary Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fields in C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\walkscore_fishnet after dropping specified fields:\n",
      "Name: OBJECTID, Type: OID\n",
      "Name: Shape, Type: Geometry\n",
      "Name: FID_walkscore_fishnet, Type: Integer\n",
      "Name: IndexID, Type: Integer\n",
      "Name: total_area, Type: Double\n",
      "Name: Max_Speed_Limit, Type: Double\n",
      "Name: SUM_proportional_population, Type: Double\n",
      "Name: COUNT_Offense_ID, Type: Integer\n",
      "Name: Grid_Slope_MEAN, Type: Double\n",
      "Name: Sidewalks_Slope_Mean, Type: Double\n",
      "Name: Streets_Slope_Mean, Type: Double\n",
      "Name: MultiUseTrails_Slope_Mean, Type: Double\n",
      "Name: trails_Slope_Mean, Type: Double\n",
      "Name: effective_slope, Type: Double\n",
      "Name: business_density, Type: Double\n",
      "Name: Industrial_SUM_Industrial_effective_area, Type: Double\n",
      "Name: ParkingLots_SUM_ParkingLots_effective_area, Type: Double\n",
      "Name: GolfCourse_SUM_GolfCourse_effective_area, Type: Double\n",
      "Name: Cemeteries_SUM_Cemeteries_effective_area, Type: Double\n",
      "Name: Hospitals_SUM_Hospitals_effective_area, Type: Double\n",
      "Name: Slope_MEAN, Type: Double\n",
      "Name: Bike_greenways_SUM_Bike_greenways_area, Type: Double\n",
      "Name: Bike_protected_SUM_Bike_protected_area, Type: Double\n",
      "Name: Bike_buffer_SUM_Bike_buffer_area, Type: Double\n",
      "Name: Healthy_Streets_SUM_Healthy_Streets_area, Type: Double\n",
      "Name: Parks_SUM_Parks_area, Type: Double\n",
      "Name: Universities_SUM_Universities_area, Type: Double\n",
      "Name: Sidewalks_SUM_Sidewalks_area, Type: Double\n",
      "Name: Plaza_SUM_Plaza_area, Type: Double\n",
      "Name: trails_SUM_trails_area, Type: Double\n",
      "Name: MultiUseTrails_SUM_MultiUseTrails_area, Type: Double\n",
      "Name: Streets_SUM_Streets_effective_area, Type: Double\n",
      "Name: population_SUM_proportional_population, Type: Double\n",
      "Name: SPD_Crime_Data_COUNT_Offense_ID, Type: Double\n",
      "Name: FID_neighborhoods, Type: Integer\n",
      "Name: city, Type: String\n",
      "Name: nested, Type: String\n",
      "Name: neighborhood_area, Type: Double\n",
      "Name: is_tourist, Type: Integer\n",
      "Name: latitude, Type: Double\n",
      "Name: longitude, Type: Double\n",
      "Name: is_industrial, Type: Integer\n",
      "Name: Shape_Length, Type: Double\n",
      "Name: Shape_Area, Type: Double\n",
      "Name: Fragment_Area, Type: Double\n",
      "Name: IsIndustrial, Type: SmallInteger\n",
      "Name: IsParkingLots, Type: SmallInteger\n",
      "Name: IsGolfCourse, Type: SmallInteger\n",
      "Name: IsCemeteries, Type: SmallInteger\n",
      "Name: IsHospitals, Type: SmallInteger\n",
      "Name: IsMultiUseTrails, Type: SmallInteger\n",
      "Name: IsParks, Type: SmallInteger\n",
      "Name: adjusted_population, Type: Double\n",
      "Name: crime_density, Type: Double\n",
      "Fields dropped successfully.\n"
     ]
    }
   ],
   "source": [
    "# Drop specified fields\n",
    "fields_to_drop = []\n",
    "for field in arcpy.ListFields(walkscore_fishnet):\n",
    "    if field.name.endswith(\"FREQUENCY\") or field.name.endswith(\"_OBJECTID\") or field.name.endswith(\"Slope_AREA\") or field.name.endswith(\"Slope_COUNT\") or field.name.endswith(\"IndexID_1\") or field.name.endswith(\"_id\"):\n",
    "        fields_to_drop.append(field.name)\n",
    "\n",
    "if fields_to_drop:\n",
    "    arcpy.management.DeleteField(walkscore_fishnet, fields_to_drop)\n",
    "\n",
    "# Verify fields in walkscore_fishnet after dropping specified fields\n",
    "walkscore_fishnet = f\"{output_gdb}\\\\walkscore_fishnet\"\n",
    "print(f\"\\nFields in {walkscore_fishnet} after dropping specified fields:\")\n",
    "fields = arcpy.ListFields(walkscore_fishnet)\n",
    "for field in fields:\n",
    "    print(f\"Name: {field.name}, Type: {field.type}\")\n",
    "\n",
    "print(\"Fields dropped successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Cleaning Contents Pane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_created_layers(layers_list):\n",
    "    for layer in layers_list:\n",
    "        if arcpy.Exists(layer):\n",
    "            arcpy.management.Delete(layer)\n",
    "            print(f\"Deleted layer: {layer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_layers = [\n",
    "    \"Industrial\",\n",
    "    \"ParkingLots\",\n",
    "    \"GolfCourse\",\n",
    "    \"Cemeteries\",\n",
    "    \"Hospitals\",\n",
    "#     \"Slope\",\n",
    "    \"Bike_greenways\",\n",
    "    \"Bike_protected\",\n",
    "    \"Bike_buffer\",\n",
    "    \"Healthy_Streets\",\n",
    "    \"Parks\",\n",
    "    \"Universities\",\n",
    "    \"Sidewalks\",\n",
    "    \"Plaza\",\n",
    "    \"trails\",\n",
    "    \"MultiUseTrails\",\n",
    "    \"Streets\",\n",
    "    \"fishnet_clipped\",\n",
    "    \"Marked_Crosswalks\",\n",
    "    \"fishnet_clipped\",\n",
    "    \"neighborhoods\",\n",
    "    \"population\",\n",
    "    \"sped_Crime_Data\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing layer: Industrial\n",
      "C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\Industrial is already in the target spatial reference.\n",
      "Processing layer: ParkingLots\n",
      "C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\ParkingLots is already in the target spatial reference.\n",
      "Processing layer: GolfCourse\n",
      "C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\GolfCourse is already in the target spatial reference.\n",
      "Processing layer: Cemeteries\n",
      "C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\Cemeteries is already in the target spatial reference.\n",
      "Processing layer: Hospitals\n",
      "C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\Hospitals is already in the target spatial reference.\n",
      "Processing layer: Bike_greenways\n",
      "C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\Bike_greenways is already in the target spatial reference.\n",
      "Processing layer: Bike_protected\n",
      "C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\Bike_protected is already in the target spatial reference.\n",
      "Processing layer: Bike_buffer\n",
      "C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\Bike_buffer is already in the target spatial reference.\n",
      "Processing layer: Healthy_Streets\n",
      "C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\Healthy_Streets is already in the target spatial reference.\n",
      "Processing layer: Parks\n",
      "C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\Parks is already in the target spatial reference.\n",
      "Processing layer: Universities\n",
      "C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\Universities is already in the target spatial reference.\n",
      "Processing layer: Sidewalks\n",
      "C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\Sidewalks is already in the target spatial reference.\n",
      "Processing layer: Plaza\n",
      "C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\Plaza is already in the target spatial reference.\n",
      "Processing layer: trails\n",
      "C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\trails is already in the target spatial reference.\n",
      "Processing layer: MultiUseTrails\n",
      "C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\MultiUseTrails is already in the target spatial reference.\n",
      "Processing layer: Streets\n",
      "C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\Streets is already in the target spatial reference.\n",
      "Processing layer: fishnet_clipped\n",
      "C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\fishnet_clipped is already in the target spatial reference.\n",
      "Processing layer: Marked_Crosswalks\n",
      "C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\Marked_Crosswalks is already in the target spatial reference.\n",
      "Processing layer: fishnet_clipped\n",
      "C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\fishnet_clipped is already in the target spatial reference.\n",
      "Processing layer: neighborhoods\n",
      "C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\neighborhoods is already in the target spatial reference.\n",
      "Processing layer: population\n",
      "C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\population is already in the target spatial reference.\n",
      "Processing layer: sped_Crime_Data\n",
      "Layer C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\\sped_Crime_Data does not exist. Skipping.\n",
      "All layers have been projected to the target spatial reference.\n"
     ]
    }
   ],
   "source": [
    "base_layers_group = r\"C:\\Users\\rtvpd\\Documents\\Walkability_Seattle\\Walkability_Seattle.gdb\"\n",
    "target_spatial_reference = arcpy.SpatialReference(3857)  # WGS 1984 Web Mercator (auxiliary sphere)\n",
    "\n",
    "def project_layer(input_layer, target_sr):\n",
    "    input_layer_sr = arcpy.Describe(input_layer).spatialReference\n",
    "    \n",
    "    if input_layer_sr.name != target_sr.name:\n",
    "        temp_projected_layer = os.path.join(arcpy.env.scratchGDB, f\"{os.path.basename(input_layer)}_proj\")\n",
    "        arcpy.management.Project(input_layer, temp_projected_layer, target_sr)\n",
    "        print(f\"Projected {input_layer} to {temp_projected_layer}.\")\n",
    "        \n",
    "        # Overwrite the original layer with the projected version\n",
    "        arcpy.management.Delete(input_layer)\n",
    "        arcpy.management.CopyFeatures(temp_projected_layer, input_layer)\n",
    "        arcpy.management.Delete(temp_projected_layer)\n",
    "        print(f\"Replaced original {input_layer} with projected version.\")\n",
    "    else:\n",
    "        print(f\"{input_layer} is already in the target spatial reference.\")\n",
    "\n",
    "# Process each base layer\n",
    "for layer_name in base_layers:\n",
    "    print(f\"Processing layer: {layer_name}\")  # Debugging statement\n",
    "\n",
    "    # Access the layer\n",
    "    input_layer = f\"{base_layers_group}\\\\{layer_name}\"\n",
    "    \n",
    "    # Verify if the input_layer exists\n",
    "    if not arcpy.Exists(input_layer):\n",
    "        print(f\"Layer {input_layer} does not exist. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    # Project the input layer to the target spatial reference\n",
    "    project_layer(input_layer, target_spatial_reference)\n",
    "\n",
    "print(\"All layers have been projected to the target spatial reference.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "file_extension": ".py",
   "name": "python",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
